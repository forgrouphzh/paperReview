[{"instruction": "Please extract the key word of this abstract and list their type.", "input": " Information extraction techniques  automatically create  structured databases  from  unstructured data sources , such as the Web or  newswire documents . Despite the successes of these systems,  accuracy  will always be imperfect. For many reasons, it is highly desirable to accurately estimate the  confidence  the system has in the correctness of each  extracted field . The  information extraction system  we evaluate is based on a  linear-chain conditional random field (CRF) , a  probabilistic model  which has performed well on  information extraction tasks  because of its ability to capture arbitrary, overlapping  features  of the  input  in a  Markov model . We implement several techniques to estimate the  confidence  of both  extracted fields  and entire  multi-field records , obtaining an  average precision  of 98% for retrieving correct  fields  and 87% for multi-field records.\n", "output": "The abstract text contains these key words below: 'Information extraction techniques' whose type is Method, 'structured databases' whose type is Material, 'unstructured data sources' whose type is Material, 'Web' whose type is Material, 'newswire documents' whose type is Material, 'systems' whose type is Generic, 'accuracy' whose type is Metric, 'system' whose type is Generic, 'information extraction system' whose type is Method, 'linear-chain conditional random field (CRF)' whose type is Method, 'probabilistic model' whose type is Method, 'information extraction tasks' whose type is Task, 'arbitrary, overlapping  features' whose type is OtherScientificTerm, 'input' whose type is Generic, 'Markov model' whose type is Method, 'techniques' whose type is Generic, 'extracted fields' whose type is Generic, 'multi-field records' whose type is Material, 'average precision' whose type is Metric, 'multi-field records' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose a novel limited-memory stochastic block BFGS update for incorporating enriched curvature information in stochastic approximation methods. In our method, the estimate of the inverse Hessian matrix that is maintained by it, is updated at each iteration using a sketch of the Hessian, i.e., a randomly generated compressed form of the Hessian. We propose several sketching strategies, present a new quasi-Newton method that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute batch stochastic gradients, and prove linear convergence of the resulting method. Numerical tests on large-scale logistic regression problems reveal that our method is more robust and substantially outperforms current state-of-the-art methods.", "output": "The abstract text contains these key words below: 'limited-memory stochastic block BFGS update' whose type is Method, 'incorporating enriched curvature information in stochastic approximation methods' whose type is Task, 'method' whose type is Generic, 'inverse Hessian matrix' whose type is OtherScientificTerm, 'it' whose type is Generic, 'Hessian' whose type is OtherScientificTerm, 'randomly generated compressed form of the Hessian' whose type is OtherScientificTerm, 'sketching strategies' whose type is Method, 'quasi-Newton method' whose type is Method, 'stochastic block BFGS updates' whose type is Method, 'variance reduction approach SVRG' whose type is Method, 'batch stochastic gradients' whose type is OtherScientificTerm, 'linear convergence' whose type is OtherScientificTerm, 'method' whose type is Generic, 'large-scale logistic regression problems' whose type is Task, 'method' whose type is Generic, 'state-of-the-art methods' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ecently, Stacked Auto-Encoders (SAE) have been successfully used for learning imbalanced datasets. In this paper, for the first time, we propose to use a Neural Network classifier furnished by an SAE structure for detecting the errors made by a strong Automatic Speech Recognition (ASR) system. Error detection on an automatic transcription provided by a \" strong \" ASR system, i.e. exhibiting a small word error rate, is difficult due to the limited number of \" positive \" examples (i.e. words erroneously recognized) available for training a binary classi-fier. In this paper we investigate and compare different types of classifiers for automatically detecting ASR errors, including the one based on a stacked auto-encoder architecture. We show the effectiveness of the latter by measuring and comparing performance on the automatic transcriptions of an English corpus collected from TED talks. Performance of each investigated classifier is evaluated both via receiving operating curve and via a measure, called mean absolute error, related to the quality in predicting the corresponding word error rate. The results demonstrates that the classifier based on SAE detects the ASR errors better than the other classification methods.", "output": "The abstract text contains these key words below: 'Stacked Auto-Encoders (SAE)' whose type is Method, 'learning imbalanced datasets' whose type is Task, 'Neural Network classifier' whose type is Method, 'SAE structure' whose type is Method, 'Automatic Speech Recognition (ASR) system' whose type is Method, 'Error detection' whose type is Task, 'automatic transcription' whose type is Material, 'ASR system' whose type is Method, 'word error rate' whose type is Metric, 'binary classi-fier' whose type is Method, 'classifiers' whose type is Method, 'automatically detecting ASR errors' whose type is Task, 'one' whose type is Generic, 'stacked auto-encoder architecture' whose type is Method, 'automatic transcriptions' whose type is Material, 'English corpus' whose type is Material, 'TED talks' whose type is Material, 'classifier' whose type is Method, 'receiving operating curve' whose type is Metric, 'measure' whose type is Generic, 'mean absolute error' whose type is Metric, 'word error rate' whose type is Metric, 'classifier' whose type is Method, 'SAE' whose type is Method, 'ASR errors' whose type is OtherScientificTerm, 'classification methods' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we propose a novel method, called local non-negative matrix factorization (LNMF), for learning spatially localized, parts-based subspace representation of visual patterns. An objective function is defined to impose lo-calization constraint, in addition to the non-negativity constraint in the standard NMF [1]. This gives a set of bases which not only allows a non-subtractive (part-based) representation of images but also manifests localized features. An algorithm is presented for the learning of such basis components. Experimental results are presented to compare LNMF with the NMF and PCA methods for face representation and recognition, which demonstrates advantages of LNMF.", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'local non-negative matrix factorization (LNMF)' whose type is Method, 'spatially localized, parts-based subspace representation of visual patterns' whose type is Task, 'objective function' whose type is OtherScientificTerm, 'lo-calization constraint' whose type is OtherScientificTerm, 'non-negativity constraint' whose type is OtherScientificTerm, 'NMF' whose type is Method, 'non-subtractive (part-based) representation of images' whose type is Method, 'localized features' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'learning' whose type is Task, 'LNMF' whose type is Method, 'NMF and PCA methods' whose type is Method, 'face representation and recognition' whose type is Task, 'LNMF' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The theoretical study of the  range concatenation grammar [RCG] formalism  has revealed many attractive properties which may be used in  NLP  . In particular,  range concatenation languages [RCL]  can be parsed in  polynomial time  and many classical  grammatical formalisms  can be translated into equivalent  RCGs  without increasing their  worst-case parsing time complexity  . For example, after  translation  into an equivalent  RCG  , any  tree adjoining grammar  can be parsed in  O(n6) time  . In this paper, we study a  parsing technique  whose purpose is to improve the practical efficiency of  RCL parsers  . The  non-deterministic parsing choices  of the  main parser  for a  language L  are directed by a  guide  which uses the  shared derivation forest  output by a prior  RCL parser  for a suitable  superset of L  . The results of a practical evaluation of this method on a  wide coverage English grammar  are given. ", "output": "The abstract text contains these key words below: 'range concatenation grammar [RCG] formalism' whose type is Method, 'NLP' whose type is Task, 'range concatenation languages [RCL]' whose type is OtherScientificTerm, 'polynomial time' whose type is OtherScientificTerm, 'grammatical formalisms' whose type is Method, 'RCGs' whose type is Method, 'worst-case parsing time complexity' whose type is Metric, 'RCG' whose type is Method, 'tree adjoining grammar' whose type is Method, 'O(n6) time' whose type is OtherScientificTerm, 'parsing technique' whose type is Method, 'RCL parsers' whose type is Method, 'non-deterministic parsing' whose type is Method, 'main parser' whose type is Method, 'language L' whose type is OtherScientificTerm, 'shared derivation forest' whose type is OtherScientificTerm, 'RCL parser' whose type is Method, 'method' whose type is Generic, 'wide coverage English grammar' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "olor is known to be highly discriminative for many object recognition tasks, but is difficult to infer from uncontrolled images in which the illuminant is not known. Traditional methods for color constancy can improve surface re-flectance estimates from such uncalibrated images, but their output depends significantly on the background scene. In many recognition and retrieval applications, we have access to image sets that contain multiple views of the same object in different environments; we show in this paper that correspondences between these images provide important constraints that can improve color constancy. We introduce the multi-view color constancy problem, and present a method to recover estimates of underlying surface re-flectance based on joint estimation of these surface properties and the illuminants present in multiple images. The method can exploit image correspondences obtained by various alignment techniques, and we show examples based on matching local region features. Our results show that multi-view constraints can significantly improve estimates of both scene illuminants and object color (surface reflectance) when compared to a baseline single-view method.", "output": "The abstract text contains these key words below: 'object recognition tasks' whose type is Task, 'uncontrolled images' whose type is Material, 'illuminant' whose type is OtherScientificTerm, 'methods' whose type is Generic, 'color constancy' whose type is Task, 'surface re-flectance estimates' whose type is Method, 'uncalibrated images' whose type is Material, 'background scene' whose type is OtherScientificTerm, 'recognition and retrieval applications' whose type is Task, 'color constancy' whose type is Task, 'multi-view color constancy problem' whose type is Task, 'method' whose type is Generic, 'estimates of underlying surface re-flectance' whose type is Task, 'surface properties' whose type is OtherScientificTerm, 'illuminants' whose type is OtherScientificTerm, 'method' whose type is Generic, 'image correspondences' whose type is OtherScientificTerm, 'alignment techniques' whose type is Method, 'matching local region features' whose type is OtherScientificTerm, 'multi-view constraints' whose type is OtherScientificTerm, 'estimates of both scene illuminants and object color (surface reflectance)' whose type is Task, 'baseline single-view method' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present a method for estimating the relative pose of two calibrated or uncalibrated non-overlapping surveillance cameras from observing a moving object. We show how to tackle the problem of missing point correspondences heavily required by SfM pipelines and how to go beyond this basic paradigm. We relax the non-linear nature of the problem by accepting two assumptions which surveillance scenarios offer, ie. the presence of a moving object and easily estimable gravity vector. By those assumptions we cast the problem as a Quadratic Eigenvalue Problem offering an elegant way of treating nonlinear monomials and delivering a quasi closed-form solution as a reliable starting point for a further bundle adjustment. We are the first to bring the closed form solution to such a very practical problem arising in video surveillance. Results in different camera setups demonstrate the feasibility of the approach.", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'relative pose of two calibrated or uncalibrated non-overlapping surveillance cameras' whose type is Task, 'missing point correspondences' whose type is Task, 'SfM pipelines' whose type is Method, 'paradigm' whose type is Generic, 'non-linear nature' whose type is OtherScientificTerm, 'problem' whose type is Generic, 'assumptions' whose type is Generic, 'surveillance scenarios' whose type is OtherScientificTerm, 'gravity vector' whose type is OtherScientificTerm, 'assumptions' whose type is Generic, 'problem' whose type is Generic, 'Quadratic Eigenvalue Problem' whose type is Task, 'nonlinear monomials' whose type is OtherScientificTerm, 'quasi closed-form solution' whose type is Method, 'bundle adjustment' whose type is Task, 'closed form solution' whose type is Method, 'problem' whose type is Generic, 'video surveillance' whose type is Task, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Metagrammatical formalisms  that combine  context-free phrase structure rules  and  metarules (MPS grammars)  allow concise statement of generalizations about the  syntax  of  natural languages .  Unconstrained MPS grammars , unfortunately, are not computationally safe. We evaluate several proposals for constraining them, basing our assessment on  computational tractability and explanatory adequacy . We show that none of them satisfies both criteria, and suggest new directions for research on alternative  metagrammatical formalisms . ", "output": "The abstract text contains these key words below: 'Metagrammatical formalisms' whose type is Method, 'context-free phrase structure rules' whose type is OtherScientificTerm, 'metarules (MPS grammars)' whose type is OtherScientificTerm, 'syntax  of  natural languages' whose type is OtherScientificTerm, 'Unconstrained MPS grammars' whose type is Method, 'them' whose type is Generic, 'computational tractability and explanatory adequacy' whose type is Metric, 'them' whose type is Generic, 'criteria' whose type is Generic, 'metagrammatical formalisms' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n The paper provides an overview of the research conducted at  LIMSI  in the field of  speech processing  , but also in the related areas of  Human-Machine Communication  , including  Natural Language Processing  ,  Non Verbal and Multimodal Communication  . Also presented are the commercial applications of some of the research projects. When applicable, the discussion is placed in the framework of international collaborations. ", "output": "The abstract text contains these key words below: 'speech processing' whose type is Task, 'Human-Machine Communication' whose type is Task, 'Natural Language Processing' whose type is Task, 'Non Verbal and Multimodal Communication' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "etecting fine-grained subtle changes among a scene is critically important in practice. Previous change detection methods, focusing on detecting large-scale significant changes, cannot do this well. This paper proposes a feasible end-to-end approach to this challenging problem. We start from active camera relocation that quickly relocates camera to nearly the same pose and position of the last time observation. To guarantee detection sensitivity and accuracy of minute changes, in an observation, we capture a group of images under multiple illuminations, which need only to be roughly aligned to the last time lighting conditions. Given two times observations, we formulate fine-grained change detection as a joint optimization problem of three related factors, i.e., normal-aware lighting difference, camera geometry correction flow, and real scene change mask. We solve the three factors in a coarse-to-fine manner and achieve reliable change decision by rank minimization. We build three real-world datasets to benchmark fine-grained change detection of misaligned scenes under varied multiple lighting conditions. Extensive experiments show the superior performance of our approach over state-of-the-art change detection methods and its ability to distinguish real scene changes from false ones caused by lighting variations.", "output": "The abstract text contains these key words below: 'Detecting fine-grained subtle changes among a scene' whose type is Task, 'change detection methods' whose type is Method, 'detecting large-scale significant changes' whose type is Task, 'end-to-end approach' whose type is Method, 'problem' whose type is Generic, 'active camera relocation' whose type is Method, 'detection sensitivity' whose type is Metric, 'accuracy' whose type is Metric, 'illuminations' whose type is OtherScientificTerm, 'fine-grained change detection' whose type is Task, 'joint optimization problem' whose type is Task, 'factors' whose type is Generic, 'normal-aware lighting difference' whose type is OtherScientificTerm, 'camera geometry correction flow' whose type is OtherScientificTerm, 'real scene change mask' whose type is OtherScientificTerm, 'factors' whose type is Generic, 'coarse-to-fine manner' whose type is OtherScientificTerm, 'change decision' whose type is OtherScientificTerm, 'rank minimization' whose type is Method, 'real-world datasets' whose type is Material, 'fine-grained change detection of misaligned scenes' whose type is Task, 'varied multiple lighting conditions' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'change detection methods' whose type is Method, 'real scene changes' whose type is OtherScientificTerm, 'lighting variations' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Sentence planning  is a set of inter-related but distinct tasks, one of which is  sentence scoping  , i.e. the choice of  syntactic structure  for elementary  speech acts  and the decision of how to combine them into one or more  sentences  . In this paper, we present  SPoT  , a  sentence planner  , and a new methodology for automatically training  SPoT  on the basis of  feedback  provided by  human judges  . We reconceptualize the task into two distinct phases. First, a very simple,  randomized sentence-plan-generator (SPG)  generates a potentially large list of possible  sentence plans  for a given  text-plan input  . Second, the  sentence-plan-ranker (SPR)  ranks the list of output  sentence plans  , and then selects the top-ranked  plan  . The  SPR  uses  ranking rules  automatically learned from  training data  . We show that the trained  SPR  learns to select a  sentence plan  whose rating on average is only 5% worse than the  top human-ranked sentence plan  . ", "output": "The abstract text contains these key words below: 'Sentence planning' whose type is Task, 'tasks' whose type is Generic, 'sentence scoping' whose type is Task, 'syntactic structure' whose type is OtherScientificTerm, 'speech acts' whose type is OtherScientificTerm, 'SPoT' whose type is Method, 'sentence planner' whose type is Method, 'methodology' whose type is Generic, 'SPoT' whose type is Method, 'randomized sentence-plan-generator (SPG)' whose type is Method, 'sentence plans' whose type is OtherScientificTerm, 'text-plan input' whose type is OtherScientificTerm, 'sentence-plan-ranker (SPR)' whose type is Method, 'sentence plans' whose type is OtherScientificTerm, 'SPR' whose type is Method, 'ranking rules' whose type is OtherScientificTerm, 'SPR' whose type is Method, 'sentence plan' whose type is OtherScientificTerm, 'top human-ranked sentence plan' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper addresses the problem of identifying likely  topics  of  texts  by their position in the  text . It describes the automated  training  and evaluation of an  Optimal Position Policy , a method of locating the likely positions of  topic-bearing sentences  based on  genre-specific regularities  of  discourse structure . This method can be used in applications such as  information retrieval ,  routing , and  text summarization .\n", "output": "The abstract text contains these key words below: 'Optimal Position Policy' whose type is Method, 'method' whose type is Generic, 'positions of  topic-bearing sentences' whose type is OtherScientificTerm, 'genre-specific regularities  of  discourse structure' whose type is OtherScientificTerm, 'method' whose type is Generic, 'applications' whose type is Generic, 'information retrieval' whose type is Task, 'routing' whose type is Task, 'text summarization' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The applicability of many current  information extraction techniques  is severely limited by the need for  supervised training data . \nWe demonstrate that for certain  field structured extraction tasks , such as classified advertisements and bibliographic citations, small amounts of  prior knowledge  can be used to learn effective models in a primarily unsupervised fashion. Although  hidden Markov models (HMMs)  provide a suitable  generative model  for  field structured text , general  unsupervised HMM learning  fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple  prior knowledge  of the desired solutions. In both domains, we found that  unsupervised methods  can attain  accuracies  with 400  unlabeled examples  comparable to those attained by  supervised methods  on 50  labeled examples , and that  semi-supervised methods  can make good use of small amounts of  labeled data .\n", "output": "The abstract text contains these key words below: 'information extraction techniques' whose type is Method, 'supervised training data' whose type is Material, 'field structured extraction tasks' whose type is Task, 'classified advertisements' whose type is Material, 'bibliographic citations' whose type is Material, 'prior knowledge' whose type is OtherScientificTerm, 'hidden Markov models (HMMs)' whose type is Method, 'generative model' whose type is Method, 'field structured text' whose type is Material, 'unsupervised HMM learning' whose type is Method, 'prior knowledge' whose type is OtherScientificTerm, 'unsupervised methods' whose type is Method, 'accuracies' whose type is Metric, 'unlabeled examples' whose type is Material, 'supervised methods' whose type is Method, 'labeled examples' whose type is Material, 'semi-supervised methods' whose type is Method, 'labeled data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper presents a specialized  editor  for a highly structured  dictionary  . The basic goal in building that  editor  was to provide an adequate tool to help  lexicologists  produce a valid and coherent  dictionary  on the basis of a  linguistic theory  . If we want valuable  lexicons  and  grammars  to achieve complex  natural language processing  , we must provide very powerful tools to help create and ensure the validity of such complex  linguistic databases  . Our most important task in building the  editor  was to define a set of  coherence rules  that could be computationally applied to ensure the validity of  lexical entries  . A customized  interface  for browsing and editing was also designed and implemented. ", "output": "The abstract text contains these key words below: 'editor' whose type is Method, 'structured  dictionary' whose type is Material, 'editor' whose type is Method, 'dictionary' whose type is Generic, 'linguistic theory' whose type is Method, 'natural language processing' whose type is Task, 'linguistic databases' whose type is Material, 'editor' whose type is Method, 'coherence rules' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n Although every  natural language system  needs a  computational lexicon  , each system puts different amounts and types of information into its  lexicon  according to its individual needs. However, some of the information needed across systems is shared or identical information. This paper presents our experience in planning and building  COMPLEX  , a  computational lexicon  designed to be a repository of  shared lexical information  for use by  Natural Language Processing (NLP) systems  . We have drawn primarily on explicit and implicit information from  machine-readable dictionaries (MRD's)  to create a  broad coverage lexicon  . ", "output": "The abstract text contains these key words below: 'natural language system' whose type is Method, 'computational lexicon' whose type is OtherScientificTerm, 'system' whose type is Generic, 'COMPLEX' whose type is Method, 'computational lexicon' whose type is OtherScientificTerm, 'shared lexical information' whose type is OtherScientificTerm, 'Natural Language Processing (NLP) systems' whose type is Task, 'machine-readable dictionaries (MRD's)' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a  tool , called  ILIMP , which takes as input a  raw text  in  French  and produces as output the same  text  in which every occurrence of the  pronoun il  is tagged either with tag  [ANA]  for  anaphoric  or  [IMP]  for  impersonal  or  expletive . This  tool  is therefore designed to distinguish between the  anaphoric occurrences of il , for which an  anaphora resolution system  has to look for an antecedent, and the  expletive occurrences  of this  pronoun , for which it does not make sense to look for an antecedent. The  precision rate  for  ILIMP  is 97,5%. The few  errors  are analyzed in detail. Other  tasks  using the  method  developed for  ILIMP  are described briefly, as well as the use of  ILIMP  in a modular  syntactic analysis system . ", "output": "The abstract text contains these key words below: 'tool' whose type is Generic, 'ILIMP' whose type is Method, 'raw text  in  French' whose type is Material, 'tool' whose type is Generic, 'anaphoric occurrences of il' whose type is Task, 'anaphora resolution system' whose type is Method, 'precision rate' whose type is Metric, 'ILIMP' whose type is Method, 'tasks' whose type is Generic, 'method' whose type is Generic, 'ILIMP' whose type is Method, 'ILIMP' whose type is Method, 'modular  syntactic analysis system' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper proposes a generic  mathematical formalism  for the combination of various  structures  :  strings  ,  trees  ,  dags  ,  graphs  , and products of them. The  polarization  of the objects of the  elementary structures  controls the  saturation  of the final  structure  . This formalism is both elementary and powerful enough to strongly simulate many  grammar formalisms  , such as  rewriting systems  ,  dependency grammars  ,  TAG  ,  HPSG  and  LFG  . ", "output": "The abstract text contains these key words below: 'mathematical formalism' whose type is Method, 'structures' whose type is Generic, 'strings' whose type is OtherScientificTerm, 'trees' whose type is OtherScientificTerm, 'dags' whose type is OtherScientificTerm, 'graphs' whose type is OtherScientificTerm, 'them' whose type is Generic, 'polarization' whose type is OtherScientificTerm, 'elementary structures' whose type is OtherScientificTerm, 'formalism' whose type is Generic, 'grammar formalisms' whose type is Method, 'rewriting systems' whose type is Method, 'dependency grammars' whose type is Method, 'TAG' whose type is OtherScientificTerm, 'HPSG' whose type is OtherScientificTerm, 'LFG' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "isually-guided arm reaching movements are produced by distributed neural networks within parietal and frontal regions of the cerebral cortex. Experimental data indicate that (I) single neurons in these regions are broadly tuned to parameters of movement; (2) appropriate commands are elaborated by populations of neurons; (3) the coordinated action of neu-rons can be visualized using a neuronal population vector (NPV). However , the NPV provides only a rough estimate of movement parameters (direction, velocity) and may even fail to reflect the parameters of movement when arm posture is changed. We designed a model of the cortical motor command to investigate the relation between the desired direction of the movement, the actual direction of movement and the direction of the NPV in motor cortex. The model is a two-layer self-organizing neural network which combines broadly-tuned (muscular) proprioceptive and (cartesian) visual information to calculate (angular) motor commands for the initial part of the movement of a two-link arm. The network was trained by motor babbling in 5 positions. Simulations showed that (1) the network produced appropriate movement direction over a large part of the workspace; (2) small deviations of the actual trajectory from the desired trajectory existed at the extremities of the workspace; (3) these deviations were accompanied by large deviations of the NPV from both trajectories. These results suggest the NPV does not give a faithful image of cortical processing during arm reaching movements.", "output": "The abstract text contains these key words below: 'Visually-guided arm reaching movements' whose type is OtherScientificTerm, 'distributed neural networks' whose type is Method, 'coordinated action of neu-rons' whose type is OtherScientificTerm, 'neuronal population vector (NPV)' whose type is OtherScientificTerm, 'NPV' whose type is OtherScientificTerm, 'arm posture' whose type is OtherScientificTerm, 'model' whose type is Generic, 'cortical motor command' whose type is OtherScientificTerm, 'NPV' whose type is OtherScientificTerm, 'motor cortex' whose type is OtherScientificTerm, 'model' whose type is Generic, 'two-layer self-organizing neural network' whose type is Method, 'broadly-tuned (muscular) proprioceptive' whose type is OtherScientificTerm, '(cartesian) visual information' whose type is OtherScientificTerm, '(angular) motor commands' whose type is OtherScientificTerm, 'two-link arm' whose type is OtherScientificTerm, 'network' whose type is Generic, 'network' whose type is Generic, 'NPV' whose type is OtherScientificTerm, 'NPV' whose type is OtherScientificTerm, 'image of cortical processing' whose type is OtherScientificTerm, 'arm reaching movements' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "elping end users build and check process models is a challenge for many science and engineering fields. Many AI researchers have investigated useful ways of verifying and validating knowledge bases for ontologies and rules, but it is not easy to directly apply them to checking process models. Other techniques developed for checking and refining planning knowledge tend to focus on automated plan generation rather than helping users author process information. In this paper, we propose a complementary approach which helps users author and check process models. Our system, called KANAL, relates pieces of information in process models among themselves and to the existing KB, analyzing how different pieces of input are put together to achieve some effect. It builds interdepen-dency models from this analysis and uses them to find errors and propose fixes. Our initial evaluation shows that KANAL was able to find most of the errors in the process models and suggest useful fixes including the fixes that directly point to the sources of the errors.", "output": "The abstract text contains these key words below: 'process models' whose type is Method, 'science and engineering fields' whose type is Task, 'knowledge bases' whose type is Material, 'ontologies' whose type is OtherScientificTerm, 'rules' whose type is OtherScientificTerm, 'checking process models' whose type is Task, 'process models' whose type is Method, 'checking and refining planning knowledge' whose type is Task, 'automated plan generation' whose type is Task, 'complementary approach' whose type is Generic, 'process models' whose type is Method, 'system' whose type is Generic, 'KANAL' whose type is Method, 'process models' whose type is Method, 'KB' whose type is Material, 'It' whose type is Generic, 'interdepen-dency models' whose type is Method, 'them' whose type is Generic, 'errors' whose type is OtherScientificTerm, 'fixes' whose type is OtherScientificTerm, 'KANAL' whose type is Method, 'process models' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we explore multilingual feature-level data sharing via Deep Neural Network (DNN) stacked bottleneck features. Given a set of available source languages, we apply language identification to pick the language most similar to the target language , for more efficient use of multilingual resources. Our experiments with IARPA-Babel languages show that bottleneck features trained on the most similar source language perform better than those trained on all available source languages. Further analysis suggests that only data similar to the target language is useful for multilingual training.", "output": "The abstract text contains these key words below: 'multilingual feature-level data sharing' whose type is Task, 'Deep Neural Network (DNN) stacked bottleneck features' whose type is Method, 'language identification' whose type is Task, 'multilingual resources' whose type is Material, 'IARPA-Babel languages' whose type is Material, 'bottleneck features' whose type is OtherScientificTerm, 'those' whose type is Generic, 'data' whose type is Generic, 'multilingual training' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n In this paper  discourse segments  are defined and a method for  discourse segmentation  primarily based on  abduction  of  temporal relations  between  segments  is proposed. This method is precise and  computationally feasible  and is supported by previous work in the area of  temporal anaphora resolution  . ", "output": "The abstract text contains these key words below: 'discourse segments' whose type is OtherScientificTerm, 'method' whose type is Generic, 'discourse segmentation' whose type is Task, 'abduction  of  temporal relations' whose type is Task, 'method' whose type is Generic, 'temporal anaphora resolution' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper a system which understands and conceptualizes  scenes descriptions in natural language  is presented. Specifically, the following components of the system are described: the  syntactic analyzer , based on a  Procedural Systemic Grammar , the  semantic analyzer  relying on the  Conceptual Dependency Theory , and the  dictionary . ", "output": "The abstract text contains these key words below: 'system' whose type is Generic, 'scenes descriptions in natural language' whose type is Task, 'components' whose type is Generic, 'system' whose type is Generic, 'syntactic analyzer' whose type is Method, 'Procedural Systemic Grammar' whose type is Method, 'semantic analyzer' whose type is Method, 'Conceptual Dependency Theory' whose type is Method, 'dictionary' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents a  machine learning approach  to  bare slice disambiguation  in  dialogue . We extract a set of  heuristic principles  from a  corpus-based sample  and formulate them as  probabilistic Horn clauses . We then use the predicates of such  clauses  to create a set of  domain independent features  to annotate an  input dataset , and run two different  machine learning algorithms  : SLIPPER, a  rule-based learning algorithm , and TiMBL, a  memory-based system . Both learners perform well, yielding similar  success rates  of approx 90%. The results show that the  features  in terms of which we formulate our  heuristic principles  have significant predictive power, and that  rules  that closely resemble our  Horn clauses  can be learnt automatically from these  features .\n", "output": "The abstract text contains these key words below: 'machine learning approach' whose type is Method, 'bare slice disambiguation' whose type is Task, 'dialogue' whose type is Material, 'heuristic principles' whose type is Method, 'corpus-based sample' whose type is Material, 'probabilistic Horn clauses' whose type is OtherScientificTerm, 'clauses' whose type is OtherScientificTerm, 'domain independent features' whose type is OtherScientificTerm, 'machine learning algorithms' whose type is Method, 'SLIPPER' whose type is Generic, 'rule-based learning algorithm' whose type is Method, 'TiMBL' whose type is Generic, 'memory-based system' whose type is Method, 'success rates' whose type is Metric, 'features' whose type is OtherScientificTerm, 'heuristic principles' whose type is Method, 'rules' whose type is OtherScientificTerm, 'Horn clauses' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a  practically unsupervised learning method  to produce  single-snippet answers  to  definition questions  in  question answering systems  that supplement  Web search engines . The method exploits  on-line encyclopedias and dictionaries  to generate automatically an arbitrarily large number of  positive and negative definition examples , which are then used to train an  svm  to separate the two classes. We show experimentally that the proposed method is viable, that it outperforms the alternative of training the  system  on  questions  and  news articles from trec , and that it helps the  search engine  handle  definition questions  significantly better.\n", "output": "The abstract text contains these key words below: 'unsupervised learning method' whose type is Method, 'single-snippet answers' whose type is OtherScientificTerm, 'question answering systems' whose type is Method, 'Web search engines' whose type is Method, 'method' whose type is Generic, 'on-line encyclopedias and dictionaries' whose type is Material, 'positive and negative definition examples' whose type is Material, 'svm' whose type is Method, 'method' whose type is Generic, 'it' whose type is Generic, 'alternative' whose type is Generic, 'system' whose type is Generic, 'news articles' whose type is Material, 'trec' whose type is Material, 'it' whose type is Generic, 'search engine' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Porting a  Natural Language Processing (NLP) system  to a  new domain  remains one of the bottlenecks in  syntactic parsing , because of the amount of effort required to fix gaps in the  lexicon , and to attune the  existing grammar  to the idiosyncracies of the  new sublanguage . This paper shows how the process of fitting a  lexicalized grammar  to a  domain  can be automated to a great extent by using a  hybrid system  that combines  traditional knowledge-based techniques  with a  corpus-based approach .\n", "output": "The abstract text contains these key words below: 'Natural Language Processing (NLP) system' whose type is Task, 'new domain' whose type is Material, 'syntactic parsing' whose type is Task, 'grammar' whose type is Method, 'idiosyncracies of the  new sublanguage' whose type is OtherScientificTerm, 'lexicalized grammar' whose type is Method, 'hybrid system' whose type is Method, 'knowledge-based techniques' whose type is Method, 'corpus-based approach' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "nformation distillation aims to extract relevant pieces of information related to a given query from massive, possibly multilingual, audio and textual document sources. In this paper , we present our approach for using information extraction annotations to augment document retrieval for distillation. We take advantage of the fact that some of the distillation queries can be associated with annotation elements introduced for the NIST Automatic Content Extraction (ACE) task. We experimentally show that using the ACE events to constrain the document set returned by an information retrieval engine significantly improves the precision at various recall rates for two different query templates.", "output": "The abstract text contains these key words below: 'Information distillation' whose type is Task, 'massive, possibly multilingual, audio and textual document sources' whose type is Material, 'information extraction annotations' whose type is OtherScientificTerm, 'document retrieval for distillation' whose type is Task, 'distillation queries' whose type is OtherScientificTerm, 'annotation elements' whose type is OtherScientificTerm, 'NIST Automatic Content Extraction (ACE) task' whose type is Task, 'ACE events' whose type is OtherScientificTerm, 'information retrieval engine' whose type is Method, 'precision' whose type is Metric, 'recall rates' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n State-of-the-art  Question Answering (QA) systems  are very sensitive to variations in the phrasing of an  information need  . Finding the preferred  language  for such a  need  is a valuable task. We investigate that claim by adopting a simple  MT-based paraphrasing technique  and evaluating  QA system  performance on  paraphrased questions  . We found a potential increase of 35% in  MRR  with respect to the original  question  . ", "output": "The abstract text contains these key words below: 'Question Answering (QA) systems' whose type is Method, 'MT-based paraphrasing technique' whose type is Method, 'QA system' whose type is Method, 'paraphrased questions' whose type is Material, 'MRR' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents a new approach to  statistical sentence generation  in which alternative  phrases  are represented as packed sets of  trees , or  forests , and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent  syntactic information . It also facilitates more efficient  statistical ranking  than a previous approach to  statistical generation . An efficient  ranking algorithm  is described, together with experimental results showing significant improvements over simple enumeration or a  lattice-based approach .\n", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'statistical sentence generation' whose type is Task, 'trees' whose type is OtherScientificTerm, 'forests' whose type is OtherScientificTerm, 'representation' whose type is Generic, 'syntactic information' whose type is OtherScientificTerm, 'It' whose type is Generic, 'statistical ranking' whose type is Method, 'approach' whose type is Generic, 'statistical generation' whose type is Task, 'ranking algorithm' whose type is Method, 'enumeration' whose type is Method, 'lattice-based approach' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This article is devoted to the problem of  quantifying noun groups  in  German . After a thorough description of the phenomena, the results of  corpus-based investigations  are described. Moreover, some examples are given that underline the necessity of integrating some kind of information other than  grammar sensu stricto  into the  treebank . We argue that a more sophisticated and fine-grained  annotation  in the  tree-bank  would have very positve effects on  stochastic parsers  trained on the  tree-bank  and on  grammars  induced from the  treebank , and it would make the  treebank  more valuable as a  source of data  for  theoretical linguistic investigations . The information gained from  corpus research  and the analyses that are proposed are realized in the framework of  SILVA , a  parsing  and  extraction tool  for  German text corpora .\n", "output": "The abstract text contains these key words below: 'quantifying noun groups  in  German' whose type is Task, 'German' whose type is Material, 'grammar sensu stricto' whose type is OtherScientificTerm, 'treebank' whose type is Material, 'fine-grained  annotation' whose type is OtherScientificTerm, 'tree-bank' whose type is Material, 'stochastic parsers' whose type is Method, 'tree-bank' whose type is Material, 'grammars' whose type is Method, 'treebank' whose type is Material, 'treebank' whose type is Material, 'theoretical linguistic investigations' whose type is Task, 'SILVA' whose type is Method, 'parsing  and  extraction tool' whose type is Method, 'German text corpora' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we present an  unlexicalized parser  for  German  which employs  smoothing  and  suffix analysis  to achieve a  labelled bracket F-score  of 76.2, higher than previously reported results on the  NEGRA corpus . In addition to the high  accuracy  of the model, the use of  smoothing  in an  unlexicalized parser  allows us to better examine the interplay between  smoothing  and  parsing  results.\n", "output": "The abstract text contains these key words below: 'unlexicalized parser' whose type is Method, 'German' whose type is Material, 'smoothing' whose type is Method, 'suffix analysis' whose type is Method, 'labelled bracket F-score' whose type is Metric, 'NEGRA corpus' whose type is Material, 'accuracy' whose type is Metric, 'model' whose type is Generic, 'smoothing' whose type is Method, 'unlexicalized parser' whose type is Method, 'smoothing' whose type is Method, 'parsing' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper proposes  document oriented preference sets(DoPS)  for the disambiguation of the  dependency structure  of  sentences  . The  DoPS system  extracts preference knowledge from a  target document  or other  documents  automatically.  Sentence ambiguities  can be resolved by using domain targeted preference knowledge without using complicated large  knowledgebases  .  Implementation  and  empirical results  are described for the the analysis of  dependency structures  of  Japanese patent claim sentences  . ", "output": "The abstract text contains these key words below: 'document oriented preference sets(DoPS)' whose type is Method, 'disambiguation of the  dependency structure' whose type is Task, 'DoPS system' whose type is Method, 'Sentence ambiguities' whose type is OtherScientificTerm, 'domain targeted preference knowledge' whose type is OtherScientificTerm, 'knowledgebases' whose type is Material, 'dependency structures' whose type is OtherScientificTerm, 'Japanese patent claim sentences' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper we investigate the simple logical properties of contexts. We describe both the syntax and semantics of a general propositional language of context, and give a Hilbert style proof system for this language. A propositional logic of context extends classical propositional logic in two ways. Firstly, a new modality, ist(;), is introduced. It is used to express that the sentence, , holds in the context. Secondly, each context has its own vocabulary, i.e. a set of propositional atoms which are deened or meaningful in that context. The main results of this paper are the sound-ness and completeness of this Hilbert style proof system. We also provide soundness and completeness results (i.e. correspondence theory) for various extensions of the general system.", "output": "The abstract text contains these key words below: 'logical properties of contexts' whose type is Task, 'syntax' whose type is OtherScientificTerm, 'semantics' whose type is OtherScientificTerm, 'propositional language of context' whose type is OtherScientificTerm, 'Hilbert style proof system' whose type is Method, 'language' whose type is Generic, 'propositional logic of context' whose type is OtherScientificTerm, 'classical propositional logic' whose type is OtherScientificTerm, 'modality' whose type is OtherScientificTerm, 'It' whose type is Generic, 'Hilbert style proof system' whose type is Method, 'correspondence theory' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "oint image filters can leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or enhancing spatial resolution. Existing methods rely on various kinds of explicit filter construction or hand-designed objective functions. It is thus difficult to understand, improve, and accelerate them in a coherent framework. In this paper, we propose a learning-based approach to construct a joint filter based on Convolution-al Neural Networks. In contrast to existing methods that consider only the guidance image, our method can selectively transfer salient structures that are consistent in both guidance and target images. We show that the model trained on a certain type of data, e.g., RGB and depth images, generalizes well for other modalities, e.g., Flash/Non-Flash and RGB/NIR images. We validate the effectiveness of the proposed joint filter through extensive comparisons with state-of-the-art methods.", "output": "The abstract text contains these key words below: 'Joint image filters' whose type is Method, 'guidance image' whose type is Material, 'guidance image' whose type is Material, 'suppressing noise' whose type is Task, 'enhancing spatial resolution' whose type is Task, 'Existing methods' whose type is Generic, 'explicit filter construction' whose type is Method, 'hand-designed objective functions' whose type is Method, 'them' whose type is Generic, 'coherent framework' whose type is Generic, 'learning-based approach' whose type is Method, 'joint filter' whose type is Method, 'Convolution-al Neural Networks' whose type is Method, 'methods' whose type is Generic, 'guidance image' whose type is Material, 'method' whose type is Generic, 'transfer salient structures' whose type is Task, 'model' whose type is Generic, 'data' whose type is Generic, 'RGB and depth images' whose type is Material, 'modalities' whose type is Generic, 'Flash/Non-Flash and RGB/NIR images' whose type is Material, 'joint filter' whose type is Method, 'state-of-the-art methods' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "A novel  bootstrapping approach  to  Named Entity (NE) tagging  using  concept-based seeds  and  successive learners  is presented. This approach only requires a few  common noun  or  pronoun   seeds  that correspond to the  concept  for the targeted  NE  , e.g. he/she/man/woman for  PERSON NE  . The  bootstrapping procedure  is implemented as training two  successive learners  . First,  decision list  is used to learn the  parsing-based NE rules  . Then, a  Hidden Markov Model  is trained on a  corpus  automatically tagged by the first  learner  . The resulting  NE system  approaches  supervised NE  performance for some  NE types  . ", "output": "The abstract text contains these key words below: 'bootstrapping approach' whose type is Method, 'Named Entity (NE) tagging' whose type is Task, 'concept-based seeds' whose type is OtherScientificTerm, 'successive learners' whose type is Method, 'approach' whose type is Generic, 'NE' whose type is OtherScientificTerm, 'PERSON NE' whose type is OtherScientificTerm, 'bootstrapping procedure' whose type is Method, 'successive learners' whose type is Method, 'decision list' whose type is OtherScientificTerm, 'parsing-based NE rules' whose type is OtherScientificTerm, 'Hidden Markov Model' whose type is Method, 'learner' whose type is Generic, 'NE system' whose type is Method, 'supervised NE' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n The goal of this work is the enrichment of  human-machine interactions  in a  natural language environment  . Because a  speaker  and  listener  cannot be assured to have the same  beliefs  ,  contexts  ,  perceptions  ,  backgrounds  , or  goals  , at each point in a  conversation  , difficulties and mistakes arise when a  listener  interprets a  speaker's utterance  . These mistakes can lead to various kinds of misunderstandings between  speaker  and  listener  , including  reference failures  or failure to understand the  speaker's intention  . We call these misunderstandings  miscommunication  . Such mistakes can slow, and possibly break down,  communication  . Our goal is to recognize and isolate such  miscommunications  and circumvent them. This paper highlights a particular class of  miscommunication  ---  reference problems  --- by describing a case study and techniques for avoiding  failures of reference  . We want to illustrate a framework less restrictive than earlier ones by allowing a  speaker  leeway in forming an  utterance  about a task and in determining the conversational vehicle to deliver it. The paper also promotes a new view for  extensional reference  . ", "output": "The abstract text contains these key words below: 'human-machine interactions' whose type is Task, 'natural language environment' whose type is OtherScientificTerm, 'reference failures' whose type is OtherScientificTerm, 'speaker's intention' whose type is OtherScientificTerm, 'miscommunication' whose type is Task, 'miscommunications' whose type is Task, 'them' whose type is Generic, 'miscommunication' whose type is Task, 'reference problems' whose type is Task, 'techniques' whose type is Generic, 'failures of reference' whose type is Task, 'extensional reference' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "uring late-2013 through early-2014 NIST coordinated a special i-vector challenge based on data used in previous NIST Speaker Recognition Evaluations (SREs). Unlike evaluations in the SRE series, the i-vector challenge was run entirely online and used fixed-length feature vectors projected into a low-dimensional space (i-vectors) rather than audio recordings. These changes made the challenge more readily accessible, especially to participants from outside the audio processing field. Compared to the 2012 SRE, the i-vector challenge saw an increase in the number of participants by nearly a factor of two, and a two orders of magnitude increase in the number of systems submitted for evaluation. Initial results indicate the leading system achieved an approximate 37% improvement relative to the baseline system.", "output": "The abstract text contains these key words below: 'i-vector challenge' whose type is Material, 'NIST Speaker Recognition Evaluations (SREs)' whose type is Material, 'SRE series' whose type is Material, 'i-vector challenge' whose type is Material, 'fixed-length feature vectors' whose type is OtherScientificTerm, 'low-dimensional space (i-vectors)' whose type is OtherScientificTerm, 'audio recordings' whose type is Material, 'audio processing field' whose type is OtherScientificTerm, 'SRE' whose type is Material, 'i-vector challenge' whose type is Material, 'leading system' whose type is Generic, 'baseline system' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "INPRAN, a new robust operator, nds good ts in data sets where more than 50% of the points are outliers. Unlike other techniques that handle large outlier percentages, MINPRAN does not rely on a known error bound for the good data. Instead it assumes that the bad data are randomly (uniformly) distributed within the dynamic range of the sensor. Based on this, MINPRAN uses random sampling to search for the t and the number of inliers to the t that are least likely to have occurred randomly. It runs in time O(N 2 + SN log N), where S is the number of random samples and N is the number of data points. We demonstrate analytically that MINPRAN distinguishes good ts from ts to random data, and that MINPRAN nds accurate ts and nearly the correct number of inliers, regardless of the percentage of true inliers. MINPRAN's properties are connrmed experimentally on synthetic data and compare favorably to least median of squares. Related work applies MINPRAN to complex range and intensity data 23].", "output": "The abstract text contains these key words below: 'MINPRAN' whose type is Method, 'robust operator' whose type is Method, 'techniques' whose type is Generic, 'large outlier percentages' whose type is Metric, 'MINPRAN' whose type is Method, 'error bound' whose type is OtherScientificTerm, 'it' whose type is Generic, 'dynamic range of the sensor' whose type is OtherScientificTerm, 'MINPRAN' whose type is Method, 'random sampling' whose type is Method, 'It' whose type is Generic, 'MINPRAN' whose type is Method, 'MINPRAN' whose type is Method, 'percentage of true inliers' whose type is Metric, 'MINPRAN' whose type is Method, 'synthetic data' whose type is Material, 'least median of squares' whose type is Method, 'MINPRAN' whose type is Method, 'complex range' whose type is OtherScientificTerm, 'intensity data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Most state-of-the-art  evaluation measures  for  machine translation  assign high  costs  to movements of  word  blocks. In many cases though such movements still result in correct or almost correct  sentences  . In this paper, we will present a new  evaluation measure  which explicitly models  block reordering  as an  edit operation  . Our  measure  can be exactly calculated in  quadratic time  . Furthermore, we will show how some  evaluation measures  can be improved by the introduction of  word-dependent substitution costs  . The correlation of the new  measure  with  human judgment  has been investigated systematically on two different  language pairs  . The experimental results will show that it significantly outperforms state-of-the-art approaches in  sentence-level correlation  . Results from experiments with  word dependent substitution costs  will demonstrate an additional increase of correlation between  automatic evaluation measures  and  human judgment  . ", "output": "The abstract text contains these key words below: 'evaluation measures' whose type is Metric, 'machine translation' whose type is Task, 'evaluation measure' whose type is Metric, 'block reordering' whose type is OtherScientificTerm, 'edit operation' whose type is OtherScientificTerm, 'measure' whose type is Generic, 'quadratic time' whose type is OtherScientificTerm, 'evaluation measures' whose type is Metric, 'word-dependent substitution costs' whose type is OtherScientificTerm, 'measure' whose type is Generic, 'human judgment' whose type is OtherScientificTerm, 'language pairs' whose type is Material, 'it' whose type is Generic, 'approaches' whose type is Generic, 'sentence-level correlation' whose type is Metric, 'word dependent substitution costs' whose type is OtherScientificTerm, 'automatic evaluation measures' whose type is Metric, 'human judgment' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper proposes a method for learning joint embed-dings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large-margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase lo-calization on the Flickr30K Entities dataset.", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'joint embed-dings of images and text' whose type is OtherScientificTerm, 'two-branch neural network' whose type is Method, 'multiple layers of linear projections' whose type is OtherScientificTerm, 'nonlinearities' whose type is OtherScientificTerm, 'network' whose type is Generic, 'large-margin objective' whose type is OtherScientificTerm, 'cross-view ranking constraints' whose type is OtherScientificTerm, 'within-view neighborhood structure preservation constraints' whose type is OtherScientificTerm, 'metric learning literature' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'accuracy' whose type is Metric, 'image-to-text and text-to-image retrieval' whose type is Task, 'method' whose type is Generic, 'Flickr30K and MSCOCO image-sentence datasets' whose type is Material, 'phrase lo-calization' whose type is Task, 'Flickr30K Entities dataset' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n In this paper, we describe the  pronominal anaphora resolution module  of  Lucy  , a portable  English understanding system  . The design of this module was motivated by the observation that, although there exist many theories of  anaphora resolution  , no one of these theories is complete. Thus we have implemented a  blackboard-like architecture  in which individual  partial theories  can be encoded as separate modules that can interact to propose candidate  antecedents  and to evaluate each other's proposals. ", "output": "The abstract text contains these key words below: 'pronominal anaphora resolution module' whose type is Method, 'Lucy' whose type is Method, 'English understanding system' whose type is Method, 'module' whose type is Generic, 'anaphora resolution' whose type is Task, 'blackboard-like architecture' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "hen classifying high-dimensional sequence data, traditional methods (e.g., HMMs, CRFs) may require large amounts of training data to avoid overfitting. In such cases dimensionality reduction can be employed to find a low-dimensional representation on which classification can be done more efficiently. Existing methods for supervised dimensionality reduction often presume that the data is densely sampled so that a neighborhood graph structure can be formed, or that the data arises from a known distribution. Sufficient dimension reduction techniques aim to find a low dimensional representation such that the remaining degrees of freedom become conditionally independent of the output values. In this paper we develop a novel sequence kernel dimension reduction approach (S-KDR). Our approach does not make strong assumptions on the distribution of the input data. Spatial, temporal and periodic information is combined in a principled manner, and an optimal manifold is learned for the end-task. We demonstrate the effectiveness of our approach on several tasks involving the discrimination of human gesture and motion categories, as well as on a database of dynamic textures.", "output": "The abstract text contains these key words below: 'classifying high-dimensional sequence data' whose type is Task, 'HMMs' whose type is Method, 'CRFs' whose type is Method, 'overfitting' whose type is OtherScientificTerm, 'dimensionality reduction' whose type is Method, 'low-dimensional representation' whose type is Method, 'classification' whose type is Task, 'Existing methods' whose type is Generic, 'supervised dimensionality reduction' whose type is Task, 'neighborhood graph structure' whose type is OtherScientificTerm, 'known distribution' whose type is OtherScientificTerm, 'Sufficient dimension reduction techniques' whose type is Method, 'low dimensional representation' whose type is Method, 'sequence kernel dimension reduction approach (S-KDR)' whose type is Method, 'approach' whose type is Generic, 'Spatial, temporal and periodic information' whose type is OtherScientificTerm, 'manifold' whose type is OtherScientificTerm, 'end-task' whose type is Generic, 'approach' whose type is Generic, 'discrimination of human gesture and motion categories' whose type is Task, 'database of dynamic textures' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We investigate the  verbal and nonverbal means  for  grounding  , and propose a design for  embodied conversational agents  that relies on both kinds of  signals  to establish  common ground  in  human-computer interaction  . We analyzed  eye gaze  ,  head nods  and  attentional focus  in the context of a  direction-giving task  . The distribution of  nonverbal behaviors  differed depending on the type of  dialogue move  being grounded, and the overall pattern reflected a monitoring of lack of  negative feedback  . Based on these results, we present an  ECA  that uses  verbal and nonverbal grounding acts  to update  dialogue state  . ", "output": "The abstract text contains these key words below: 'verbal and nonverbal means' whose type is Method, 'grounding' whose type is Task, 'design' whose type is Generic, 'embodied conversational agents' whose type is Method, 'common ground' whose type is Task, 'human-computer interaction' whose type is Task, 'eye gaze' whose type is OtherScientificTerm, 'head nods' whose type is OtherScientificTerm, 'attentional focus' whose type is OtherScientificTerm, 'direction-giving task' whose type is Task, 'nonverbal behaviors' whose type is OtherScientificTerm, 'dialogue move' whose type is OtherScientificTerm, 'negative feedback' whose type is OtherScientificTerm, 'ECA' whose type is Method, 'verbal and nonverbal grounding acts' whose type is OtherScientificTerm, 'dialogue state' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " \" graphics for vision \" approach is proposed to address the problem of reconstruction from a large and imperfect data set: reconstruction on demand by tensor voting, or ROD-TV. ROD-TV simultaneously delivers good efficiency and robust-ness, by adapting to a continuum of primitive connectivity, view dependence, and levels of detail (LOD). Locally inferred surface elements are robust to noise and better capture local shapes. By inferring per-vertex normals at sub-voxel precision on the fly, we can achieve interpolative shading. Since these missing details can be recovered at the current level of detail, our result is not upper bounded by the scanning resolution. By relaxing the mesh connectivity requirement, we extend ROD-TV and propose a simple but effective multiscale feature extraction algorithm. ROD-TV consists of a hierarchical data structure that encodes different levels of detail. The local reconstruction algorithm is tensor voting. It is applied on demand to the visible subset of data at a desired level of detail , by traversing the data hierarchy and collecting tensorial support in a neighborhood. We compare our approach and present encouraging results.", "output": "The abstract text contains these key words below: '\" graphics for vision \" approach' whose type is Method, 'reconstruction' whose type is Task, 'large and imperfect data set' whose type is Material, 'reconstruction' whose type is Task, 'tensor voting' whose type is Method, 'ROD-TV' whose type is Method, 'ROD-TV' whose type is Method, 'efficiency' whose type is Metric, 'robust-ness' whose type is Metric, 'primitive connectivity' whose type is OtherScientificTerm, 'view dependence' whose type is OtherScientificTerm, 'levels of detail (LOD)' whose type is OtherScientificTerm, 'Locally inferred surface elements' whose type is OtherScientificTerm, 'noise' whose type is OtherScientificTerm, 'local shapes' whose type is OtherScientificTerm, 'per-vertex normals' whose type is OtherScientificTerm, 'sub-voxel precision' whose type is Metric, 'interpolative shading' whose type is Task, 'scanning resolution' whose type is OtherScientificTerm, 'mesh connectivity requirement' whose type is OtherScientificTerm, 'ROD-TV' whose type is Method, 'multiscale feature extraction algorithm' whose type is Method, 'ROD-TV' whose type is Method, 'hierarchical data structure' whose type is Method, 'local reconstruction algorithm' whose type is Method, 'tensor voting' whose type is Method, 'It' whose type is Generic, 'traversing the data hierarchy' whose type is Method, 'collecting tensorial support' whose type is Method, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We propose a method of organizing reading materials for  vocabulary learning . It enables us to select a concise set of reading  texts  (from a  target corpus ) that contains all the  target vocabulary  to be learned. We used a specialized  vocabulary  for an English certification test as the  target vocabulary  and used  English Wikipedia , a free-content encyclopedia, as the  target corpus . The organized reading materials would enable learners not only to study the  target vocabulary  efficiently but also to gain a variety of knowledge through reading. The reading materials are available on our web site.\n", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'organizing reading materials' whose type is Task, 'vocabulary learning' whose type is Task, 'It' whose type is Generic, 'target vocabulary' whose type is OtherScientificTerm, 'English Wikipedia' whose type is Material, 'free-content encyclopedia' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we state the challenges of high-level program execution in multi-agent settings. We first introduce high-level program execution and the related work. Then we describe the completed work, the future work and its approaches. We conclude with the expected contributions of our research.", "output": "The abstract text contains these key words below: 'high-level program execution' whose type is Task, 'high-level program execution' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Syntax-based statistical machine translation (MT)  aims at applying  statistical models  to  structured data  . In this paper, we present a  syntax-based statistical machine translation system  based on a  probabilistic synchronous dependency insertion grammar  .  Synchronous dependency insertion grammars  are a version of  synchronous grammars  defined on  dependency trees  . We first introduce our approach to inducing such a  grammar  from  parallel corpora  . Second, we describe the  graphical model  for the  machine translation task  , which can also be viewed as a  stochastic tree-to-tree transducer  . We introduce a  polynomial time decoding algorithm  for the  model  . We evaluate the outputs of our  MT system  using the  NIST and Bleu automatic MT evaluation software  . The result shows that our system outperforms the  baseline system  based on the  IBM models  in both  translation speed and quality  . ", "output": "The abstract text contains these key words below: 'Syntax-based statistical machine translation (MT)' whose type is Task, 'statistical models' whose type is Method, 'structured data' whose type is Material, 'syntax-based statistical machine translation system' whose type is Task, 'probabilistic synchronous dependency insertion grammar' whose type is Method, 'Synchronous dependency insertion grammars' whose type is Method, 'synchronous grammars' whose type is Method, 'dependency trees' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'grammar' whose type is Method, 'parallel corpora' whose type is Material, 'graphical model' whose type is Method, 'machine translation task' whose type is Task, 'stochastic tree-to-tree transducer' whose type is Method, 'polynomial time decoding algorithm' whose type is Method, 'model' whose type is Generic, 'MT system' whose type is Method, 'NIST and Bleu automatic MT evaluation software' whose type is Metric, 'system' whose type is Generic, 'baseline system' whose type is Generic, 'IBM models' whose type is Method, 'translation speed and quality' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "epresenting images with layers has many important applications , such as video compression, motion analysis, and 3D scene analysis. This paper presents an approach to reliably extracting layers from images by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace. Layers in the input images will be mapped in the subspace, where it is proven that they form well-defined clusters and can be reliably identified by a simple mean-shift based clustering algorithm. Global optimality is achieved since all valid regions are simultaneously taken into account, and noise can be effectively reduced by enforcing the subspace constraint. Good layer descriptions are shown to be extracted in the experimental results.", "output": "The abstract text contains these key words below: 'Representing images with layers' whose type is Method, 'applications' whose type is Generic, 'video compression' whose type is Task, 'motion analysis' whose type is Task, '3D scene analysis' whose type is Task, 'approach' whose type is Generic, 'layers' whose type is OtherScientificTerm, 'images' whose type is Material, 'homographies' whose type is OtherScientificTerm, 'planar patches' whose type is OtherScientificTerm, 'scene' whose type is OtherScientificTerm, 'low dimensional linear subspace' whose type is OtherScientificTerm, 'Layers' whose type is OtherScientificTerm, 'images' whose type is Material, 'subspace' whose type is OtherScientificTerm, 'clusters' whose type is OtherScientificTerm, 'mean-shift based clustering algorithm' whose type is Method, 'Global optimality' whose type is Task, 'regions' whose type is OtherScientificTerm, 'noise' whose type is OtherScientificTerm, 'subspace constraint' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper describes an  unsupervised learning method  for  associative relationships between verb phrases , which is important in developing reliable  Q&A systems . Consider the situation that a user gives a  query  \"How much petrol was imported to Japan from Saudi Arabia?\" to a  Q&A system , but the  text  given to the system includes only the  description  \"X tonnes of petrol was conveyed to Japan from Saudi Arabia\". We think that the  description  is a good clue to find the answer for our  query , \"X tonnes\". But there is no  large-scale database  that provides the  associative relationship  between \"imported\" and \"conveyed\". Our aim is to develop an  unsupervised learning method  that can obtain such an  associative relationship , which we call  scenario consistency . The method we are currently working on uses an  expectation-maximization (EM) based word-clustering algorithm , and we have evaluated the effectiveness of this method using  Japanese verb phrases . \n", "output": "The abstract text contains these key words below: 'unsupervised learning method' whose type is Method, 'associative relationships between verb phrases' whose type is OtherScientificTerm, 'Q&A systems' whose type is Task, 'Q&A system' whose type is Task, 'large-scale database' whose type is Material, 'associative relationship' whose type is OtherScientificTerm, 'unsupervised learning method' whose type is Method, 'associative relationship' whose type is OtherScientificTerm, 'scenario consistency' whose type is OtherScientificTerm, 'method' whose type is Generic, 'expectation-maximization (EM) based word-clustering algorithm' whose type is Method, 'method' whose type is Generic, 'Japanese verb phrases' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper a novel solution to automatic and  unsupervised word sense induction (WSI)  is introduced. It represents an instantiation of the  one sense per collocation observation  (Gale et al., 1992). Like most existing approaches it utilizes  clustering of word co-occurrences  . This approach differs from other approaches to  WSI  in that it enhances the effect of the  one sense per collocation observation  by using triplets of  words  instead of pairs. The combination with a  two-step clustering process  using  sentence co-occurrences  as  features  allows for accurate results. Additionally, a novel and likewise automatic and  unsupervised evaluation method  inspired by Schutze's (1992) idea of evaluation of  word sense disambiguation algorithms  is employed. Offering advantages like reproducability and independency of a given biased  gold standard  it also enables  automatic parameter optimization  of the  WSI algorithm  . ", "output": "The abstract text contains these key words below: 'solution' whose type is Generic, 'automatic and  unsupervised word sense induction (WSI)' whose type is Task, 'It' whose type is Generic, 'one sense per collocation observation' whose type is Method, 'it' whose type is Generic, 'clustering of word co-occurrences' whose type is Method, 'approach' whose type is Generic, 'approaches' whose type is Generic, 'WSI' whose type is Task, 'it' whose type is Generic, 'one sense per collocation observation' whose type is Method, 'triplets of  words' whose type is OtherScientificTerm, 'two-step clustering process' whose type is Method, 'sentence co-occurrences' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'automatic and  unsupervised evaluation method' whose type is Metric, 'word sense disambiguation algorithms' whose type is Method, 'automatic parameter optimization' whose type is Method, 'WSI algorithm' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n Recent advances in  Automatic Speech Recognition technology  have put the goal of naturally sounding  dialog systems  within reach. However, the improved  speech recognition  has brought to light a new problem: as  dialog systems  understand more of what the  user  tells them, they need to be more sophisticated at responding to the  user  . The issue of  system response  to  users  has been extensively studied by the  natural language generation community  , though rarely in the context of  dialog systems  . We show how research in  generation  can be adapted to  dialog systems  , and how the high cost of hand-crafting  knowledge-based generation systems  can be overcome by employing  machine learning techniques  . ", "output": "The abstract text contains these key words below: 'Automatic Speech Recognition technology' whose type is Method, 'dialog systems' whose type is Task, 'speech recognition' whose type is Task, 'dialog systems' whose type is Task, 'they' whose type is Generic, 'system response' whose type is OtherScientificTerm, 'natural language generation community' whose type is Task, 'dialog systems' whose type is Task, 'generation' whose type is Task, 'dialog systems' whose type is Task, 'hand-crafting  knowledge-based generation systems' whose type is Method, 'machine learning techniques' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents an algorithm for selecting an appropriate  classifier word  for a  noun . In  Thai language , it frequently happens that there is fluctuation in the choice of  classifier  for a given  concrete noun , both from the point of view of the whole  speech community  and  individual speakers . Basically, there is no exact rule for  classifier selection . As far as we can do in the  rule-based approach  is to give a  default rule  to pick up a corresponding  classifier  of each  noun . Registration of  classifier  for each  noun  is limited to the  type of unit classifier  because other types are open due to the meaning of representation. We propose a  corpus-based method  (Biber,1993; Nagao,1993; Smadja,1993) which generates  Noun Classifier Associations (NCA)  to overcome the problems in  classifier assignment  and  semantic construction of noun phrase . The  NCA  is created statistically from a large  corpus  and recomposed under  concept hierarchy constraints  and  frequency of occurrences . \n", "output": "The abstract text contains these key words below: 'Thai language' whose type is Material, 'classifier' whose type is Method, 'classifier selection' whose type is Task, 'rule-based approach' whose type is Method, 'classifier' whose type is Method, 'classifier' whose type is Method, 'type of unit classifier' whose type is OtherScientificTerm, 'corpus-based method' whose type is Method, 'Noun Classifier Associations (NCA)' whose type is Method, 'classifier assignment' whose type is Task, 'semantic construction of noun phrase' whose type is Task, 'NCA' whose type is Method, 'concept hierarchy constraints' whose type is OtherScientificTerm, 'frequency of occurrences' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we present a novel  training method  for a  localized phrase-based prediction model  for  statistical machine translation (SMT)  . The  model  predicts  blocks  with orientation to handle  local phrase re-ordering  . We use a  maximum likelihood criterion  to train a  log-linear block bigram model  which uses  real-valued features  (e.g. a  language model score  ) as well as  binary features  based on the  block  identities themselves, e.g. block bigram features. Our  training algorithm  can easily handle millions of  features  . The best system obtains a 18.6% improvement over the  baseline  on a standard  Arabic-English translation task  . ", "output": "The abstract text contains these key words below: 'training method' whose type is Method, 'localized phrase-based prediction model' whose type is Method, 'statistical machine translation (SMT)' whose type is Task, 'model' whose type is Generic, 'local phrase re-ordering' whose type is Task, 'maximum likelihood criterion' whose type is OtherScientificTerm, 'log-linear block bigram model' whose type is Method, 'real-valued features' whose type is OtherScientificTerm, 'language model score' whose type is OtherScientificTerm, 'binary features' whose type is OtherScientificTerm, 'training algorithm' whose type is Method, 'features' whose type is OtherScientificTerm, 'system' whose type is Generic, 'baseline' whose type is Generic, 'Arabic-English translation task' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, a novel method to learn the intrinsic object structure for robust visual tracking is proposed. The basic assumption is that the parameterized object state lies on a low dimensional manifold and can be learned from training data. Based on this assumption, firstly we derived the dimensionality reduction and density estimation algorithm for unsupervised learning of object intrinsic representation, the obtained non-rigid part of object state reduces even to 2 dimensions. Secondly the dynamical model is derived and trained based on this intrinsic representation. Thirdly the learned intrinsic object structure is integrated into a particle-filter style tracker. We will show that this intrinsic object representation has some interesting properties and based on which the newly derived dynamical model makes particle-filter style tracker more robust and reliable. Experiments show that the learned tracker performs much better than existing trackers on the tracking of complex non-rigid motions such as fish twisting with self-occlusion and large inter-frame lip motion. The proposed method also has the potential to solve other type of tracking problems.", "output": "The abstract text contains these key words below: 'method' whose type is Method, 'intrinsic object structure' whose type is OtherScientificTerm, 'robust visual tracking' whose type is Task, 'parameterized object state' whose type is OtherScientificTerm, 'low dimensional manifold' whose type is OtherScientificTerm, 'dimensionality reduction and density estimation algorithm' whose type is Method, 'unsupervised learning of object intrinsic representation' whose type is Task, 'object intrinsic representation' whose type is Method, 'non-rigid part of object state' whose type is OtherScientificTerm, 'dynamical model' whose type is Method, 'intrinsic representation' whose type is Method, 'intrinsic object structure' whose type is OtherScientificTerm, 'particle-filter style tracker' whose type is Method, 'intrinsic object representation' whose type is Method, 'dynamical model' whose type is Method, 'particle-filter style tracker' whose type is Method, 'tracker' whose type is Generic, 'trackers' whose type is Generic, 'tracking of complex non-rigid motions' whose type is Task, 'complex non-rigid motions' whose type is OtherScientificTerm, 'fish twisting' whose type is OtherScientificTerm, 'self-occlusion' whose type is OtherScientificTerm, 'inter-frame lip motion' whose type is OtherScientificTerm, 'method' whose type is Generic, 'tracking problems' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "erson re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)'s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples , for approximating the geodesic distance. From this point of view, selecting suitable positive (i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has large intra-class variations. In this paper, we propose a novel moderate positive sample mining method to train robust CNN for person re-identification, dealing with the problem of large variation. In addition, we improve the learning by a metric weight constraint, so that the learned metric has a better generalization ability. Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification, and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification. Therefore, the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification.", "output": "The abstract text contains these key words below: 'Person re-identification' whose type is Task, 'pose' whose type is OtherScientificTerm, 'illumination' whose type is OtherScientificTerm, 'occlusion' whose type is OtherScientificTerm, 'camera view' whose type is OtherScientificTerm, 'pedestrian data' whose type is Material, 'highly-curved manifolds' whose type is Method, 'feature space' whose type is OtherScientificTerm, 'convolutional neural networks (CNN)' whose type is Method, 'feature extraction' whose type is OtherScientificTerm, 'geodesic distance' whose type is OtherScientificTerm, 'deep embedding methods' whose type is Method, 'Euclidean distance' whose type is OtherScientificTerm, 'manifold learning methods' whose type is Method, 'Euclidean distance' whose type is OtherScientificTerm, 'local range' whose type is OtherScientificTerm, 'graphical relationship' whose type is OtherScientificTerm, 'geodesic distance' whose type is OtherScientificTerm, 'local range' whose type is OtherScientificTerm, 'CNN embedding' whose type is OtherScientificTerm, 'data' whose type is Generic, 'intra-class variations' whose type is OtherScientificTerm, 'moderate positive sample mining method' whose type is Method, 'robust CNN' whose type is Method, 'person re-identification' whose type is Task, 'learning' whose type is Generic, 'metric weight constraint' whose type is OtherScientificTerm, 'learned metric' whose type is OtherScientificTerm, 'generalization ability' whose type is OtherScientificTerm, 'robust deep metrics' whose type is OtherScientificTerm, 'person re-identification' whose type is Task, 'deep model' whose type is Method, 'state-of-the-art methods' whose type is Generic, 'person re-identification' whose type is Task, 'deep models' whose type is Method, 'person re-identification' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe a simple  unsupervised technique  for learning  morphology  by identifying  hubs  in an  automaton  . For our purposes, a  hub  is a  node  in a  graph  with  in-degree  greater than one and  out-degree  greater than one. We create a  word-trie  , transform it into a  minimal DFA  , then identify  hubs  . Those  hubs  mark the boundary between  root  and  suffix  , achieving similar  performance  to more complex mixtures of techniques. ", "output": "The abstract text contains these key words below: 'unsupervised technique' whose type is Method, 'morphology' whose type is Task, 'hubs' whose type is OtherScientificTerm, 'automaton' whose type is OtherScientificTerm, 'hub' whose type is OtherScientificTerm, 'node' whose type is OtherScientificTerm, 'graph' whose type is OtherScientificTerm, 'word-trie' whose type is Method, 'it' whose type is Generic, 'minimal DFA' whose type is OtherScientificTerm, 'hubs' whose type is OtherScientificTerm, 'hubs' whose type is OtherScientificTerm, 'root' whose type is OtherScientificTerm, 'suffix' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a  Czech-English statistical machine translation system  which performs  tree-to-tree translation  of  dependency structures  . The only  bilingual resource  required is a  sentence-aligned parallel corpus  . All other  resources  are  monolingual  . We also refer to an  evaluation method  and plan to compare our  system's output  with a  benchmark system  . ", "output": "The abstract text contains these key words below: 'Czech-English statistical machine translation system' whose type is Method, 'tree-to-tree translation  of  dependency structures' whose type is Task, 'bilingual resource' whose type is Material, 'sentence-aligned parallel corpus' whose type is Material, 'system' whose type is Generic, 'benchmark system' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present a single-image highlight removal method that incorporates illumination-based constraints into image in-painting. Unlike occluded image regions filled by traditional inpainting, highlight pixels contain some useful information for guiding the inpainting process. Constraints provided by observed pixel colors, highlight color analysis and illumination color uniformity are employed in our method to improve estimation of the underlying diffuse color. The inclusion of these illumination constraints allows for better recovery of shading and textures by inpainting. Experimental results are given to demonstrate the performance of our method.", "output": "The abstract text contains these key words below: 'single-image highlight removal method' whose type is Method, 'illumination-based constraints' whose type is OtherScientificTerm, 'image in-painting' whose type is Task, 'occluded image regions' whose type is OtherScientificTerm, 'inpainting' whose type is OtherScientificTerm, 'highlight pixels' whose type is OtherScientificTerm, 'inpainting process' whose type is Task, 'Constraints' whose type is OtherScientificTerm, 'pixel colors' whose type is OtherScientificTerm, 'highlight color analysis' whose type is OtherScientificTerm, 'illumination color uniformity' whose type is OtherScientificTerm, 'method' whose type is Generic, 'estimation of the underlying diffuse color' whose type is OtherScientificTerm, 'illumination constraints' whose type is OtherScientificTerm, 'recovery of shading and textures' whose type is OtherScientificTerm, 'inpainting' whose type is Task, 'method' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This article introduces a  bidirectional grammar generation system  called  feature structure-directed generation  , developed for a  dialogue translation system  . The system utilizes  typed feature structures  to control the  top-down derivation  in a declarative way. This  generation system  also uses  disjunctive feature structures  to reduce the number of copies of the  derivation tree  . The  grammar  for this  generator  is designed to properly generate the  speaker's intention  in a  telephone dialogue  . ", "output": "The abstract text contains these key words below: 'bidirectional grammar generation system' whose type is Method, 'feature structure-directed generation' whose type is Method, 'dialogue translation system' whose type is Task, 'system' whose type is Generic, 'typed feature structures' whose type is OtherScientificTerm, 'top-down derivation' whose type is OtherScientificTerm, 'generation system' whose type is Method, 'disjunctive feature structures' whose type is OtherScientificTerm, 'derivation tree' whose type is OtherScientificTerm, 'grammar' whose type is Method, 'generator' whose type is Generic, 'speaker's intention' whose type is OtherScientificTerm, 'telephone dialogue' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The reality of  analogies between words  is refuted by noone (e.g., I walked is to to walk as I laughed is to to laugh, noted I walked : to walk :: I laughed : to laugh). But  computational linguists  seem to be quite dubious about  analogies between sentences  : they would not be enough numerous to be of any use. We report experiments conducted on a  multilingual corpus  to estimate the number of  analogies  among the  sentences  that it contains. We give two estimates, a lower one and a higher one. As an  analogy  must be valid on the level of  form  as well as on the level of  meaning  , we relied on the idea that  translation  should preserve  meaning  to test for similar  meanings  . ", "output": "The abstract text contains these key words below: 'analogies between words' whose type is Task, 'analogies between sentences' whose type is Task, 'multilingual corpus' whose type is Material, 'analogies' whose type is OtherScientificTerm, 'analogy' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we present a digital signal processor (DSP) implementation of real-time statistical voice conversion (VC) for silent speech enhancement and electrolaryngeal speech enhancement. As a silent speech interface, we focus on non-audible murmur (NAM), which can be used in situations where audible speech is not acceptable. Electrolaryngeal speech is one of the typical types of alaryngeal speech produced by an alternative speaking method for laryngectomees. However, the sound quality of NAM and electrolaryngeal speech suffers from lack of naturalness. VC has proven to be one of the promising approaches to address this problem, and it has been successfully implemented on devices with sufficient computational resources. An implementation on devices that are highly portable but have limited computational resources would greatly contribute to its practical use. In this paper we further implement real-time VC on a DSP. To implement the two speech enhancement systems based on real-time VC, one from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice, we propose several methods for reducing computational cost while preserving conversion accuracy. We conduct experimental evaluations and show that real-time VC is capable of running on a DSP with little degradation.", "output": "The abstract text contains these key words below: 'digital signal processor (DSP) implementation' whose type is Method, 'real-time statistical voice conversion (VC)' whose type is Method, 'silent speech enhancement' whose type is Task, 'electrolaryngeal speech enhancement' whose type is Task, 'silent speech interface' whose type is OtherScientificTerm, 'non-audible murmur (NAM)' whose type is Material, 'audible speech' whose type is Material, 'Electrolaryngeal speech' whose type is Material, 'alaryngeal speech' whose type is Material, 'speaking method' whose type is Method, 'laryngectomees' whose type is OtherScientificTerm, 'sound quality' whose type is Metric, 'NAM' whose type is Material, 'NAM and electrolaryngeal speech' whose type is Material, 'VC' whose type is Method, 'it' whose type is Generic, 'devices' whose type is Generic, 'sufficient computational resources' whose type is Material, 'devices' whose type is Generic, 'limited computational resources' whose type is Material, 'real-time VC' whose type is Method, 'DSP' whose type is OtherScientificTerm, 'speech enhancement systems' whose type is Method, 'real-time VC' whose type is Method, 'one' whose type is Generic, 'NAM' whose type is Material, 'whispered voice' whose type is Material, 'other' whose type is Generic, 'electrolaryngeal speech' whose type is Material, 'natural voice' whose type is Material, 'methods' whose type is Generic, 'computational cost' whose type is Metric, 'conversion accuracy' whose type is Metric, 'real-time VC' whose type is Method, 'DSP' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper describes the understanding process of the  spatial descriptions  in  Japanese  . In order to understand the described  world  , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space. It is done by an experimental  computer program   SPRINT  , which takes  natural language texts  and produces a  model  of the described  world  . To reconstruct the  model  , the authors extract the  qualitative spatial constraints  from the  text  , and represent them as the  numerical constraints  on the  spatial attributes  of the  entities  . This makes it possible to express the vagueness of the  spatial concepts  and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints. The interpretation reflects the  temporary belief  about the  world  . ", "output": "The abstract text contains these key words below: 'spatial descriptions' whose type is OtherScientificTerm, 'Japanese' whose type is Material, 'geometric model' whose type is Method, 'computer program   SPRINT' whose type is Method, 'natural language texts' whose type is Material, 'model' whose type is Generic, 'model' whose type is Generic, 'qualitative spatial constraints' whose type is OtherScientificTerm, 'numerical constraints' whose type is OtherScientificTerm, 'spatial attributes  of the  entities' whose type is OtherScientificTerm, 'spatial concepts' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper investigates some  computational problems  associated with  probabilistic translation models  that have recently been adopted in the literature on  machine translation  . These  models  can be viewed as pairs of  probabilistic context-free grammars  working in a 'synchronous' way. Two  hardness  results for the class  NP  are reported, along with an  exponential time lower-bound  for certain classes of algorithms that are currently used in the literature. ", "output": "The abstract text contains these key words below: 'computational problems' whose type is OtherScientificTerm, 'probabilistic translation models' whose type is Method, 'machine translation' whose type is Task, 'models' whose type is Generic, 'probabilistic context-free grammars' whose type is Method, 'exponential time lower-bound' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n There are several approaches that model  information extraction  as a  token classification task  , using various  tagging strategies  to combine multiple  tokens  . We describe the  tagging strategies  that can be found in the literature and evaluate their relative performances. We also introduce a new strategy, called  Begin/After tagging  or  BIA  , and show that it is competitive to the best other strategies. ", "output": "The abstract text contains these key words below: 'approaches' whose type is Generic, 'information extraction' whose type is Task, 'token classification task' whose type is Task, 'tagging strategies' whose type is Method, 'tagging strategies' whose type is Method, 'strategy' whose type is Generic, 'Begin/After tagging' whose type is Method, 'BIA' whose type is Method, 'it' whose type is Generic, 'strategies' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we address the problem of combining several  language models (LMs)  . We find that simple  interpolation methods  , like  log-linear and linear interpolation  , improve the  performance  but fall short of the  performance  of an  oracle  . The  oracle  knows the  reference word string  and selects the  word string  with the best  performance  (typically,  word or semantic error rate  ) from a list of  word strings  , where each  word string  has been obtained by using a different  LM  . Actually, the  oracle  acts like a  dynamic combiner  with  hard decisions  using the  reference  . We provide experimental results that clearly show the need for a  dynamic language model combination  to improve the  performance  further . We suggest a method that mimics the behavior of the  oracle  using a  neural network  or a  decision tree  . The method amounts to tagging  LMs  with  confidence measures  and picking the best  hypothesis  corresponding to the  LM  with the best  confidence  . ", "output": "The abstract text contains these key words below: 'language models (LMs)' whose type is Method, 'interpolation methods' whose type is Method, 'log-linear and linear interpolation' whose type is Method, 'word or semantic error rate' whose type is Metric, 'LM' whose type is Method, 'dynamic combiner' whose type is Method, 'hard decisions' whose type is OtherScientificTerm, 'dynamic language model combination' whose type is Method, 'method' whose type is Generic, 'neural network' whose type is Method, 'decision tree' whose type is Method, 'method' whose type is Generic, 'LMs' whose type is Method, 'confidence measures' whose type is Metric, 'LM' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe an implementation of data-driven selection of emphatic facial displays for an  embodied conversational agent  in a  dialogue system . A  corpus of sentences  in the domain of the  target dialogue system  was recorded, and the facial displays used by the  speaker  were annotated. The data from those recordings was used in a range of models for generating facial displays, each model making use of a different amount of  context  or choosing displays differently within a  context . The models were evaluated in two ways: by  cross-validation  against the  corpus , and by asking users to rate the output. The predictions of the  cross-validation  study differed from the actual user ratings. While the  cross-validation  gave the highest scores to models making a majority choice within a context, the user study showed a significant preference for models that produced more variation. This preference was especially strong among the female subjects.\n", "output": "The abstract text contains these key words below: 'data-driven selection' whose type is Task, 'emphatic facial displays' whose type is Material, 'embodied conversational agent' whose type is Task, 'dialogue system' whose type is Task, 'corpus of sentences' whose type is Material, 'dialogue system' whose type is Method, 'facial displays' whose type is Material, 'data' whose type is Generic, 'models' whose type is Generic, 'facial displays' whose type is Material, 'model' whose type is Generic, 'models' whose type is Generic, 'cross-validation' whose type is Method, 'corpus' whose type is Generic, 'cross-validation' whose type is Method, 'cross-validation' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose a process model for hierarchical perceptual sound organization, which recognizes perceptual sounds included in incoming sound signals. We consider perceptual sound organization as a scene analysis problem in the auditory domain. Our model consists of multiple processing modules and a hypothesis network for quantitative integration of multiple sources of information. When input information for each processing module is available, the module rises to process it and asynchronously writes output information to the hypothesis network. On the hypothesis network, individual information is integrated and an optimal internal model of perceptual sounds is automatically constructed. Based on the model, a music scene analysis system has been developed for acoustic signals of ensemble music, which recognizes rhythm, chords, and source-separated musical notes. Experimental results show that our method has permitted autonomous, stable and effective information integration to construct the internal model of hierarchical perceptual sounds.\n", "output": "The abstract text contains these key words below: 'process model' whose type is Method, 'hierarchical perceptual sound organization' whose type is Task, 'perceptual sounds' whose type is OtherScientificTerm, 'incoming sound signals' whose type is OtherScientificTerm, 'perceptual sound organization' whose type is Task, 'scene analysis problem' whose type is Task, 'auditory domain' whose type is Material, 'model' whose type is Generic, 'processing modules' whose type is Method, 'hypothesis network' whose type is Method, 'processing module' whose type is Method, 'module' whose type is Generic, 'hypothesis network' whose type is Method, 'hypothesis network' whose type is Method, 'internal model' whose type is Method, 'perceptual sounds' whose type is Material, 'model' whose type is Method, 'music scene analysis system' whose type is Method, 'acoustic signals of ensemble music' whose type is OtherScientificTerm, 'rhythm' whose type is Material, 'chords' whose type is Material, 'source-separated musical notes' whose type is Material, 'method' whose type is Generic, 'information integration' whose type is OtherScientificTerm, 'internal model' whose type is Method, 'hierarchical perceptual sounds' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "A  flexible parser  can deal with input that deviates from its  grammar , in addition to input that conforms to it. Ideally, such a  parser  will correct the deviant input: sometimes, it will be unable to correct it at all; at other times,  correction  will be possible, but only to within a range of ambiguous possibilities. This paper is concerned with such ambiguous situations, and with making it as easy as possible for the  ambiguity  to be resolved through consultation with the user of the  parser  - we presume interactive use. We show the importance of asking the user for clarification in as focused a way as possible.  Focused interaction  of this kind is facilitated by a  construction-specific approach  to  flexible parsing , with  specialized parsing techniques  for each type of  construction , and specialized  ambiguity representations  for each type of  ambiguity  that a particular  construction  can give rise to. A  construction-specific approach  also aids in  task-specific language development  by allowing a  language definition  that is natural in terms of the  task domain  to be interpreted directly without compilation into a  uniform grammar formalism , thus greatly speeding the  testing  of changes to the  language definition . ", "output": "The abstract text contains these key words below: 'flexible parser' whose type is Method, 'it' whose type is Generic, 'parser' whose type is Method, 'ambiguity' whose type is OtherScientificTerm, 'parser' whose type is Method, 'construction-specific approach' whose type is Method, 'flexible parsing' whose type is Task, 'specialized parsing techniques' whose type is Method, 'construction' whose type is Task, 'ambiguity representations' whose type is Method, 'ambiguity' whose type is OtherScientificTerm, 'construction' whose type is Task, 'construction-specific approach' whose type is Method, 'task-specific language development' whose type is Task, 'uniform grammar formalism' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper reports recent research into methods for  creating natural language text . A new  processing paradigm  called  Fragment-and-Compose  has been created and an experimental system implemented in it. The  knowledge  to be expressed in  text  is first divided into small  propositional units , which are then composed into appropriate combinations and converted into  text . KDS (Knowledge Delivery System) , which embodies this paradigm, has distinct parts devoted to creation of the  propositional units , to organization of the  text , to prevention of  excess redundancy , to creation of combinations of units, to evaluation of these combinations as potential  sentences , to selection of the best among competing combinations, and to creation of the  final text . The  Fragment-and-Compose paradigm  and the  computational methods  of  KDS  are described.\n", "output": "The abstract text contains these key words below: 'methods' whose type is Generic, 'creating natural language text' whose type is Task, 'processing paradigm' whose type is Generic, 'Fragment-and-Compose' whose type is Method, 'KDS (Knowledge Delivery System)' whose type is Method, 'paradigm' whose type is Generic, 'Fragment-and-Compose paradigm' whose type is Method, 'computational methods' whose type is Generic, 'KDS' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "or mobile speech application, speaker DOA estimation accuracy, interference robustness and compact physical size are three key factors. Considering the size, we utilized acoustic vector sensor (AVS) and proposed a DOA estimation algorithm previously [1], offering high accuracy with larger-than-15dB SNR but is deteriorated by non-speech interferences (NSI). This paper develops a robust speaker DOA estimation algorithm. It is achieved by deriving the inter-sensor data ratio model of an AVS in bispectrum domain (BISDR) and exploring the favorable properties of bispectrum, such as zero value of Gaussian process and different distribution of speech and NSI. Specifically, a reliable bispectrum mask is generated to guarantee that the speaker DOA cues, derived from BISDR, are robust to NSI in terms of speech sparsity and large bispectrum amplitude of the captured signals. Intensive experiments demonstrate an improved performance of our proposed algorithm under various NSI conditions even when SIR is smaller than 0dB.", "output": "The abstract text contains these key words below: 'mobile speech application' whose type is Task, 'speaker DOA estimation accuracy' whose type is Metric, 'interference robustness' whose type is Metric, 'compact physical size' whose type is Metric, 'acoustic vector sensor (AVS)' whose type is Method, 'DOA estimation algorithm' whose type is Method, 'non-speech interferences (NSI)' whose type is OtherScientificTerm, 'robust speaker DOA estimation algorithm' whose type is Method, 'It' whose type is Generic, 'inter-sensor data ratio model' whose type is Method, 'AVS' whose type is Method, 'bispectrum domain (BISDR)' whose type is Method, 'favorable properties' whose type is Generic, 'bispectrum' whose type is Method, 'zero value of Gaussian process' whose type is OtherScientificTerm, 'distribution of speech and NSI' whose type is OtherScientificTerm, 'NSI' whose type is OtherScientificTerm, 'bispectrum mask' whose type is Method, 'speaker DOA cues' whose type is OtherScientificTerm, 'BISDR' whose type is Method, 'NSI' whose type is OtherScientificTerm, 'speech sparsity' whose type is OtherScientificTerm, 'bispectrum amplitude' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'NSI conditions' whose type is OtherScientificTerm, 'SIR' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents an  unsupervised learning approach  to disambiguate various relations between  named entities  by use of various  lexical and syntactic features  from the  contexts  . It works by calculating  eigenvectors  of an  adjacency graph  's  Laplacian  to recover a  submanifold  of data from a  high dimensionality space  and then performing  cluster number estimation  on the  eigenvectors  . Experiment results on  ACE corpora  show that this  spectral clustering based approach  outperforms the other  clustering methods  . ", "output": "The abstract text contains these key words below: 'unsupervised learning approach' whose type is Method, 'relations between  named entities' whose type is OtherScientificTerm, 'lexical and syntactic features' whose type is OtherScientificTerm, 'It' whose type is Generic, 'eigenvectors' whose type is OtherScientificTerm, 'adjacency graph  's  Laplacian' whose type is OtherScientificTerm, 'submanifold' whose type is OtherScientificTerm, 'high dimensionality space' whose type is OtherScientificTerm, 'cluster number estimation' whose type is Task, 'eigenvectors' whose type is OtherScientificTerm, 'ACE corpora' whose type is Material, 'spectral clustering based approach' whose type is Method, 'clustering methods' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper reports a completed stage of ongoing research at the University of York. Landsbergen's advocacy of  analytical inverses  for  compositional syntax rules  encourages the application of  Definite Clause Grammar techniques  to the construction of a  parser  returning  Montague analysis trees . A  parser MDCC  is presented which implements an  augmented Friedman - Warren algorithm  permitting  post referencing * and interfaces with a language of  intenslonal logic translator LILT  so as to display the  derivational history  of corresponding  reduced IL formulae . Some familiarity with  Montague's PTQ  and the  basic DCG mechanism  is assumed.\n", "output": "The abstract text contains these key words below: 'analytical inverses' whose type is Method, 'compositional syntax rules' whose type is OtherScientificTerm, 'Definite Clause Grammar techniques' whose type is Method, 'parser  returning  Montague analysis trees' whose type is Task, 'parser MDCC' whose type is Method, 'augmented Friedman - Warren algorithm' whose type is Method, 'post referencing' whose type is OtherScientificTerm, 'intenslonal logic translator LILT' whose type is OtherScientificTerm, 'derivational history' whose type is OtherScientificTerm, 'reduced IL formulae' whose type is OtherScientificTerm, 'Montague's PTQ' whose type is Method, 'basic DCG mechanism' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents an algorithm for learning the time-varying shape of a non-rigid 3D object from uncalibrated 2D tracking data. We model shape motion as a rigid component (rotation and translation) combined with a non-rigid deformation. Reconstruction is ill-posed if arbitrary deformations are allowed. We constrain the problem by assuming that the object shape at each time instant is drawn from a Gaussian distribution. Based on this assumption, the algorithm simultaneously estimates 3D shape and motion for each time frame, learns the parameters of the Gaussian, and robustly fills-in missing data points. We then extend the algorithm to model temporal smoothness in object shape, thus allowing it to handle severe cases of missing data.", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'learning the time-varying shape of a non-rigid 3D object' whose type is Task, 'uncalibrated 2D tracking data' whose type is Material, 'shape motion' whose type is OtherScientificTerm, 'rigid component' whose type is Method, 'rotation' whose type is OtherScientificTerm, 'translation' whose type is OtherScientificTerm, 'non-rigid deformation' whose type is OtherScientificTerm, 'Reconstruction' whose type is OtherScientificTerm, 'arbitrary deformations' whose type is OtherScientificTerm, 'object shape' whose type is OtherScientificTerm, 'Gaussian distribution' whose type is Method, 'algorithm' whose type is Generic, '3D shape and motion' whose type is OtherScientificTerm, 'Gaussian' whose type is Method, 'algorithm' whose type is Generic, 'temporal smoothness in object shape' whose type is OtherScientificTerm, 'it' whose type is Generic, 'missing data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Statistical language modeling  remains a challenging task, in particular for  morphologically rich languages . Recently, new approaches based on  factored language models  have been developed to address this problem. These  models  provide principled ways of including additional  conditioning variables  other than the  preceding words , such as  morphological or syntactic features . However, the number of possible choices for  model parameters  creates a  large space of models  that cannot be searched exhaustively. This paper presents an  entirely data-driven model selection procedure  based on  genetic search , which is shown to outperform both  knowledge-based and random selection procedures  on two different  language modeling tasks  ( Arabic  and  Turkish ).\n", "output": "The abstract text contains these key words below: 'Statistical language modeling' whose type is Method, 'task' whose type is Generic, 'morphologically rich languages' whose type is Material, 'approaches' whose type is Generic, 'factored language models' whose type is Method, 'models' whose type is Generic, 'conditioning variables' whose type is OtherScientificTerm, 'morphological or syntactic features' whose type is OtherScientificTerm, 'model parameters' whose type is OtherScientificTerm, 'entirely data-driven model selection procedure' whose type is Method, 'genetic search' whose type is Method, 'knowledge-based and random selection procedures' whose type is Method, 'language modeling tasks' whose type is Task, 'Arabic' whose type is Material, 'Turkish' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper we introduce Ant-Q, a family of algorithms which present many similarities with Q-learning (Watkins, 1989), and which we apply to the solution of symmetric and asym-metric instances of the traveling salesman problem (TSP). Ant-Q algorithms were inspired by work on the ant system (AS), a distributed algorithm for combinatorial optimization based on the metaphor of ant colonies which was recently proposed in (Dorigo, 1992; Dorigo, Maniezzo and Colorni, 1996). We show that AS is a particular instance of the Ant-Q family, and that there are instances of this family which perform better than AS. We experimentally investigate the functioning of Ant-Q and we show that the results obtained by Ant-Q on symmetric TSP's are competitive with those obtained by other heuristic approaches based on neural networks or local search. Finally, we apply Ant-Q to some difficult asymmetric TSP's obtaining very good results: Ant-Q was able to find solutions of a quality which usually can be found only by very specialized algorithms.", "output": "The abstract text contains these key words below: 'Ant-Q' whose type is Method, 'Q-learning' whose type is Method, 'symmetric and asym-metric instances of the traveling salesman problem (TSP)' whose type is Task, 'Ant-Q algorithms' whose type is Method, 'ant system (AS)' whose type is Method, 'distributed algorithm' whose type is Method, 'combinatorial optimization' whose type is Task, 'AS' whose type is Method, 'Ant-Q family' whose type is Method, 'instances' whose type is Generic, 'family' whose type is Generic, 'AS' whose type is OtherScientificTerm, 'Ant-Q' whose type is Method, 'Ant-Q' whose type is Method, 'symmetric TSP' whose type is Task, 'heuristic approaches' whose type is Method, 'neural networks' whose type is Method, 'local search' whose type is Method, 'Ant-Q' whose type is Method, 'asymmetric TSP' whose type is Task, 'Ant-Q' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n the Object Recognition task, there exists a di-chotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep archi-tectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose estimation using these approaches has received relatively less attention. In this work, we study how Convolutional Neural Networks (CNN) architectures can be adapted to the task of simultaneous object recognition and pose estimation. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations. We extensively experiment on two recent large and challenging multi-view datasets and we achieve better than the state-of-the-art.", "output": "The abstract text contains these key words below: 'Object Recognition task' whose type is Task, 'categorization of objects' whose type is Task, 'estimating object pose' whose type is Task, 'former' whose type is Generic, 'view-invariant representation' whose type is Method, 'latter' whose type is Generic, 'representation' whose type is Generic, 'pose information' whose type is OtherScientificTerm, 'deep archi-tectures' whose type is Method, 'object category recognition' whose type is Task, 'Deep learning methods' whose type is Method, 'task' whose type is Generic, 'object pose estimation' whose type is Task, 'approaches' whose type is Generic, 'Convolutional Neural Networks (CNN) architectures' whose type is Method, 'object recognition' whose type is Task, 'pose estimation' whose type is Task, 'layers' whose type is OtherScientificTerm, 'CNN models' whose type is Method, 'them' whose type is Generic, 'layers of distributed representations' whose type is Method, 'CNNs' whose type is Method, 'object pose information' whose type is OtherScientificTerm, 'this' whose type is Generic, 'object category representations' whose type is Method, 'multi-view datasets' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "While  sentence extraction  as an approach to  summarization  has been shown to work in  documents  of certain  genres , because of the conversational nature of  email communication  where  utterances  are made in relation to one made previously,  sentence extraction  may not capture the necessary  segments  of  dialogue  that would make a  summary  coherent. In this paper, we present our work on the detection of  question-answer pairs  in an  email conversation  for the task of  email summarization . We show that various  features  based on the structure of email-threads can be used to improve upon  lexical similarity  of  discourse segments  for  question-answer pairing .\n", "output": "The abstract text contains these key words below: 'sentence extraction' whose type is Method, 'summarization' whose type is Task, 'email communication' whose type is Material, 'sentence extraction' whose type is Method, 'detection of  question-answer pairs' whose type is Task, 'email conversation' whose type is Material, 'email summarization' whose type is Task, 'features' whose type is OtherScientificTerm, 'structure of email-threads' whose type is OtherScientificTerm, 'lexical similarity' whose type is OtherScientificTerm, 'discourse segments' whose type is OtherScientificTerm, 'question-answer pairing' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Language resource quality  is crucial in  NLP . Many of the resources used are derived from data created by human beings out of an  NLP  context, especially regarding  MT  and  reference translations . Indeed,  automatic evaluations  need  high-quality data  that allow the comparison of both  automatic and human translations . The validation of these resources is widely recommended before being used. This paper describes the impact of using  different-quality references  on  evaluation . Surprisingly enough, similar scores are obtained in many cases regardless of the quality. Thus, the limitations of the  automatic metrics  used within  MT  are also discussed in this regard.\n", "output": "The abstract text contains these key words below: 'Language resource quality' whose type is Metric, 'NLP' whose type is Task, 'NLP' whose type is Task, 'MT' whose type is Task, 'reference translations' whose type is Task, 'automatic evaluations' whose type is Task, 'high-quality data' whose type is Material, 'resources' whose type is Generic, 'different-quality references' whose type is OtherScientificTerm, 'evaluation' whose type is Generic, 'automatic metrics' whose type is Metric, 'MT' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "isitors who browse the web from wireless PDAs, cell phones, and pagers are frequently stymied by web interfaces optimized for desktop PCs. Simply replacing graphics with text and reformatting tables does not solve the problem, because deep link structures can still require minutes to traverse. In this paper we develop an algorithm, MINPATH, that automatically improves wireless web navigation by suggesting useful shortcut links in real time. MINPATH finds shortcuts by using a learned model of web visitor behavior to estimate the savings of shortcut links, and suggests only the few best links. We explore a variety of predictive models, including Na\u00a8\u0131ve Bayes mixture models and mixtures of Markov models, and report empirical evidence that MINPATH finds useful shortcuts that save substantial navigational effort.", "output": "The abstract text contains these key words below: 'web interfaces' whose type is Method, 'desktop PCs' whose type is OtherScientificTerm, 'deep link structures' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'MINPATH' whose type is Method, 'wireless web navigation' whose type is Task, 'MINPATH' whose type is Method, 'model' whose type is Generic, 'web visitor behavior' whose type is OtherScientificTerm, 'savings of shortcut links' whose type is Task, 'predictive models' whose type is Generic, 'Na\u00a8\u0131ve Bayes mixture models' whose type is Method, 'mixtures of Markov models' whose type is Method, 'MINPATH' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe a novel technique and implemented system for constructing a  subcategorization dictionary  from  textual corpora . Each  dictionary entry  encodes the  relative frequency of occurrence  of a comprehensive set of  subcategorization classes  for  English . An initial experiment, on a sample of 14  verbs  which exhibit  multiple complementation patterns , demonstrates that the technique achieves  accuracy  comparable to previous approaches, which are all limited to a highly restricted set of  subcategorization classes . We also demonstrate that a  subcategorization dictionary  built with the system improves the  accuracy  of a  parser  by an appreciable amount", "output": "The abstract text contains these key words below: 'system' whose type is Generic, 'subcategorization dictionary' whose type is OtherScientificTerm, 'textual corpora' whose type is Material, 'relative frequency of occurrence' whose type is OtherScientificTerm, 'subcategorization dictionary' whose type is OtherScientificTerm, 'system' whose type is Generic, 'accuracy' whose type is Metric, 'parser' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We propose a  detection method  for orthographic variants caused by  transliteration  in a large  corpus . The method employs two  similarities . One is  string similarity  based on  edit distance . The other is  contextual similarity  by a  vector space model . Experimental results show that the method performed a 0.889  F-measure  in an open test.\n", "output": "The abstract text contains these key words below: 'detection method' whose type is Method, 'orthographic variants' whose type is OtherScientificTerm, 'transliteration' whose type is Task, 'method' whose type is Generic, 'similarities' whose type is Generic, 'string similarity' whose type is OtherScientificTerm, 'edit distance' whose type is Method, 'contextual similarity' whose type is OtherScientificTerm, 'vector space model' whose type is Method, 'method' whose type is Generic, 'F-measure' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\nA central problem of  word sense disambiguation (WSD)  is the lack of  manually sense-tagged data  required for  supervised learning  . In this paper, we evaluate an approach to automatically acquire  sense-tagged training data  from  English-Chinese parallel corpora  , which are then used for disambiguating the  nouns  in the  SENSEVAL-2 English lexical sample task  . Our investigation reveals that this  method of acquiring sense-tagged data  is promising. On a subset of the most difficult  SENSEVAL-2 nouns  , the  accuracy  difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that  manually sense-tagged data  have in their  sense coverage  . Our analysis also highlights the importance of the issue of  domain dependence  in evaluating  WSD programs  . ", "output": "The abstract text contains these key words below: 'word sense disambiguation (WSD)' whose type is Task, 'manually sense-tagged data' whose type is Material, 'supervised learning' whose type is Method, 'approach' whose type is Generic, 'sense-tagged training data' whose type is Material, 'English-Chinese parallel corpora' whose type is Material, 'nouns' whose type is OtherScientificTerm, 'SENSEVAL-2 English lexical sample task' whose type is Material, 'method' whose type is Generic, 'acquiring sense-tagged data' whose type is Task, 'SENSEVAL-2 nouns' whose type is OtherScientificTerm, 'accuracy' whose type is Metric, 'manually sense-tagged data' whose type is Material, 'sense coverage' whose type is Metric, 'domain dependence' whose type is OtherScientificTerm, 'evaluating  WSD programs' whose type is Task, 'WSD programs' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e investigated whether automatic phonetic transcriptions (APTs) can replace manually verified phonetic transcriptions (MPTs) in a large corpus-based study on pronunciation variation. To this end, we compared the performance of both transcription types in a classification experiment aimed at establishing the direct influence of a particular situational setting on pronunciation variation. We trained classifiers on the speech processes extracted from the alignments of an APT and an MPT with a canonical transcription. We tested whether the classifiers were equally good at verifying whether unknown transcriptions represent read speech or telephone dialogues, and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings. Our results not only show that similar distinguishing speech processes were identified; our APT-based classifier yielded better classification accuracy than the MPT-based classifier whilst using fewer classification features.", "output": "The abstract text contains these key words below: 'automatic phonetic transcriptions (APTs)' whose type is OtherScientificTerm, 'manually verified phonetic transcriptions' whose type is OtherScientificTerm, 'pronunciation variation' whose type is Task, 'classification' whose type is Task, 'pronunciation variation' whose type is Task, 'classifiers' whose type is Method, 'speech processes' whose type is Material, 'alignments' whose type is OtherScientificTerm, 'APT' whose type is OtherScientificTerm, 'MPT' whose type is OtherScientificTerm, 'canonical transcription' whose type is OtherScientificTerm, 'classifiers' whose type is Material, 'unknown transcriptions' whose type is OtherScientificTerm, 'read speech' whose type is Material, 'telephone dialogues' whose type is Material, 'speech processes' whose type is Material, 'APT-based classifier' whose type is Method, 'classification accuracy' whose type is Metric, 'MPT-based classifier' whose type is Method, 'classification features' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Recent years have seen increasing research on extracting and using temporal information in  natural language applications . However most of the works found in the literature have focused on identifying and understanding  temporal expressions  in  newswire texts . In this paper we report our work on anchoring  temporal expressions  in a novel  genre , emails. The highly under-specified nature of these  expressions  fits well with our  constraint-based representation  of time,  Time Calculus for Natural Language (TCNL) . We have developed and evaluated a  Temporal Expression Anchoror (TEA) , and the result shows that it performs significantly better than the  baseline , and compares favorably with some of the closely related work.", "output": "The abstract text contains these key words below: 'temporal information' whose type is OtherScientificTerm, 'natural language applications' whose type is Task, 'temporal expressions' whose type is OtherScientificTerm, 'temporal expressions  in  newswire texts' whose type is OtherScientificTerm, 'newswire texts' whose type is Material, 'temporal expressions' whose type is OtherScientificTerm, 'emails' whose type is Material, 'expressions' whose type is Generic, 'constraint-based representation  of time' whose type is OtherScientificTerm, 'Time Calculus for Natural Language (TCNL)' whose type is OtherScientificTerm, 'Temporal Expression Anchoror (TEA)' whose type is Method, 'it' whose type is Generic, 'baseline' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e attempt to understand visual classification in humans using both psy-chophysical and machine learning techniques. Frontal views of human faces were used for a gender classification task. Human subjects classified the faces and their gender judgment, reaction time and confidence rating were recorded. Several hyperplane learning algorithms were used on the same classification task using the Principal Components of the texture and flowfield representation of the faces. The classification performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. Our results suggest that human classification can be modeled by some hyperplane algorithms in the feature space we used. For classification , the brain needs more processing for stimuli close to that hyperplane than for those further away.", "output": "The abstract text contains these key words below: 'visual classification' whose type is Task, 'psy-chophysical and machine learning techniques' whose type is Method, 'Frontal views of human faces' whose type is OtherScientificTerm, 'gender classification task' whose type is Task, 'gender judgment' whose type is OtherScientificTerm, 'reaction time' whose type is OtherScientificTerm, 'confidence rating' whose type is OtherScientificTerm, 'hyperplane learning algorithms' whose type is Method, 'classification task' whose type is Task, 'Principal Components of the texture' whose type is OtherScientificTerm, 'flowfield representation of the faces' whose type is Method, 'classification' whose type is Task, 'learning algorithms' whose type is Method, 'face database' whose type is Task, 'hyperplane of the learning algorithms' whose type is OtherScientificTerm, 'learning algorithms' whose type is Method, 'human classification' whose type is Task, 'hyperplane algorithms' whose type is Method, 'feature space' whose type is OtherScientificTerm, 'classification' whose type is Task, 'hyperplane' whose type is OtherScientificTerm, 'those' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "lthough the study of clustering is centered around an intuitively compelling goal, it has been very difficult to develop a unified framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the difficulty in finding such a unification, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-offs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median.", "output": "The abstract text contains these key words below: 'clustering' whose type is Task, 'unified framework' whose type is Method, 'reasoning' whose type is Task, 'unification' whose type is Method, 'impossibility theorem' whose type is Method, 'clustering function' whose type is OtherScientificTerm, 'well-studied clustering techniques' whose type is Method, 'single-linkage' whose type is Method, 'sum-of-pairs' whose type is Method, 'k-means' whose type is Method, 'k-median' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e consider a problem of blind source separation from a set of instantaneous linear mixtures, where the mixing matrix is unknown. It was discovered recently, that exploiting the sparsity of sources in an appropriate representation according to some signal dictionary, dramatically improves the quality of separation. In this work we use the property of multi scale transforms, such as wavelet or wavelet packets, to decompose signals into sets of local features with various degrees of sparsity. We use this intrinsic property for selecting the best (most sparse) subsets of features for further separation. The performance of the algorithm is verified on noise-free and noisy data. Experiments with simulated signals, musical sounds and images demonstrate significant improvement of separation quality over previously reported results.", "output": "The abstract text contains these key words below: 'blind source separation' whose type is Task, 'instantaneous linear mixtures' whose type is OtherScientificTerm, 'mixing matrix' whose type is OtherScientificTerm, 'sparsity of sources' whose type is OtherScientificTerm, 'signal dictionary' whose type is OtherScientificTerm, 'quality of separation' whose type is Metric, 'multi scale transforms' whose type is OtherScientificTerm, 'wavelet or wavelet packets' whose type is OtherScientificTerm, 'local features' whose type is OtherScientificTerm, 'sparsity' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'noise-free and noisy data' whose type is Material, 'simulated signals' whose type is Material, 'musical sounds' whose type is Material, 'images' whose type is Material, 'separation quality' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "here is an increased interest in the efficient creation of city models, be it virtual or as-built. We present a method for synthesizing complex, photo-realistic facade images, from a single example. After parsing the example image into its semantic components, a tiling for it is generated. Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts inpainted. A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. Promising results for multiple standard datasets \u2013 in particular for the different building styles they contain \u2013 demonstrate the potential of the method.", "output": "The abstract text contains these key words below: 'creation of city models' whose type is Task, 'method' whose type is Generic, 'synthesizing complex, photo-realistic facade images' whose type is Task, 'semantic components' whose type is Method, 'tiling' whose type is OtherScientificTerm, 'tilings' whose type is OtherScientificTerm, 'facade textures' whose type is OtherScientificTerm, 'occluded parts inpainted' whose type is OtherScientificTerm, 'genetic algorithm' whose type is Generic, 'facades' whose type is OtherScientificTerm, 'inpainted parts' whose type is OtherScientificTerm, 'multiple standard datasets' whose type is Generic, 'method' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper considers the problem of automatic assessment of  local coherence . We present a novel  entity-based representation  of  discourse  which is inspired by  Centering Theory  and can be computed automatically from  raw text . We view  coherence assessment  as a  ranking learning problem  and show that the proposed  discourse representation  supports the effective learning of a  ranking function . Our experiments demonstrate that the  induced model  achieves significantly higher  accuracy  than a  state-of-the-art coherence model .\n", "output": "The abstract text contains these key words below: 'automatic assessment of  local coherence' whose type is Task, 'entity-based representation  of  discourse' whose type is Method, 'Centering Theory' whose type is Method, 'raw text' whose type is Material, 'coherence assessment' whose type is Task, 'ranking learning problem' whose type is Task, 'discourse representation' whose type is Method, 'ranking function' whose type is OtherScientificTerm, 'induced model' whose type is Method, 'accuracy' whose type is Metric, 'coherence model' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we show how to construct a  transfer dictionary  automatically.  Dictionary construction , one of the most difficult tasks in developing a  machine translation system , is expensive. To avoid this problem, we investigate how we build a  dictionary  using existing  linguistic resources . Our  algorithm  can be applied to any  language pairs , but for the present we focus on building a  Korean-to-Japanese dictionary  using  English  as a  pivot . We attempt three ways of  automatic construction  to corroborate the effect of the  directionality  of  dictionaries . First, we introduce  \"one-time look up\" method  using a  Korean-to-English and a Japanese-to-English dictionary . Second, we show a method using  \"overlapping constraint\"  with a  Korean-to-English dictionary  and an  English-to-Japanese dictionary . Third, we consider another alternative method rarely used for building a  dictionary : an  English-to-Korean dictionary  and  English-to-Japanese dictionary . We found that the first method is the most effective and the best result can be obtained from combining the three methods.\n", "output": "The abstract text contains these key words below: 'transfer dictionary' whose type is Material, 'Dictionary construction' whose type is Task, 'machine translation system' whose type is Task, 'dictionary' whose type is Generic, 'linguistic resources' whose type is Material, 'Korean-to-Japanese dictionary' whose type is Material, 'English' whose type is Material, 'automatic construction' whose type is Task, 'directionality  of  dictionaries' whose type is OtherScientificTerm, 'dictionaries' whose type is Generic, '\"one-time look up\" method' whose type is Method, 'Korean-to-English and a Japanese-to-English dictionary' whose type is Material, 'method' whose type is Generic, '\"overlapping constraint\"' whose type is OtherScientificTerm, 'Korean-to-English dictionary' whose type is Material, 'English-to-Japanese dictionary' whose type is Material, 'method' whose type is Generic, 'dictionary' whose type is Generic, 'English-to-Korean dictionary' whose type is Material, 'English-to-Japanese dictionary' whose type is Material, 'method' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper solves a specialized regression problem to obtain sampling probabilities for records in databases. The goal is to sample a small set of records over which evaluating aggregate queries can be done both efficiently and accurately. We provide a principled and provable solution for this problem; it is parameterless and requires no data insights. Unlike standard regression problems , the loss is inversely proportional to the regressed-to values. Moreover, a cost zero solution always exists and can only be excluded by hard budget constraints. A unique form of reg-ularization is also needed. We provide an efficient and simple regularized Empirical Risk Minimization (ERM) algorithm along with a theoretical generalization result. Our extensive experimental results significantly improve over both uniform sampling and standard stratified sampling which are de-facto the industry standards.", "output": "The abstract text contains these key words below: 'specialized regression problem' whose type is Task, 'sampling probabilities' whose type is OtherScientificTerm, 'records' whose type is Material, 'databases' whose type is Material, 'records' whose type is Material, 'aggregate queries' whose type is OtherScientificTerm, 'principled and provable solution' whose type is Method, 'problem' whose type is Generic, 'it' whose type is Generic, 'regression problems' whose type is Task, 'loss' whose type is OtherScientificTerm, 'regressed-to values' whose type is OtherScientificTerm, 'cost zero solution' whose type is Method, 'hard budget constraints' whose type is OtherScientificTerm, 'reg-ularization' whose type is OtherScientificTerm, 'regularized Empirical Risk Minimization (ERM) algorithm' whose type is Method, 'uniform sampling' whose type is Method, 'stratified sampling' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we propose a novel algorithm to detect/compensate on-line interference effects when integrating Global Navigation Satellite System (GNSS) and Inertial Navigation System (INS). The GNSS/INS coupling is usually performed by an Extended Kalman Filter (EKF) which yields an accurate and robust localization. However , interference cause the GNSS measurement noise to increase unexpectedly, hence degrade the positioning accuracy. In this context , our contribution is twofold. We first study the impact of the GNSS noise inflation on the covariance of the EKF outputs so as to compute a least square estimate of the potential variance jumps. Then, this estimation is used in a Bayesian test which decides whether interference are corrupting the GNSS signal or not. It allows us to estimate their times of occurrence as well. In this way, the impaired measurements can be discarded while their impact on the navigation solution can be compensated. The results show the performance of the proposed approach on simulated data.", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'on-line interference effects' whose type is Task, 'Global Navigation Satellite System (GNSS)' whose type is Task, 'Inertial Navigation System (INS)' whose type is Task, 'GNSS/INS coupling' whose type is Task, 'Extended Kalman Filter (EKF)' whose type is Method, 'accurate and robust localization' whose type is Task, 'interference' whose type is OtherScientificTerm, 'GNSS measurement noise' whose type is OtherScientificTerm, 'positioning accuracy' whose type is Metric, 'covariance' whose type is OtherScientificTerm, 'EKF outputs' whose type is OtherScientificTerm, 'least square estimate' whose type is Method, 'variance jumps' whose type is OtherScientificTerm, 'estimation' whose type is Generic, 'Bayesian test' whose type is Method, 'GNSS signal' whose type is OtherScientificTerm, 'navigation solution' whose type is Task, 'approach' whose type is Generic, 'simulated data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his work presents a real-time system for multiple object tracking in dynamic scenes. A unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a prior knowledge about the shape or motion of objects. The system produces good segment and tracking results at a frame rate of 15-20 fps for image size of 320x240, as demonstrated by extensive experiments performed using video sequences under different conditions indoor and outdoor with long-duration and complete occlusions in changing background.", "output": "The abstract text contains these key words below: 'real-time system' whose type is Method, 'multiple object tracking in dynamic scenes' whose type is Task, 'system' whose type is Generic, 'long-duration and complete occlusion' whose type is OtherScientificTerm, 'prior knowledge' whose type is OtherScientificTerm, 'shape' whose type is OtherScientificTerm, 'motion of objects' whose type is OtherScientificTerm, 'system' whose type is Generic, 'tracking' whose type is Task, 'frame rate' whose type is OtherScientificTerm, 'image size' whose type is OtherScientificTerm, 'video sequences' whose type is Material, 'long-duration and complete occlusions' whose type is OtherScientificTerm, 'changing background' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The  JAVELIN system  integrates a flexible,  planning-based architecture  with a variety of  language processing modules  to provide an  open-domain question answering capability  on  free text  . The demonstration will focus on how  JAVELIN  processes  questions  and retrieves the most likely  answer candidates  from the given  text corpus  . The operation of the system will be explained in depth through browsing the  repository  of  data objects  created by the system during each  question answering session  . ", "output": "The abstract text contains these key words below: 'JAVELIN system' whose type is Method, 'planning-based architecture' whose type is Method, 'language processing modules' whose type is Method, 'open-domain question answering capability' whose type is Task, 'JAVELIN' whose type is Method, 'system' whose type is Generic, 'system' whose type is Generic, 'question answering' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The  TIPSTER Architecture  has been designed to enable a variety of different  text applications  to use a set of  common text processing modules . Since  user interfaces  work best when customized for  particular applications  , it is appropriator that no particular  user interface styles or conventions  are described in the  TIPSTER Architecture specification . However, the  Computing Research Laboratory (CRL)  has constructed several  TIPSTER applications  that use a common set of configurable  Graphical User Interface (GUI) functions . These  GUIs  were constructed using  CRL's TIPSTER User Interface Toolkit (TUIT) .  TUIT  is a  software library  that can be used to construct  multilingual TIPSTER user interfaces  for a set of common user tasks.  CRL  developed  TUIT  to support their work to integrate  TIPSTER modules  for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects. This paper briefly describes  TUIT  and its capabilities. \n", "output": "The abstract text contains these key words below: 'TIPSTER Architecture' whose type is Method, 'text applications' whose type is Task, 'common text processing modules' whose type is Method, 'user interfaces' whose type is OtherScientificTerm, 'applications' whose type is Generic, 'user interface styles or conventions' whose type is OtherScientificTerm, 'TIPSTER Architecture specification' whose type is OtherScientificTerm, 'TIPSTER applications' whose type is Task, 'Graphical User Interface (GUI) functions' whose type is OtherScientificTerm, 'GUIs' whose type is OtherScientificTerm, 'CRL's TIPSTER User Interface Toolkit (TUIT)' whose type is Method, 'TUIT' whose type is Method, 'software library' whose type is Method, 'multilingual TIPSTER user interfaces' whose type is OtherScientificTerm, 'TUIT' whose type is Method, 'TIPSTER modules' whose type is Method, 'TUIT' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph. We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion, and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges. When the region graph has two layers, corresponding to a Bethe approximation, we show that our sufficient conditions for concavity are also necessary. Finally, we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph. We conclude with simulations that demonstrate the advantages of the reweighted Kikuchi approach.", "output": "The abstract text contains these key words below: 'reweighted version of the Kikuchi approximation' whose type is Method, 'Kikuchi approximation' whose type is Method, 'log partition function of a product distribution' whose type is Task, 'region graph' whose type is OtherScientificTerm, 'concavity' whose type is OtherScientificTerm, 'reweighted objective function' whose type is OtherScientificTerm, 'weight assignments' whose type is OtherScientificTerm, 'Kikuchi expansion' whose type is OtherScientificTerm, 'reweighted version of the sum product algorithm' whose type is Method, 'Kikuchi region graph' whose type is OtherScientificTerm, 'global optima' whose type is OtherScientificTerm, 'Kikuchi approximation' whose type is Method, 'algorithm' whose type is Generic, 'region graph' whose type is OtherScientificTerm, 'Bethe approximation' whose type is Method, 'concavity' whose type is OtherScientificTerm, 'concavity' whose type is OtherScientificTerm, 'cycle structure' whose type is OtherScientificTerm, 'region graph' whose type is OtherScientificTerm, 'reweighted Kikuchi approach' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "oosting methods are known not to usually overfit training data even as the size of the generated classifiers becomes large. Schapire et al. attempted to explain this phenomenon in terms of the margins the classifier achieves on training examples. Later, however, Breiman cast serious doubt on this explanation by introducing a boosting algorithm, arc-gv, that can generate a higher margins distribution than AdaBoost and yet performs worse. In this paper, we take a close look at Breiman's compelling but puzzling results. Although we can reproduce his main finding, we find that the poorer performance of arc-gv can be explained by the increased complexity of the base classifiers it uses, an explanation supported by our experiments and entirely consistent with the margins theory. Thus, we find maximizing the margins is desirable, but not necessarily at the expense of other factors, especially base-classifier complexity.", "output": "The abstract text contains these key words below: 'Boosting methods' whose type is Method, 'classifiers' whose type is Method, 'classifier' whose type is Method, 'boosting algorithm' whose type is Method, 'arc-gv' whose type is Method, 'margins distribution' whose type is OtherScientificTerm, 'AdaBoost' whose type is Method, 'arc-gv' whose type is Method, 'complexity' whose type is Metric, 'base classifiers' whose type is Method, 'margins theory' whose type is Method, 'base-classifier complexity' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\"To explain complex phenomena, an  explanation system  must be able to select information from a formal representation of  domain knowledge , organize the selected information into  multisentential discourse plans , and realize the  discourse plans  in text. Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for  explanation , empirical results have been limited. This paper reports on a seven-year effort to empirically study  explanation generation  from  semantically rich, large-scale knowledge bases . In particular, it describes a  robust explanation system  that constructs  multisentential and multi-paragraph explanations  from the a  large-scale knowledge base  in the domain of botanical anatomy, physiology, and development. We introduce the evaluation methodology and describe how performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system. In this evaluation, scored within \"\"half a grade\"\" of domain experts, and its performance exceeded that of one of the domain experts.\"\n", "output": "The abstract text contains these key words below: 'explanation system' whose type is Method, 'multisentential discourse plans' whose type is OtherScientificTerm, 'discourse plans' whose type is OtherScientificTerm, 'explanation generation' whose type is Task, 'semantically rich, large-scale knowledge bases' whose type is Material, 'robust explanation system' whose type is Method, 'multisentential and multi-paragraph explanations' whose type is OtherScientificTerm, 'large-scale knowledge base' whose type is Material, 'botanical anatomy' whose type is Material, 'physiology' whose type is Material, 'development' whose type is Material, 'evaluation methodology' whose type is Generic, 'methodology' whose type is Generic, 'explanation system' whose type is Method, 'evaluation' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper proposes an  alignment adaptation approach  to improve  domain-specific (in-domain) word alignment . The basic idea of  alignment adaptation  is to use  out-of-domain corpus  to improve  in-domain word alignment  results. In this paper, we first train two  statistical word alignment models  with the large-scale  out-of-domain corpus  and the small-scale  in-domain corpus  respectively, and then interpolate these two models to improve the  domain-specific word alignment . Experimental results show that our approach improves  domain-specific word alignment  in terms of both  precision  and  recall , achieving a  relative error rate reduction  of 6.56% as compared with the state-of-the-art technologies.\n", "output": "The abstract text contains these key words below: 'alignment adaptation approach' whose type is Method, 'domain-specific (in-domain) word alignment' whose type is Task, 'alignment adaptation' whose type is Method, 'out-of-domain corpus' whose type is Material, 'in-domain word alignment' whose type is Task, 'statistical word alignment models' whose type is Method, 'large-scale  out-of-domain corpus' whose type is Material, 'small-scale  in-domain corpus' whose type is Material, 'models' whose type is Generic, 'domain-specific word alignment' whose type is Task, 'approach' whose type is Generic, 'domain-specific word alignment' whose type is Task, 'precision' whose type is Metric, 'recall' whose type is Metric, 'relative error rate reduction' whose type is Metric, 'state-of-the-art technologies' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his work proposes a new research direction to address the lack of structures in traditional n-gram models. It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus. Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge. Bayesian inference then samples the rules, disambiguating and combining them to create complex tree structures that maximize a discriminative model's posterior on a target unlabeled corpus. This posterior encodes sparse se-lectional preferences between a head word and its dependents. The model is evaluated on English and Czech newspaper texts, and is then validated on French broadcast news transcriptions.", "output": "The abstract text contains these key words below: 'lack of structures' whose type is OtherScientificTerm, 'lack of structures in traditional n-gram models' whose type is Task, 'n-gram models' whose type is Method, 'weakly supervised dependency parser' whose type is Task, 'speech syntax' whose type is OtherScientificTerm, 'annotated training corpus' whose type is Material, 'Labeled data' whose type is OtherScientificTerm, 'hand-crafted rules' whose type is OtherScientificTerm, 'syntactic knowledge' whose type is OtherScientificTerm, 'Bayesian inference' whose type is Method, 'rules' whose type is OtherScientificTerm, 'them' whose type is Generic, 'complex tree structures' whose type is OtherScientificTerm, 'discriminative model's posterior' whose type is OtherScientificTerm, 'unlabeled corpus' whose type is Material, 'posterior' whose type is Generic, 'sparse se-lectional preferences' whose type is OtherScientificTerm, 'model' whose type is Generic, 'English and Czech newspaper texts' whose type is Material, 'French broadcast news transcriptions' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We propose a framework to derive the  distance  between  concepts  from  distributional measures of word co-occurrences . We use the  categories  in a published  thesaurus  as  coarse-grained concepts , allowing all possible  distance values  to be stored in a  concept-concept matrix  roughly.01% the size of that created by existing measures. We show that the newly proposed  concept-distance measures  outperform  traditional distributional word-distance measures  in the tasks of (1) ranking  word pairs  in order of  semantic distance , and (2) correcting  real-word spelling errors . In the latter task, of all the  WordNet-based measures , only that proposed by Jiang and Conrath outperforms the best  distributional concept-distance measures .\n", "output": "The abstract text contains these key words below: 'framework' whose type is Generic, 'concepts' whose type is OtherScientificTerm, 'distributional measures of word co-occurrences' whose type is Method, 'coarse-grained concepts' whose type is OtherScientificTerm, 'concept-concept matrix' whose type is OtherScientificTerm, 'concept-distance measures' whose type is Method, 'distributional word-distance measures' whose type is Method, 'tasks' whose type is Generic, 'ranking  word pairs  in order of  semantic distance' whose type is Task, 'correcting  real-word spelling errors' whose type is Task, 'task' whose type is Generic, 'WordNet-based measures' whose type is Method, 'distributional concept-distance measures' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n Current  natural language interfaces  have concentrated largely on determining the literal  meaning  of  input  from their  users  . While such  decoding  is an essential underpinning, much recent work suggests that  natural language interfaces  will never appear cooperative or graceful unless they also incorporate numerous  non-literal aspects of communication  , such as robust  communication procedures  . This paper defends that view, but claims that direct imitation of human performance is not the best way to implement many of these  non-literal aspects of communication  ; that the new technology of powerful  personal computers  with integral  graphics displays  offers techniques superior to those of humans for these aspects, while still satisfying  human communication needs  . The paper proposes  interfaces  based on a judicious mixture of these techniques and the still valuable methods of more traditional  natural language interfaces . ", "output": "The abstract text contains these key words below: 'natural language interfaces' whose type is OtherScientificTerm, 'decoding' whose type is Method, 'natural language interfaces' whose type is OtherScientificTerm, 'they' whose type is Generic, 'non-literal aspects of communication' whose type is Method, 'robust  communication procedures' whose type is Method, 'non-literal aspects of communication' whose type is Method, 'personal computers' whose type is OtherScientificTerm, 'graphics displays' whose type is OtherScientificTerm, 'natural language interfaces' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e study and compare two novel embedding methods for segmenting feature points of piece-wise planar structures from two (uncalibrated) perspective images. We show that a set of different homographies can be embedded in different ways to a higher-dimensional real or complex space, so that each homography corresponds to either a complex bilinear form or a real quadratic form. Each embedding reveals different algebraic properties and relations of homo-graphies. We give a closed-form segmentation solution for each case by utilizing these properties based on subspace-segmentation methods. These theoretical results show that one can intrinsically segment a piece-wise planar scene from 2-D images without explicitly performing any 3-D reconstruction. The resulting segmentation may make subsequent 3-D reconstruction much better-conditioned. We demonstrate the proposed methods with some convincing experimental results.", "output": "The abstract text contains these key words below: 'embedding methods' whose type is Method, 'segmenting feature points of piece-wise planar structures' whose type is Task, 'homographies' whose type is OtherScientificTerm, 'higher-dimensional real or complex space' whose type is OtherScientificTerm, 'homography' whose type is OtherScientificTerm, 'complex bilinear form' whose type is OtherScientificTerm, 'real quadratic form' whose type is OtherScientificTerm, 'homo-graphies' whose type is OtherScientificTerm, 'closed-form segmentation solution' whose type is Method, 'subspace-segmentation methods' whose type is Method, 'piece-wise planar scene' whose type is OtherScientificTerm, '2-D images' whose type is Material, '3-D reconstruction' whose type is Method, '3-D reconstruction' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "raditional linear Fukunaga-Koontz Transform (FKT) [1] is a powerful discriminative subspaces building approach. Previous work has successfully extended FKT to be able to deal with small-sample-size. In this paper, we extend traditional linear FKT to enable it to work in multi-class problem and also in higher dimensional (kernel) subspaces and therefore provide enhanced discrimination ability. We verify the effectiveness of the proposed Kernel Fukunaga-Koontz Transform by demonstrating its effectiveness in face recognition applications; however the proposed non-linear generalization can be applied to any other domain specific problems.", "output": "The abstract text contains these key words below: 'linear Fukunaga-Koontz Transform (FKT)' whose type is Method, 'discriminative subspaces building approach' whose type is Method, 'FKT' whose type is Method, 'small-sample-size' whose type is OtherScientificTerm, 'linear FKT' whose type is Method, 'it' whose type is Generic, 'multi-class problem' whose type is Task, 'higher dimensional (kernel) subspaces' whose type is OtherScientificTerm, 'discrimination ability' whose type is OtherScientificTerm, 'Kernel Fukunaga-Koontz Transform' whose type is Method, 'face recognition applications' whose type is Task, 'non-linear generalization' whose type is Method, 'domain specific problems' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ace images of non-frontal views under poor illumination with low resolution reduce dramatically face recognition accuracy. This is evident most compellingly by the very low recognition rate of all existing face recognition systems when applied to live CCTV camera input. In this paper, we present a Bayesian framework to perform multi-modal (such as variations in viewpoint and illumination) face image super-resolution for recognition in tensor space. Given a single modal low-resolution face image, we benefit from the multiple factor interactions of training tensor, and super-resolve its high-resolution reconstructions across different modalities for face recognition. Instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes, we integrate the tasks of super-resolution and recognition by directly computing a maximum likelihood identity parameter vector in high-resolution tensor space for recognition. We show results from multi-modal super-resolution and face recognition experiments across different imaging modalities, using low-resolution images as testing inputs and demonstrate improved recognition rates over standard tensorface and eigenface representations.", "output": "The abstract text contains these key words below: 'Face images of non-frontal views' whose type is Task, 'face recognition accuracy' whose type is Metric, 'recognition rate' whose type is Metric, 'face recognition systems' whose type is Method, 'live CCTV camera input' whose type is Material, 'Bayesian framework' whose type is Method, 'viewpoint' whose type is OtherScientificTerm, 'illumination' whose type is OtherScientificTerm, 'face image super-resolution' whose type is Task, 'recognition' whose type is Task, 'tensor space' whose type is OtherScientificTerm, 'single modal low-resolution face image' whose type is Material, 'multiple factor interactions of training tensor' whose type is OtherScientificTerm, 'high-resolution reconstructions' whose type is Task, 'modalities' whose type is OtherScientificTerm, 'face recognition' whose type is Task, 'pixel-domain super-resolution and recognition' whose type is Task, 'super-resolution' whose type is Task, 'recognition' whose type is Task, 'maximum likelihood identity parameter vector' whose type is OtherScientificTerm, 'high-resolution tensor space' whose type is OtherScientificTerm, 'recognition' whose type is Task, 'multi-modal super-resolution and face recognition' whose type is Task, 'imaging modalities' whose type is OtherScientificTerm, 'low-resolution images' whose type is Material, 'recognition rates' whose type is Metric, 'tensorface and eigenface representations' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper proposes a novel method of building  polarity-tagged corpus  from  HTML documents  . The characteristics of this method is that it is fully automatic and can be applied to arbitrary  HTML documents  . The idea behind our method is to utilize certain  layout structures  and  linguistic pattern  . By using them, we can automatically extract such  sentences  that express opinion. In our experiment, the method could construct a  corpus  consisting of 126,610  sentences  . ", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'building  polarity-tagged corpus' whose type is Task, 'HTML documents' whose type is Material, 'method' whose type is Generic, 'it' whose type is Generic, 'HTML documents' whose type is Material, 'method' whose type is Generic, 'layout structures' whose type is OtherScientificTerm, 'linguistic pattern' whose type is OtherScientificTerm, 'them' whose type is Generic, 'method' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This article considers approaches which rerank the output of an existing  probabilistic parser  . The base  parser  produces a set of  candidate parses  for each input  sentence  , with associated  probabilities  that define an initial  ranking  of these  parses  . A second  model  then attempts to improve upon this initial  ranking  , using additional  features  of the  tree  as evidence. The strength of our approach is that it allows a  tree  to be represented as an arbitrary set of  features  , without concerns about how these  features  interact or overlap and without the need to define a  derivation  or a  generative model  which takes these  features  into account . We introduce a new method for the  reranking task  , based on the  boosting approach  to  ranking problems  described in Freund et al. (1998). We apply the  boosting method  to  parsing  the  Wall Street Journal treebank  . The method combined the  log-likelihood  under a  baseline model  (that of Collins [1999]) with evidence from an additional 500,000  features  over  parse trees  that were not included in the original  model  . The new  model  achieved 89.75%  F-measure  , a 13% relative decrease in  F-measure  error over the  baseline model's score  of 88.2%. The article also introduces a new algorithm for the  boosting approach  which takes advantage of the  sparsity of the feature space  in the  parsing data  . Experiments show significant efficiency gains for the new algorithm over the obvious  implementation  of the  boosting approach  . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on  feature selection methods  within  log-linear (maximum-entropy) models  . Although the experiments in this article are on  natural language parsing (NLP)  , the approach should be applicable to many other  NLP problems  which are naturally framed as  ranking tasks  , for example,  speech recognition  ,  machine translation  , or  natural language generation  . ", "output": "The abstract text contains these key words below: 'approaches' whose type is Generic, 'probabilistic parser' whose type is Method, 'parser' whose type is Method, 'candidate parses' whose type is OtherScientificTerm, 'ranking' whose type is OtherScientificTerm, 'parses' whose type is Generic, 'model' whose type is Generic, 'ranking' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'tree' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'it' whose type is Generic, 'tree' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'derivation' whose type is OtherScientificTerm, 'generative model' whose type is Method, 'features' whose type is OtherScientificTerm, 'method' whose type is Generic, 'reranking task' whose type is Task, 'boosting approach' whose type is Method, 'ranking problems' whose type is Task, 'boosting method' whose type is Method, 'parsing' whose type is Task, 'Wall Street Journal treebank' whose type is Material, 'method' whose type is Generic, 'log-likelihood' whose type is OtherScientificTerm, 'baseline model' whose type is Method, 'features' whose type is OtherScientificTerm, 'parse trees' whose type is OtherScientificTerm, 'model' whose type is Generic, 'model' whose type is Generic, 'F-measure' whose type is Metric, 'F-measure' whose type is Metric, 'baseline model' whose type is Generic, 'algorithm' whose type is Generic, 'boosting approach' whose type is Method, 'sparsity of the feature space' whose type is OtherScientificTerm, 'parsing data' whose type is Material, 'algorithm' whose type is Generic, 'boosting approach' whose type is Method, 'method' whose type is Generic, 'feature selection methods' whose type is Method, 'log-linear (maximum-entropy) models' whose type is Method, 'natural language parsing (NLP)' whose type is Task, 'NLP problems' whose type is Task, 'ranking tasks' whose type is Task, 'speech recognition' whose type is Task, 'machine translation' whose type is Task, 'natural language generation' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n a motorized vehicle a number of easily measurable signals with frequency components related to the rotational speed of the engine can be found, e.g., vibrations, electrical system voltage level, and ambient sound. These signals could potentially be used to estimate the speed and related states of the vehicle. Unfortunately, such estimates would typically require the relations (scale factors) between the frequency components and the speed for different gears to be known. Consequently, in this article we look at the problem of estimating these gear scale factors from training data consisting only of speed measurements and measurements of the signal in question. The estimation problem is formulated as a maximum likelihood estimation problem and heuristics is used to find initial values for a numerical evaluation of the estimator. Finally, a measurement campaign is conducted and the functionality of the estimation method is verified on real data.", "output": "The abstract text contains these key words below: 'measurable signals' whose type is Material, 'frequency components' whose type is Method, 'rotational speed of the engine' whose type is OtherScientificTerm, 'vibrations' whose type is OtherScientificTerm, 'electrical system voltage level' whose type is OtherScientificTerm, 'ambient sound' whose type is OtherScientificTerm, 'signals' whose type is Generic, 'speed and related states of the vehicle' whose type is Task, 'frequency components' whose type is Method, 'speed' whose type is OtherScientificTerm, 'gears' whose type is OtherScientificTerm, 'gear scale factors' whose type is OtherScientificTerm, 'training data' whose type is Generic, 'speed measurements' whose type is OtherScientificTerm, 'estimation problem' whose type is Task, 'maximum likelihood estimation problem' whose type is Task, 'heuristics' whose type is Method, 'numerical evaluation of the estimator' whose type is Task, 'estimator' whose type is Generic, 'estimation method' whose type is Method, 'real data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he Rete and Treat algorithms are considered the most efficient implementation techniques for Forward Chaining rule systems. These algorithms support a language of limited expressive power. Assertions are not allowed to contain variables, making universal quantification impossible to express except as a rule. In this paper we show how to support full unification in these algorithms. We also show that: Supporting full unification is costly; Full unification is not used frequently; A combination of compile time and run time checks can determine when full unification is not needed. We present data to show that the cost of supporting full unification can be reduced in proportion to the degree that it isn't employed and that for many practical systems this cost is negligible.", "output": "The abstract text contains these key words below: 'Rete and Treat algorithms' whose type is Method, 'implementation techniques' whose type is Method, 'Forward Chaining rule systems' whose type is Task, 'algorithms' whose type is Generic, 'language of limited expressive power' whose type is Method, 'Assertions' whose type is OtherScientificTerm, 'variables' whose type is OtherScientificTerm, 'universal quantification' whose type is OtherScientificTerm, 'rule' whose type is OtherScientificTerm, 'full unification' whose type is Task, 'algorithms' whose type is Generic, 'full unification' whose type is Task, 'Full unification' whose type is Task, 'compile time' whose type is Metric, 'run time' whose type is Metric, 'full unification' whose type is Task, 'full unification' whose type is Task, 'it' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n  Word Identification  has been an important and active issue in  Chinese Natural Language Processing  . In this paper, a new mechanism, based on the concept of  sublanguage  , is proposed for identifying  unknown words  , especially  personal names  , in  Chinese newspapers  . The proposed mechanism includes  title-driven name recognition  ,  adaptive dynamic word formation  ,  identification of 2-character and 3-character Chinese names without title  . We will show the experimental results for two  corpora  and compare them with the results by the  NTHU's statistic-based system  , the only system that we know has attacked the same problem. The experimental results have shown significant improvements over the  WI systems  without the  name identification  capability. ", "output": "The abstract text contains these key words below: 'Word Identification' whose type is Task, 'Chinese Natural Language Processing' whose type is Task, 'mechanism' whose type is Generic, 'sublanguage' whose type is OtherScientificTerm, 'unknown words' whose type is OtherScientificTerm, 'personal names' whose type is OtherScientificTerm, 'Chinese newspapers' whose type is Material, 'mechanism' whose type is Generic, 'title-driven name recognition' whose type is Task, 'adaptive dynamic word formation' whose type is Task, 'identification of 2-character and 3-character Chinese names without title' whose type is Task, 'NTHU's statistic-based system' whose type is Method, 'system' whose type is Generic, 'WI systems' whose type is Method, 'name identification' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD's. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.", "output": "The abstract text contains these key words below: 'recognition scheme' whose type is Method, 'efficiency' whose type is Metric, 'quality' whose type is Metric, 'CD-covers' whose type is Material, 'database' whose type is Generic, 'images of popular music CD's' whose type is Material, 'scheme' whose type is Generic, 'indexing descriptors' whose type is Method, 'local regions' whose type is OtherScientificTerm, 'background clutter' whose type is OtherScientificTerm, 'occlusion' whose type is OtherScientificTerm, 'local region descriptors' whose type is Method, 'vocabulary tree' whose type is OtherScientificTerm, 'vocabulary tree' whose type is OtherScientificTerm, 'retrieval quality' whose type is Metric, 'tree' whose type is OtherScientificTerm, 'quantization' whose type is OtherScientificTerm, 'quantization' whose type is OtherScientificTerm, 'indexing' whose type is OtherScientificTerm, 'recognition quality' whose type is Metric, 'retrieval' whose type is Task, 'database with ground truth' whose type is Material, 'vocabulary tree approach' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "An efficient  bit-vector-based CKY-style parser  for  context-free parsing  is presented. The  parser  computes a compact  parse forest representation  of the complete set of possible  analyses for large treebank grammars  and long  input sentences . The  parser  uses  bit-vector operations  to parallelise the  basic parsing operations . The  parser  is particularly useful when all analyses are needed rather than just the most probable one.\n", "output": "The abstract text contains these key words below: 'bit-vector-based CKY-style parser' whose type is Method, 'context-free parsing' whose type is Task, 'parser' whose type is Method, 'parse forest representation' whose type is Method, 'large treebank grammars' whose type is Method, 'parser' whose type is Method, 'bit-vector operations' whose type is Method, 'parser' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "mage composition (or mosaicing) has attracted a growing attention in recent years as one of the main elements in video analysis and representation. In this paper we deal with the problem of global alignment and super-resolution. We also propose to evaluate the quality of the resulting mosaic by measuring the amount of blurring. Global registration is achieved by combining a graph-based technique \u2013 that exploits the topological structure of the sequence induced by the spatial overlap \u2013 with a bundle adjustment which uses only the homographies computed in the previous steps. Experimental comparison with other techniques shows the effectiveness of our approach.", "output": "The abstract text contains these key words below: 'Image composition (or mosaicing)' whose type is Task, 'video analysis and representation' whose type is Task, 'global alignment' whose type is Task, 'super-resolution' whose type is Task, 'mosaic' whose type is Task, 'amount of blurring' whose type is Metric, 'Global registration' whose type is Task, 'graph-based technique' whose type is Method, 'topological structure' whose type is OtherScientificTerm, 'spatial overlap' whose type is OtherScientificTerm, 'bundle adjustment' whose type is Method, 'homographies' whose type is OtherScientificTerm, 'techniques' whose type is Generic, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "dentity uncertainty is a pervasive problem in real-world data analysis. It arises whenever objects are not labeled with unique identifiers or when those identifiers may not be perceived perfectly. In such cases, two observations may or may not correspond to the same object. In this paper, we consider the problem in the context of citation matching\u2014the problem of deciding which citations correspond to the same publication. Our approach is based on the use of a relational probability model to define a generative model for the domain, including models of author and title corruption and a probabilistic citation grammar. Identity uncertainty is handled by extending standard models to incorporate probabilities over the possible mappings between terms in the language and objects in the domain. Inference is based on Markov chain Monte Carlo, augmented with specific methods for generating efficient proposals when the domain contains many objects. Results on several citation data sets show that the method outperforms current algorithms for citation matching. The declarative, relational nature of the model also means that our algorithm can determine object characteristics such as author names by combining multiple citations of multiple papers.", "output": "The abstract text contains these key words below: 'Identity uncertainty' whose type is Task, 'real-world data analysis' whose type is Task, 'identifiers' whose type is Generic, 'identifiers' whose type is Generic, 'problem' whose type is Generic, 'citation matching' whose type is Task, 'citations' whose type is OtherScientificTerm, 'publication' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'relational probability model' whose type is Method, 'generative model' whose type is Method, 'domain' whose type is Generic, 'models of author and title corruption' whose type is Method, 'probabilistic citation grammar' whose type is Method, 'Identity uncertainty' whose type is Task, 'models' whose type is Generic, 'mappings' whose type is OtherScientificTerm, 'domain' whose type is OtherScientificTerm, 'Inference' whose type is Task, 'Markov chain Monte Carlo' whose type is Method, 'methods' whose type is Generic, 'citation data sets' whose type is Material, 'method' whose type is Generic, 'current algorithms' whose type is Method, 'citation matching' whose type is Task, 'model' whose type is Generic, 'algorithm' whose type is Generic, 'object characteristics' whose type is OtherScientificTerm, 'author names' whose type is OtherScientificTerm, 'citations' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The  PRC Adaptive Knowledge-based Text Understanding System (PAKTUS)  has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools, including a  core English lexicon ,  grammar , and concept representations, for building  natural language processing (NLP) systems  for  text understanding . Systems built with  PAKTUS  are intended to generate input to knowledge based systems ordata base systems. Input to the  NLP system  is typically derived from an existing  electronic message stream , such as a news wire.  PAKTUS  supports the adaptation of the generic core to a variety of domains:  JINTACCS messages ,  RAINFORM messages ,  news reports  about a specific type of event, such as financial transfers or terrorist acts, etc., by acquiring  sublanguage and domain-specific grammar ,  words, conceptual mappings , and  discourse patterns . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success.\n", "output": "The abstract text contains these key words below: 'PRC Adaptive Knowledge-based Text Understanding System (PAKTUS)' whose type is Method, 'system' whose type is Generic, 'core English lexicon' whose type is OtherScientificTerm, 'grammar' whose type is Method, 'concept representations' whose type is Method, 'natural language processing (NLP) systems' whose type is Method, 'text understanding' whose type is Task, 'PAKTUS' whose type is Method, 'knowledge based systems' whose type is Task, 'NLP system' whose type is Method, 'electronic message stream' whose type is Material, 'news wire' whose type is Material, 'PAKTUS' whose type is Method, 'JINTACCS messages' whose type is Material, 'RAINFORM messages' whose type is Material, 'news reports' whose type is OtherScientificTerm, 'event' whose type is OtherScientificTerm, 'financial transfers' whose type is Material, 'terrorist acts' whose type is Material, 'sublanguage and domain-specific grammar' whose type is Method, 'words' whose type is Generic, 'conceptual mappings' whose type is OtherScientificTerm, 'discourse patterns' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We investigate the utility of an  algorithm for translation lexicon acquisition (SABLE) , used previously on a very large  corpus  to acquire general  translation lexicons , when that  algorithm  is applied to a much smaller  corpus  to produce candidates for  domain-specific translation lexicons .\n", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'translation lexicon acquisition (SABLE)' whose type is Task, 'general  translation lexicons' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'domain-specific translation lexicons' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or \"simulator\") of the Markov decision process. However, for high-dimensional continuous-state tasks, it can be extremely difficult to build an accurate model, and thus often the algorithm returns a policy that works in simulation but not in real-life. The other extreme, model-free RL, tends to require infeasibly large numbers of real-life trials. In this paper, we present a hybrid algorithm that requires only an approximate model, and only a small number of real-life trials. The key idea is to successively \"ground\" the policy evaluations using real-life trials, but to rely on the approximate model to suggest local changes. Our theoretical results show that this algorithm achieves near-optimal performance in the real system, even when the model is only approximate. Empirical results also demonstrate that---when given only a crude model and a small number of real-life trials---our algorithm can obtain near-optimal performance in the real system.", "output": "The abstract text contains these key words below: 'model-based policy search approach' whose type is Method, 'reinforcement learning (RL)' whose type is Task, 'policies' whose type is OtherScientificTerm, 'Markov decision process' whose type is OtherScientificTerm, 'high-dimensional continuous-state tasks' whose type is Task, 'model' whose type is Generic, 'algorithm' whose type is Generic, 'policy' whose type is OtherScientificTerm, 'model-free RL' whose type is Task, 'real-life trials' whose type is OtherScientificTerm, 'hybrid algorithm' whose type is Method, 'approximate model' whose type is Method, 'real-life trials' whose type is OtherScientificTerm, 'policy evaluations' whose type is Task, 'real-life trials' whose type is OtherScientificTerm, 'approximate model' whose type is Method, 'algorithm' whose type is Generic, 'near-optimal performance' whose type is OtherScientificTerm, 'crude model' whose type is Generic, 'real-life trials' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'near-optimal performance' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Link detection  has been regarded as a core technology for the  Topic Detection and Tracking tasks  of  new event detection  . In this paper we formulate  story link detection  and  new event detection  as  information retrieval task  and hypothesize on the impact of  precision  and  recall  on both systems. Motivated by these arguments, we introduce a number of new performance enhancing techniques including  part of speech tagging  , new  similarity measures  and expanded  stop lists  . Experimental results validate our hypothesis. ", "output": "The abstract text contains these key words below: 'Link detection' whose type is Task, 'Topic Detection and Tracking tasks  of  new event detection' whose type is Task, 'story link detection' whose type is Task, 'new event detection' whose type is Task, 'information retrieval task' whose type is Task, 'precision' whose type is Metric, 'recall' whose type is Metric, 'systems' whose type is Generic, 'performance enhancing techniques' whose type is Method, 'part of speech tagging' whose type is Method, 'similarity measures' whose type is Method, 'expanded  stop lists' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose an efficient dialogue management for an information navigation system based on a document knowledge base. It is expected that incorporation of appropriate N-best candidates of ASR and contextual information will improve the system performance. The system also has several choices in generating responses or confirmations. In this paper, this selection is optimized as minimization of Bayes risk based on reward for correct information presentation and penalty for redundant turns. We have evaluated this strategy with our spoken dialogue system \" Dialogue Navigator for Kyoto City \" , which also has question-answering capability. Effectiveness of the proposed framework was confirmed in the success rate of retrieval and the average number of turns for information access.", "output": "The abstract text contains these key words below: 'dialogue management' whose type is Method, 'information navigation system' whose type is Task, 'document knowledge base' whose type is Material, 'N-best candidates of ASR' whose type is OtherScientificTerm, 'contextual information' whose type is OtherScientificTerm, 'system' whose type is Generic, 'system' whose type is Generic, 'generating responses or confirmations' whose type is Task, 'minimization of Bayes risk' whose type is Task, 'reward' whose type is Metric, 'correct information presentation' whose type is OtherScientificTerm, 'penalty' whose type is Metric, 'redundant turns' whose type is OtherScientificTerm, 'strategy' whose type is Generic, 'spoken dialogue system \" Dialogue Navigator for Kyoto City \"' whose type is Task, 'question-answering capability' whose type is OtherScientificTerm, 'framework' whose type is Generic, 'success rate of retrieval' whose type is Metric, 'average number of turns' whose type is Metric, 'information access' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Theoretical research in the area of  machine translation  usually involves the search for and creation of an appropriate  formalism . An important issue in this respect is the way in which the  compositionality  of  translation  is to be defined. In this paper, we will introduce the  anaphoric component  of the  Mimo formalism . It makes the definition and  translation  of  anaphoric relations  possible,  relations  which are usually problematic for systems that adhere to  strict compositionality . In  Mimo , the  translation  of  anaphoric relations  is compositional. The  anaphoric component  is used to define  linguistic phenomena  such as  wh-movement , the  passive  and the  binding of reflexives and pronouns  mono-lingually. The actual working of the component will be shown in this paper by means of a detailed discussion of  wh-movement .\n", "output": "The abstract text contains these key words below: 'machine translation' whose type is Task, 'formalism' whose type is Generic, 'translation' whose type is Task, 'anaphoric component' whose type is Method, 'Mimo formalism' whose type is Method, 'translation  of  anaphoric relations' whose type is Task, 'strict compositionality' whose type is OtherScientificTerm, 'Mimo' whose type is OtherScientificTerm, 'translation  of  anaphoric relations' whose type is Task, 'anaphoric component' whose type is Method, 'linguistic phenomena' whose type is OtherScientificTerm, 'wh-movement' whose type is OtherScientificTerm, 'the  passive  and the  binding of reflexives and pronouns' whose type is OtherScientificTerm, 'wh-movement' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents an  unsupervised learning approach  to building a  non-English (Arabic) stemmer  . The  stemming model  is based on  statistical machine translation  and it uses an  English stemmer  and a small (10K sentences)  parallel corpus  as its sole  training resources  . No  parallel text  is needed after the  training phase  .  Monolingual, unannotated text  can be used to further improve the  stemmer  by allowing it to adapt to a desired  domain  or  genre  . Examples and results will be given for  Arabic  , but the approach is applicable to any  language  that needs  affix removal  . Our  resource-frugal approach  results in 87.5%  agreement  with a state of the art, proprietary  Arabic stemmer  built using  rules  ,  affix lists  , and  human annotated text  , in addition to an  unsupervised component  .  Task-based evaluation  using  Arabic information retrieval  indicates an improvement of 22-38% in  average precision  over  unstemmed text  , and 96% of the performance of the proprietary  stemmer  above. ", "output": "The abstract text contains these key words below: 'unsupervised learning approach' whose type is Method, 'non-English (Arabic) stemmer' whose type is Task, 'stemming model' whose type is Method, 'statistical machine translation' whose type is Method, 'it' whose type is Generic, 'English stemmer' whose type is Method, 'parallel corpus' whose type is Material, 'parallel text' whose type is Material, 'Monolingual, unannotated text' whose type is Material, 'stemmer' whose type is Method, 'it' whose type is Generic, 'Arabic' whose type is Material, 'approach' whose type is Generic, 'affix removal' whose type is OtherScientificTerm, 'resource-frugal approach' whose type is Method, 'agreement' whose type is Metric, 'Arabic stemmer' whose type is Method, 'rules' whose type is OtherScientificTerm, 'affix lists' whose type is Material, 'human annotated text' whose type is Material, 'unsupervised component' whose type is Method, 'Task-based evaluation' whose type is Method, 'Arabic information retrieval' whose type is Task, 'average precision' whose type is Metric, 'unstemmed text' whose type is Material, 'stemmer' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Motivated by the success of  ensemble methods  in  machine learning  and other areas of  natural language processing  , we developed a  multi-strategy and multi-source approach to question answering  which is based on combining the results from different  answering agents  searching for  answers  in multiple  corpora  . The  answering agents  adopt fundamentally different strategies, one utilizing primarily  knowledge-based mechanisms  and the other adopting  statistical techniques  . We present our  multi-level answer resolution algorithm  that combines results from the  answering agents  at the  question, passage, and/or answer levels  . Experiments evaluating the effectiveness of our  answer resolution algorithm  show a 35.0% relative improvement over our  baseline system  in the number of  questions correctly answered  , and a 32.8% improvement according to the  average precision metric  . ", "output": "The abstract text contains these key words below: 'ensemble methods' whose type is Method, 'machine learning' whose type is Task, 'natural language processing' whose type is Task, 'multi-strategy and multi-source approach' whose type is Method, 'question answering' whose type is Task, 'answering agents' whose type is Method, 'answering agents' whose type is Method, 'strategies' whose type is Generic, 'one' whose type is Generic, 'knowledge-based mechanisms' whose type is Method, 'other' whose type is Generic, 'statistical techniques' whose type is Method, 'multi-level answer resolution algorithm' whose type is Method, 'answering agents' whose type is Method, 'question, passage, and/or answer levels' whose type is OtherScientificTerm, 'answer resolution algorithm' whose type is Method, 'baseline system' whose type is Generic, 'average precision metric' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we introduce a  modal language LT for imposing  constraints  on  trees , and an extension  LT (LF)  for imposing  constraints  on  trees decorated with feature structures . The motivation for introducing these  languages  is to provide tools for formalising  grammatical frameworks  perspicuously, and the paper illustrates this by showing how the leading ideas of  GPSG  can be captured in  LT (LF) . In addition, the role of  modal languages  (and in particular, what we have called as  constraint formalisms  for linguistic theorising is discussed in some detail.\n", "output": "The abstract text contains these key words below: 'modal language LT' whose type is Method, 'constraints  on  trees' whose type is OtherScientificTerm, 'extension  LT (LF)' whose type is Method, 'constraints  on  trees decorated with feature structures' whose type is OtherScientificTerm, 'languages' whose type is Generic, 'grammatical frameworks' whose type is Method, 'GPSG' whose type is Method, 'LT (LF)' whose type is Method, 'modal languages' whose type is Method, 'constraint formalisms' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "onstraint propagation is one of the key techniques in constraint programming, and a large body of work has built up around it. Special-purpose constraint propagation algorithms frequently make implicit use of short supports \u2014 by examining a subset of the variables, they can infer support (a justification that a variable-value pair still forms part of a solution to the constraint) for all other variables and values and save substantial work. Recently short supports have been used in general purpose prop-agators, and (when the constraint is amenable to short supports) speed ups of more than three orders of magnitude have been demonstrated. In this paper we present SHORTSTR2, a development of the Simple Tabular Reduction algorithm STR2+. We show that SHORTSTR2 is complementary to the existing algorithms SHORTGAC and HAGGISGAC that exploit short supports, while being much simpler. When a constraint is amenable to short supports, the short support set can be exponentially smaller than the full-length support set. Therefore SHORTSTR2 can efficiently propagate many constraints that STR2+ cannot even load into memory. We also show that SHORTSTR2 can be combined with a simple algorithm to identify short supports from full-length supports, to provide a superior drop-in replacement for STR2+.", "output": "The abstract text contains these key words below: 'Constraint propagation' whose type is Method, 'constraint programming' whose type is Method, 'Special-purpose constraint propagation algorithms' whose type is Method, 'general purpose prop-agators' whose type is Method, 'constraint' whose type is OtherScientificTerm, 'SHORTSTR2' whose type is Method, 'Simple Tabular Reduction algorithm STR2+' whose type is Method, 'SHORTSTR2' whose type is Method, 'SHORTGAC' whose type is Method, 'HAGGISGAC' whose type is Method, 'constraint' whose type is OtherScientificTerm, 'short supports' whose type is OtherScientificTerm, 'short support set' whose type is OtherScientificTerm, 'full-length support set' whose type is OtherScientificTerm, 'SHORTSTR2' whose type is Method, 'constraints' whose type is OtherScientificTerm, 'STR2+' whose type is Method, 'SHORTSTR2' whose type is Method, 'algorithm' whose type is Generic, 'short supports' whose type is OtherScientificTerm, 'full-length supports' whose type is OtherScientificTerm, 'drop-in replacement' whose type is OtherScientificTerm, 'STR2+' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he paper assesses the capability of an HMM-based TTS system to produce German speech. The results are discussed in qualitative terms, and compared over three different choices of context features. In addition, the system is adapted to a small set of football announcements, in an exploratory attempt to synthe-sise expressive speech. We conclude that the HMMs are able to produce highly intelligible neutral German speech, with a stable quality, and that the expressivity is partially captured in spite of the small size of the football dataset.", "output": "The abstract text contains these key words below: 'HMM-based TTS system' whose type is Method, 'German speech' whose type is Material, 'context features' whose type is OtherScientificTerm, 'system' whose type is Generic, 'football announcements' whose type is Material, 'expressive speech' whose type is Material, 'HMMs' whose type is Method, 'intelligible neutral German speech' whose type is Material, 'football dataset' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ackground maintenance is a frequent element of video surveillance systems. We develop Wallflower, a three-component system for background maintenance: the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background; the region-level component fills in homogeneous regions of foreground objects; and the frame-level component detects sudden, global changes in the image and swaps in better approximations of the background. We compare our system with 8 other background subtraction algorithms. Wallflower is shown to outperform previous algorithms by handling a greater set of the difficult situations that can occur. Finally, we analyze the experimental results and propose normative principles for background maintenance.", "output": "The abstract text contains these key words below: 'Background maintenance' whose type is Task, 'video surveillance systems' whose type is Task, 'Wallflower' whose type is Method, 'three-component system' whose type is Generic, 'background maintenance' whose type is Task, 'pixel-level component' whose type is Method, 'Wiener filtering' whose type is Method, 'probabilistic predictions of the expected background' whose type is Task, 'region-level component' whose type is Method, 'homogeneous regions of foreground objects' whose type is OtherScientificTerm, 'frame-level component' whose type is Method, 'system' whose type is Generic, 'background subtraction algorithms' whose type is Method, 'Wallflower' whose type is Method, 'algorithms' whose type is Generic, 'normative principles' whose type is Method, 'background maintenance' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe a  dialogue system  that works with its interlocutor to identify objects. Our contributions include a concise,  modular architecture  with reversible processes of  understanding  and  generation , an  information-state model of reference , and flexible links between  semantics  and  collaborative problem solving .\n", "output": "The abstract text contains these key words below: 'dialogue system' whose type is Method, 'concise,  modular architecture' whose type is Method, 'understanding' whose type is Task, 'generation' whose type is Task, 'information-state model of reference' whose type is Method, 'semantics' whose type is OtherScientificTerm, 'collaborative problem solving' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he Interval Algebra (IA) and a subset of the Region Connection Calculus (RCC), namely RCC-8, are the dominant Artificial Intelligence approaches for representing and reasoning about qualitative temporal and topological relations respectively. Such qualitative information can be formulated as a Qualitative Constraint Network (QCN). In this paper, we focus on the minimal labeling problem (MLP) and we propose an algorithm to efficiently derive all the feasible base relations of a QCN. Our algorithm considers chordal QCNs and a new form of partial consistency which we define as \u25c6 G-consistency. Further, the proposed algorithm uses tractable subclasses of relations having a specific patchwork property for which-consistency implies the consistency of the input QCN. Experi-mentations with QCNs of IA and RCC-8 show the importance and efficiency of this new approach.", "output": "The abstract text contains these key words below: 'Interval Algebra (IA)' whose type is Method, 'Region Connection Calculus (RCC)' whose type is Method, 'RCC-8' whose type is Method, 'Artificial Intelligence approaches' whose type is Method, 'representing and reasoning about qualitative temporal and topological relations' whose type is Task, 'qualitative temporal and topological relations' whose type is OtherScientificTerm, 'qualitative information' whose type is OtherScientificTerm, 'Qualitative Constraint Network (QCN)' whose type is Method, 'minimal labeling problem (MLP)' whose type is Task, 'algorithm' whose type is Generic, 'QCN' whose type is Method, 'algorithm' whose type is Generic, 'chordal QCNs' whose type is Method, 'partial consistency' whose type is OtherScientificTerm, '\u25c6 G-consistency' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'patchwork property' whose type is OtherScientificTerm, 'QCN' whose type is Method, 'QCNs' whose type is Method, 'QCNs of IA and RCC-8' whose type is Method, 'IA' whose type is Method, 'RCC-8' whose type is Method, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n We examine the relationship between the two  grammatical formalisms  :  Tree Adjoining Grammars  and  Head Grammars  . We briefly investigate the weak  equivalence  of the two  formalisms  . We then turn to a discussion comparing the  linguistic expressiveness  of the two  formalisms  . ", "output": "The abstract text contains these key words below: 'grammatical formalisms' whose type is Method, 'Tree Adjoining Grammars' whose type is Method, 'Head Grammars' whose type is Method, 'formalisms' whose type is Generic, 'linguistic expressiveness' whose type is OtherScientificTerm, 'formalisms' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n We investigate independent and relevant event-based extractive  mutli-document summarization approaches  . In this paper,  events  are defined as  event terms  and  associated event elements  . With independent approach, we identify important  contents  by frequency of  events  . With relevant approach, we identify important contents by  PageRank algorithm  on the  event map  constructed from  documents  . Experimental results are encouraging. ", "output": "The abstract text contains these key words below: 'independent and relevant event-based extractive  mutli-document summarization approaches' whose type is Method, 'independent approach' whose type is Method, 'relevant approach' whose type is Method, 'PageRank algorithm' whose type is Method, 'event map' whose type is OtherScientificTerm, 'documents' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper addresses the problem of optimal alignment of non-rigid surfaces from multi-view video observations to obtain a temporally consistent representation. Conventional non-rigid surface tracking performs frame-to-frame alignment which is subject to the accumulation of errors resulting in a drift over time. Recently, non-sequential tracking approaches have been introduced which reorder the input data based on a dissimilarity measure. One or more input sequences are represented in a tree with reduced alignment path length. They demonstrate a reduced drift and increased robustness to large non-rigid deformations. However, jumps may occur in the aligned mesh sequence where branches of the tree meet due to different error accumulation along them. Optimisation of the tree for non-sequential tracking, which minimises the errors in temporal consistency due to both the drift and the jumps, is proposed. A novel cluster tree enforces sequential tracking in local segments of the sequence while allowing global non-sequential traversal among these segments. This provides a mechanism to create a tree structure which reduces the number and size of jumps between branches and limits the lenght of branches as well. Comprehensive evaluation is performed on a variety of challenging non-rigid surfaces including face, cloth and people. It demonstrates that the proposed cluster tree achieves better temporal consistency than the previous sequential and non-sequential tracking approaches. Quantitative analysis on a created synthetic facial performance also shows an improvement by the cluster tree.", "output": "The abstract text contains these key words below: 'optimal alignment of non-rigid surfaces' whose type is Task, 'multi-view video observations' whose type is OtherScientificTerm, 'temporally consistent representation' whose type is Method, 'non-rigid surface tracking' whose type is Method, 'frame-to-frame alignment' whose type is Method, 'non-sequential tracking approaches' whose type is Method, 'dissimilarity measure' whose type is Metric, 'reduced drift' whose type is OtherScientificTerm, 'robustness' whose type is Metric, 'non-rigid deformations' whose type is OtherScientificTerm, 'branches' whose type is OtherScientificTerm, 'tree' whose type is OtherScientificTerm, 'error accumulation' whose type is OtherScientificTerm, 'Optimisation of the tree' whose type is Task, 'non-sequential tracking' whose type is Method, 'temporal consistency' whose type is Metric, 'cluster tree' whose type is Method, 'sequential tracking in local segments' whose type is Task, 'local segments' whose type is OtherScientificTerm, 'global non-sequential traversal' whose type is OtherScientificTerm, 'segments' whose type is Generic, 'tree structure' whose type is OtherScientificTerm, 'non-rigid surfaces' whose type is OtherScientificTerm, 'face' whose type is OtherScientificTerm, 'cloth' whose type is OtherScientificTerm, 'people' whose type is OtherScientificTerm, 'cluster tree' whose type is Method, 'temporal consistency' whose type is Metric, 'sequential and non-sequential tracking approaches' whose type is Method, 'synthetic facial performance' whose type is Metric, 'cluster tree' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Topical blog post retrieval  is the task of ranking  blog posts  with respect to their  relevance  for a given  topic . To improve  topical blog post retrieval  we incorporate  textual credibility indicators  in the  retrieval process . We consider two groups of  indicators : post level (determined using information about individual  blog posts  only) and blog level (determined using information from the underlying  blogs ). We describe how to estimate these  indicators  and how to integrate them into a  retrieval approach  based on  language models . Experiments on the  TREC Blog track test set  show that both groups of  credibility indicators  significantly improve  retrieval effectiveness ; the best performance is achieved when combining them.\n", "output": "The abstract text contains these key words below: 'Topical blog post retrieval' whose type is Task, 'ranking  blog posts' whose type is Task, 'blog posts' whose type is Material, 'relevance' whose type is Metric, 'topical blog post retrieval' whose type is Task, 'textual credibility indicators' whose type is OtherScientificTerm, 'retrieval process' whose type is Method, 'indicators' whose type is Generic, 'blog posts' whose type is Material, 'blogs' whose type is Material, 'indicators' whose type is Generic, 'them' whose type is Generic, 'retrieval approach' whose type is Method, 'language models' whose type is Method, 'TREC Blog track test set' whose type is Material, 'credibility indicators' whose type is OtherScientificTerm, 'retrieval effectiveness' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "raphical models such as Bayesian Networks (BNs) are being increasingly applied to various computer vision problems. One bottleneck in using BN is that learning the BN model parameters often requires a large amount of reliable and representative training data, which proves to be difficult to acquire for many computer vision tasks. On the other hand, there is often available qualitative prior knowledge about the model. Such knowledge comes either from domain experts based on their experience or from various physical or geometric constraints that govern the objects we try to model. Unlike the quantitative prior, the qualitative prior is often ignored due to the difficulty of incorporating them into the model learning process. In this paper, we introduce a closed-form solution to systematically combine the limited training data with some generic qualitative knowledge for BN parameter learning. To validate our method, we compare it with the Maximum Likelihood (ML) estimation method under sparse data and with the Expectation Maximization (EM) algorithm under incomplete data respectively. To further demonstrate its applications for computer vision, we apply it to learn a BN model for facial Action Unit (AU) recognition from real image data. The experimental results show that with simple and generic qualitative constraints and using only a small amount of training data, our method can robustly and accurately estimate the BN model parameters.", "output": "The abstract text contains these key words below: 'Graphical models' whose type is Method, 'Bayesian Networks (BNs)' whose type is Method, 'computer vision problems' whose type is Task, 'BN' whose type is Method, 'BN model parameters' whose type is OtherScientificTerm, 'representative training data' whose type is Material, 'computer vision tasks' whose type is Task, 'qualitative prior knowledge' whose type is OtherScientificTerm, 'model' whose type is Generic, 'knowledge' whose type is Generic, 'domain experts' whose type is OtherScientificTerm, 'physical or geometric constraints' whose type is OtherScientificTerm, 'quantitative prior' whose type is OtherScientificTerm, 'qualitative prior' whose type is OtherScientificTerm, 'them' whose type is Generic, 'model learning process' whose type is Task, 'closed-form solution' whose type is Method, 'limited training data' whose type is Material, 'qualitative knowledge' whose type is OtherScientificTerm, 'BN parameter learning' whose type is Method, 'method' whose type is Generic, 'it' whose type is Generic, 'Maximum Likelihood (ML) estimation method' whose type is Method, 'sparse data' whose type is Material, 'Expectation Maximization (EM) algorithm' whose type is Method, 'incomplete data' whose type is Material, 'computer vision' whose type is Task, 'it' whose type is Generic, 'BN model' whose type is Method, 'facial Action Unit (AU) recognition' whose type is Task, 'real image data' whose type is Material, 'generic qualitative constraints' whose type is OtherScientificTerm, 'training data' whose type is Material, 'method' whose type is Generic, 'BN model parameters' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he goal of this paper is to discover a set of discriminative patches which can serve as a fully unsupervised mid-level visual representation. The desired patches need to satisfy two requirements: 1) to be representative, they need to occur frequently enough in the visual world; 2) to be discriminative, they need to be different enough from the rest of the visual world. The patches could correspond to parts, objects, \" visual phrases \" , etc. but are not restricted to be any one of them. We pose this as an unsupervised discriminative clustering problem on a huge dataset of image patches. We use an iterative procedure which alternates between clustering and training discriminative classifiers, while applying careful cross-validation at each step to prevent overfitting. The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation, suggesting that it could be used in place of visual words for many tasks. Furthermore, discrim-inative patches can also be used in a supervised regime, such as scene classification, where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset.", "output": "The abstract text contains these key words below: 'discriminative patches' whose type is Method, 'unsupervised mid-level visual representation' whose type is Task, 'patches' whose type is Generic, 'this' whose type is Generic, 'unsupervised discriminative clustering problem' whose type is Task, 'image patches' whose type is Material, 'iterative procedure' whose type is Method, 'clustering' whose type is Method, 'training discriminative classifiers' whose type is Method, 'cross-validation' whose type is Method, 'overfitting' whose type is OtherScientificTerm, 'discriminative patches' whose type is Method, 'unsupervised mid-level visual representation' whose type is Task, 'it' whose type is Generic, 'discrim-inative patches' whose type is Method, 'supervised regime' whose type is Task, 'scene classification' whose type is Task, 'they' whose type is Generic, 'MIT Indoor-67 dataset' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we describe the research using  machine learning techniques  to build a  comma checker  to be integrated in a  grammar checker  for  Basque  . After several experiments, and trained with a little  corpus  of 100,000  words  , the system guesses correctly not placing  commas  with a  precision  of 96% and a  recall  of 98%. It also gets a  precision  of 70% and a  recall  of 49% in the task of placing  commas  . Finally, we have shown that these results can be improved using a bigger and a more homogeneous  corpus  to train, that is, a bigger  corpus  written by one unique  author  . ", "output": "The abstract text contains these key words below: 'machine learning techniques' whose type is Method, 'comma checker' whose type is Method, 'grammar checker' whose type is Method, 'Basque' whose type is Material, 'system' whose type is Generic, 'precision' whose type is Metric, 'recall' whose type is Metric, 'It' whose type is Generic, 'precision' whose type is Metric, 'recall' whose type is Metric, 'placing  commas' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ncertainty handling plays an important role during shape tracking. We have recently shown that the fusion of measurement information with system dynamics and shape priors greatly improves the tracking performance for very noisy images such as ultrasound sequences [22]. Nevertheless , this approach required user initialization of the tracking process. This paper solves the automatic initial-ization problem by performing boosted shape detection as a generic measurement process and integrating it in our tracking framework. We show how to propagate the local detection uncertainties of multiple shape candidates during shape alignment, fusion with the predicted shape prior, and fusion with subspace constraints. As a result, we treat all sources of information in a unified way and derive the posterior shape model as the shape with the maximum likelihood. Our framework is applied for the automatic tracking of endocardium in ultrasound sequences of the human heart. Reliable detection and robust tracking results are achieved when compared to existing approaches and inter-expert variations.", "output": "The abstract text contains these key words below: 'Uncertainty handling' whose type is Task, 'shape tracking' whose type is Task, 'fusion of measurement information with system dynamics and shape priors' whose type is Method, 'tracking' whose type is Task, 'noisy images' whose type is Material, 'ultrasound sequences' whose type is Material, 'approach' whose type is Generic, 'user initialization' whose type is OtherScientificTerm, 'tracking process' whose type is Method, 'automatic initial-ization problem' whose type is Task, 'boosted shape detection' whose type is Method, 'generic measurement process' whose type is Method, 'it' whose type is Generic, 'tracking framework' whose type is Method, 'local detection uncertainties' whose type is OtherScientificTerm, 'local detection uncertainties of multiple shape candidates' whose type is OtherScientificTerm, 'shape alignment' whose type is OtherScientificTerm, 'predicted shape prior' whose type is OtherScientificTerm, 'subspace constraints' whose type is OtherScientificTerm, 'posterior shape model' whose type is Method, 'maximum likelihood' whose type is OtherScientificTerm, 'framework' whose type is Generic, 'automatic tracking of endocardium' whose type is Task, 'endocardium' whose type is OtherScientificTerm, 'ultrasound sequences of the human heart' whose type is Material, 'detection' whose type is Task, 'tracking' whose type is Task, 'approaches' whose type is Generic, 'inter-expert variations' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper we evaluate four objective measures of speech with regards to intelligibility prediction of synthesized speech in diverse noisy situations. We evaluated three intel-ligibility measures, the Dau measure, the glimpse proportion and the Speech Intelligibility Index (SII) and a quality measure , the Perceptual Evaluation of Speech Quality (PESQ). For the generation of synthesized speech we used a state of the art HMM-based speech synthesis system. The noisy conditions comprised four additive noises. The measures were compared with subjective intelligibility scores obtained in listening tests. The results show the Dau and the glimpse measures to be the best predictors of intelligibility, with correlations of around 0.83 to subjective scores. All measures gave less accurate predictions of intelligibility for synthetic speech than have previously been found for natural speech; in particular the SII measure. In additional experiments, we processed the synthesized speech by an ideal binary mask before adding noise. The Glimpse measure gave the most accurate intelligibility predictions in this situation.", "output": "The abstract text contains these key words below: 'measures of speech' whose type is Metric, 'intelligibility prediction' whose type is Task, 'synthesized speech' whose type is Material, 'diverse noisy situations' whose type is OtherScientificTerm, 'intel-ligibility measures' whose type is Metric, 'Dau measure' whose type is Metric, 'glimpse proportion' whose type is Metric, 'Speech Intelligibility Index (SII)' whose type is Metric, 'quality measure' whose type is Metric, 'Perceptual Evaluation of Speech Quality (PESQ)' whose type is Metric, 'generation of synthesized speech' whose type is Task, 'HMM-based speech synthesis system' whose type is Method, 'noisy conditions' whose type is OtherScientificTerm, 'additive noises' whose type is OtherScientificTerm, 'measures' whose type is Generic, 'subjective intelligibility scores' whose type is Metric, 'Dau' whose type is Metric, 'glimpse measures' whose type is Metric, 'predictors of intelligibility' whose type is Method, 'correlations' whose type is Metric, 'subjective scores' whose type is OtherScientificTerm, 'measures' whose type is Generic, 'predictions of intelligibility' whose type is Task, 'synthetic speech' whose type is Material, 'natural speech' whose type is Material, 'SII measure' whose type is Metric, 'synthesized speech' whose type is Material, 'ideal binary mask' whose type is Method, 'Glimpse measure' whose type is Metric, 'intelligibility predictions' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n For  intelligent interactive systems  to communicate with  humans  in a natural manner, they must have knowledge about the  system users  . This paper explores the role of  user modeling  in such  systems  . It begins with a characterization of what a  user model  is and how it can be used. The types of information that a  user model  may be required to keep about a  user  are then identified and discussed.  User models  themselves can vary greatly depending on the requirements of the situation and the implementation, so several dimensions along which they can be classified are presented. Since acquiring the knowledge for a  user model  is a fundamental problem in  user modeling  , a section is devoted to this topic. Next, the benefits and costs of implementing a  user modeling component  for a system are weighed in light of several aspects of the  interaction requirements  that may be imposed by the system. Finally, the current state of research in  user modeling  is summarized, and future research topics that must be addressed in order to achieve powerful, general  user modeling systems  are assessed. ", "output": "The abstract text contains these key words below: 'intelligent interactive systems' whose type is Method, 'they' whose type is Generic, 'user modeling' whose type is Task, 'systems' whose type is Method, 'user model' whose type is Method, 'it' whose type is Generic, 'user model' whose type is Method, 'User models' whose type is Method, 'they' whose type is Generic, 'user model' whose type is Method, 'user modeling' whose type is Task, 'user modeling component' whose type is Method, 'system' whose type is Generic, 'system' whose type is Generic, 'user modeling' whose type is Task, 'user modeling systems' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "uman action recognition from well-segmented 3D skeleton data has been intensively studied and attracting an increasing attention. Online action detection goes one step further and is more challenging, which identifies the action type and localizes the action positions on the fly from the untrimmed stream. In this paper, we study the problem of online action detection from the streaming skeleton data. We propose a multi-task end-to-end Joint Classification-Regression Recurrent Neural Network to better explore the action type and temporal localiza-tion information. By employing a joint classification and regression optimization objective, this network is capable of automatically localizing the start and end points of actions more accurately. Specifically, by leveraging the merits of the deep Long Short-Term Memory (LSTM) subnetwork, the proposed model automatically captures the complex long-range temporal dynamics, which naturally avoids the typical sliding window design and thus ensures high computational efficiency. Furthermore, the subtask of regression optimization provides the ability to forecast the action prior to its occurrence. To evaluate our proposed model, we build a large streaming video dataset with annotations. Experimental results on our dataset and the public G3D dataset both demonstrate very promising performance of our scheme.", "output": "The abstract text contains these key words below: 'Human action recognition' whose type is Task, 'well-segmented 3D skeleton data' whose type is Material, 'Online action detection' whose type is Task, 'action type' whose type is OtherScientificTerm, 'action positions' whose type is OtherScientificTerm, 'untrimmed stream' whose type is Material, 'online action detection' whose type is Task, 'streaming skeleton data' whose type is Material, 'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network' whose type is Method, 'action type' whose type is OtherScientificTerm, 'temporal localiza-tion information' whose type is OtherScientificTerm, 'joint classification and regression optimization objective' whose type is OtherScientificTerm, 'network' whose type is Generic, 'deep Long Short-Term Memory (LSTM) subnetwork' whose type is Method, 'model' whose type is Generic, 'long-range temporal dynamics' whose type is OtherScientificTerm, 'sliding window design' whose type is Method, 'computational efficiency' whose type is OtherScientificTerm, 'regression optimization' whose type is Task, 'model' whose type is Generic, 'streaming video dataset' whose type is Material, 'dataset' whose type is Generic, 'G3D dataset' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present a new approach for building an efficient and robust classifier for the two class problem, that localizes objects that may appear in the image under different orien-tations. In contrast to other works that address this problem using multiple classifiers, each one specialized for a specific orientation, we propose a simple two-step approach with an estimation stage and a classification stage. The estimator yields an initial set of potential object poses that are then validated by the classifier. This methodology allows reducing the time complexity of the algorithm while classification results remain high. The classifier we use in both stages is based on a boosted combination of Random Ferns over local histograms of oriented gradients (HOGs), which we compute during a pre-processing step. Both the use of supervised learning and working on the gradient space makes our approach robust while being efficient at run-time. We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations, and with challenging conditions such as cluttered backgrounds, changing illumination conditions and partial occlusions.", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'classifier' whose type is Method, 'class problem' whose type is Task, 'image' whose type is Material, 'orien-tations' whose type is OtherScientificTerm, 'problem' whose type is Generic, 'classifiers' whose type is Method, 'approach' whose type is Generic, 'estimation stage' whose type is Method, 'classification stage' whose type is Method, 'estimator' whose type is Method, 'object poses' whose type is OtherScientificTerm, 'classifier' whose type is Method, 'time complexity' whose type is Metric, 'algorithm' whose type is Generic, 'classification' whose type is Task, 'classifier' whose type is Method, 'boosted combination of Random Ferns' whose type is Method, 'local histograms of oriented gradients (HOGs)' whose type is OtherScientificTerm, 'pre-processing step' whose type is Method, 'supervised learning' whose type is Method, 'gradient space' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'database' whose type is Generic, 'motorbikes under planar rotations' whose type is Material, 'conditions' whose type is Generic, 'cluttered backgrounds' whose type is OtherScientificTerm, 'changing illumination conditions' whose type is OtherScientificTerm, 'partial occlusions' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents a  word segmentation system  in France Telecom R&D Beijing, which uses a unified approach to  word breaking  and  OOV identification . The  output  can be customized to meet different  segmentation standards  through the application of an ordered list of transformation. The  system  participated in all the tracks of the  segmentation bakeoff  --  PK-open ,  PK-closed ,  AS-open ,  AS-closed ,  HK-open ,  HK-closed ,  MSR-open  and  MSR- closed  -- and achieved the  state-of-the-art performance  in  MSR-open ,  MSR-close  and  PK-open  tracks. Analysis of the results shows that each component of the system contributed to the  scores .\n", "output": "The abstract text contains these key words below: 'word segmentation system' whose type is Method, 'approach' whose type is Generic, 'word breaking' whose type is Task, 'OOV identification' whose type is Task, 'system' whose type is Generic, 'segmentation bakeoff' whose type is OtherScientificTerm, 'PK-open' whose type is OtherScientificTerm, 'PK-closed' whose type is OtherScientificTerm, 'AS-open' whose type is OtherScientificTerm, 'AS-closed' whose type is OtherScientificTerm, 'HK-open' whose type is OtherScientificTerm, 'HK-closed' whose type is OtherScientificTerm, 'MSR-open' whose type is OtherScientificTerm, 'MSR- closed' whose type is OtherScientificTerm, 'MSR-open' whose type is OtherScientificTerm, 'MSR-close' whose type is OtherScientificTerm, 'PK-open' whose type is OtherScientificTerm, 'system' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n Bayesian machine learning, conjugate priors are popular, mostly due to mathematical convenience. In this paper, we show that there are deeper reasons for choosing a conjugate prior. Specifically, we formulate the conjugate prior in the form of Bregman divergence and show that it is the inherent geometry of conjugate priors that makes them appropriate and intuitive. This geometric interpretation allows one to view the hyperparameters of conjugate priors as the effective sample points, thus providing additional intuition. We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning.", "output": "The abstract text contains these key words below: 'Bayesian machine learning' whose type is Task, 'conjugate priors' whose type is OtherScientificTerm, 'conjugate prior' whose type is OtherScientificTerm, 'conjugate prior' whose type is OtherScientificTerm, 'Bregman divergence' whose type is OtherScientificTerm, 'conjugate priors' whose type is OtherScientificTerm, 'conjugate priors' whose type is OtherScientificTerm, 'geometric understanding of conjugate priors' whose type is OtherScientificTerm, 'conjugate priors' whose type is OtherScientificTerm, 'hyperparameters' whose type is Generic, 'prior' whose type is Generic, 'generative and discriminative components' whose type is Method, 'hybrid model' whose type is Method, 'semi-supervised learning' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Recently, we initiated a project to develop a  phonetically-based spoken language understanding system  called  SUMMIT . In contrast to many of the past efforts that make use of  heuristic rules  whose development requires intense  knowledge engineering , our approach attempts to express the  speech knowledge  within a formal framework using well-defined mathematical tools. In our system,  features  and  decision strategies  are discovered and trained automatically, using a large body of  speech data . This paper describes the system, and documents its current performance.\n", "output": "The abstract text contains these key words below: 'phonetically-based spoken language understanding system' whose type is Method, 'SUMMIT' whose type is Method, 'heuristic rules' whose type is Method, 'knowledge engineering' whose type is Method, 'approach' whose type is Generic, 'speech knowledge' whose type is OtherScientificTerm, 'mathematical tools' whose type is Method, 'system' whose type is Generic, 'features' whose type is OtherScientificTerm, 'decision strategies' whose type is Method, 'speech data' whose type is Material, 'system' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "long with the increasing requirements, the hash-tag recommendation task for microblogs has been receiving considerable attention in recent years. Various researchers have studied the problem from different aspects. However, most of these methods usually need handcrafted features. Motivated by the successful use of convolutional neural networks (CNNs) for many natural language processing tasks, in this paper, we adopt CNNs to perform the hashtag recommendation problem. To incorporate the trigger words whose effectiveness have been experimentally evaluated in several previous works, we propose a novel architecture with an attention mechanism. The results of experiments on the data collected from a real world microblogging service demonstrated that the proposed model outperforms state-of-the-art methods. By incorporating trigger words into the consideration, the relative improvement of the proposed method over the state-of-the-art method is around 9.4% in the F1-score.", "output": "The abstract text contains these key words below: 'hash-tag recommendation task' whose type is Task, 'microblogs' whose type is Material, 'handcrafted features' whose type is OtherScientificTerm, 'convolutional neural networks (CNNs)' whose type is Method, 'natural language processing tasks' whose type is Task, 'CNNs' whose type is Method, 'hashtag recommendation problem' whose type is Task, 'trigger words' whose type is OtherScientificTerm, 'architecture' whose type is Generic, 'attention mechanism' whose type is OtherScientificTerm, 'data' whose type is Generic, 'model' whose type is Generic, 'state-of-the-art methods' whose type is Generic, 'trigger words' whose type is OtherScientificTerm, 'method' whose type is Generic, 'state-of-the-art method' whose type is Generic, 'F1-score' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose to incorporate a priori geometric constraints in a 3\u2013D stereo reconstruction scheme to cope with the many cases where image information alone is not sufficient to accurately recover 3\u2013D shape. Our approach is based on the iterative deformation of a 3\u2013D surface mesh to minimize an objective function. We show that combining anisotropic meshing with a non-quadratic approach to regularization enables us to obtain satisfactory reconstruction results using triangulations with few vertices. Structural or numerical constraints can then be added locally to the reconstruction process through a constrained optimization scheme. They improve the reconstruction results and enforce their consistency with a priori knowledge about object shape. The strong description and modeling properties of differential features make them useful tools that can be efficiently used as constraints for 3\u2013D reconstruction .", "output": "The abstract text contains these key words below: 'priori geometric constraints' whose type is OtherScientificTerm, '3\u2013D stereo reconstruction scheme' whose type is Method, 'image information' whose type is OtherScientificTerm, '3\u2013D shape' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'iterative deformation of a 3\u2013D surface mesh' whose type is Method, 'objective function' whose type is OtherScientificTerm, 'anisotropic meshing' whose type is Method, 'non-quadratic approach' whose type is Method, 'regularization' whose type is OtherScientificTerm, 'reconstruction' whose type is Task, 'triangulations' whose type is OtherScientificTerm, 'vertices' whose type is OtherScientificTerm, 'Structural or numerical constraints' whose type is OtherScientificTerm, 'reconstruction process' whose type is Method, 'constrained optimization scheme' whose type is Method, 'They' whose type is Generic, 'reconstruction' whose type is Task, 'priori knowledge' whose type is OtherScientificTerm, 'object shape' whose type is OtherScientificTerm, 'modeling properties' whose type is OtherScientificTerm, 'differential features' whose type is OtherScientificTerm, 'them' whose type is Generic, '3\u2013D reconstruction' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n some auction domains, there is uncertainty regarding the final availability of the goods being auctioned off. For example, a government may auction off spectrum from its public safety network, but it may need this spectrum back in times of emergency. In such a domain, standard combinatorial auctions perform poorly because they lead to violations of individual rationality (IR), even in expectation, and to very low efficiency. In this paper, we study the design of core-selecting payment rules for such domains. Surprisingly, we show that in this new domain , there does not exist a payment rule with is guaranteed to be ex-post core-selecting. However, we show that by designing rules that are \" execution-contingent, \" i.e., by charging payments that are conditioned on the realization of the availability of the goods, we can reduce IR violations. We design two core-selecting rules that always satisfy IR in expectation. To study the performance of our rules we perform a computational Bayes-Nash equilibrium analysis. We show that, in equilibrium, our new rules have better incentives, higher efficiency, and a lower rate of ex-post IR violations than standard core-selecting rules.", "output": "The abstract text contains these key words below: 'auction domains' whose type is Task, 'domain' whose type is Generic, 'combinatorial auctions' whose type is Method, 'they' whose type is Generic, 'violations of individual rationality (IR)' whose type is OtherScientificTerm, 'individual rationality (IR)' whose type is OtherScientificTerm, 'design of core-selecting payment rules' whose type is Task, 'domains' whose type is Generic, 'domain' whose type is Generic, 'payment rule' whose type is OtherScientificTerm, 'rules' whose type is OtherScientificTerm, 'IR violations' whose type is OtherScientificTerm, 'core-selecting rules' whose type is OtherScientificTerm, 'IR' whose type is OtherScientificTerm, 'rules' whose type is Generic, 'computational Bayes-Nash equilibrium analysis' whose type is Method, 'rules' whose type is Generic, 'rate of ex-post IR violations' whose type is Metric, 'core-selecting rules' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Automatic estimation  of  word significance  oriented for  speech-based Information Retrieval (IR)  is addressed. Since the  significance  of  words  differs in  IR ,  automatic speech recognition (ASR) performance  has been evaluated based on  weighted word error rate (WWER) , which gives a  weight  on errors from the viewpoint of  IR , instead of  word error rate (WER) , which treats all  words  uniformly. A  decoding strategy  that minimizes  WWER  based on a  Minimum Bayes-Risk framework  has been shown, and the reduction of errors on both  ASR  and  IR  has been reported. In this paper, we propose an  automatic estimation method  for  word significance (weights)  based on its influence on  IR . Specifically,  weights  are estimated so that  evaluation measures  of  ASR  and  IR  are equivalent. We apply the proposed method to a  speech-based information retrieval system , which is a typical  IR system , and show that the method works well.\n", "output": "The abstract text contains these key words below: 'Automatic estimation  of  word significance' whose type is Task, 'word significance' whose type is OtherScientificTerm, 'speech-based Information Retrieval (IR)' whose type is Task, 'significance' whose type is OtherScientificTerm, 'IR' whose type is Task, 'automatic speech recognition (ASR)' whose type is Task, 'weighted word error rate (WWER)' whose type is Metric, 'IR' whose type is Task, 'word error rate (WER)' whose type is Metric, 'decoding strategy' whose type is Method, 'WWER' whose type is Metric, 'Minimum Bayes-Risk framework' whose type is Method, 'ASR' whose type is Task, 'IR' whose type is Task, 'automatic estimation method' whose type is Method, 'word significance (weights)' whose type is OtherScientificTerm, 'IR' whose type is Task, 'evaluation measures' whose type is Metric, 'ASR' whose type is Task, 'IR' whose type is Task, 'method' whose type is Generic, 'speech-based information retrieval system' whose type is Task, 'IR system' whose type is Task, 'method' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The goal of this research is to develop a  spoken language system  that will demonstrate the usefulness of  voice input  for  interactive problem solving . The system will accept  continuous speech , and will handle  multiple speakers  without  explicit speaker enrollment . Combining  speech recognition  and  natural language processing  to achieve  speech understanding , the system will be demonstrated in an  application domain  relevant to the DoD. The objective of this project is to develop a  robust and high-performance speech recognition system  using a  segment-based approach  to  phonetic recognition . The  recognition system  will eventually be integrated with  natural language processing  to achieve  spoken language understanding .\n", "output": "The abstract text contains these key words below: 'spoken language system' whose type is Method, 'voice input' whose type is Material, 'interactive problem solving' whose type is Task, 'continuous speech' whose type is Material, 'speech recognition' whose type is Task, 'natural language processing' whose type is Task, 'speech understanding' whose type is Task, 'system' whose type is Generic, 'robust and high-performance speech recognition system' whose type is Method, 'segment-based approach' whose type is Method, 'phonetic recognition' whose type is Task, 'recognition system' whose type is Method, 'natural language processing' whose type is Task, 'spoken language understanding' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents an approach to estimate the intrinsic texture properties (albedo, shading, normal) of scenes from multiple view acquisition under unknown illumination conditions. We introduce the concept of intrinsic textures, which are pixel-resolution surface textures representing the intrinsic appearance parameters of a scene. Unlike previous video relighting methods, the approach does not assume regions of uniform albedo, which makes it applicable to richly textured scenes. We show that intrinsic image methods can be used to refine an initial, low-frequency shading estimate based on a global lighting reconstruction from an original texture and coarse scene geometry in order to resolve the inherent global ambiguity in shading. The method is applied to relight-ing of free-viewpoint rendering from multiple view video capture. This demonstrates relighting with reproduction of fine surface detail.\n", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'intrinsic texture properties (albedo, shading, normal) of scenes' whose type is Task, 'multiple view acquisition' whose type is Task, 'unknown illumination conditions' whose type is OtherScientificTerm, 'intrinsic textures' whose type is OtherScientificTerm, 'pixel-resolution surface textures' whose type is OtherScientificTerm, 'intrinsic appearance parameters' whose type is OtherScientificTerm, 'video relighting methods' whose type is Method, 'approach' whose type is Generic, 'uniform albedo' whose type is OtherScientificTerm, 'it' whose type is Generic, 'richly textured scenes' whose type is Material, 'intrinsic image methods' whose type is Method, 'initial, low-frequency shading estimate' whose type is Task, 'global lighting reconstruction' whose type is OtherScientificTerm, 'texture and coarse scene geometry' whose type is OtherScientificTerm, 'inherent global ambiguity in shading' whose type is OtherScientificTerm, 'method' whose type is Generic, 'relight-ing of free-viewpoint rendering' whose type is Task, 'multiple view video capture' whose type is OtherScientificTerm, 'relighting' whose type is Task, 'reproduction of fine surface detail' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper proposes an approach to  full parsing  suitable for  Information Extraction  from  texts . Sequences of cascades of  rules  deterministically analyze the  text , building  unambiguous structures . Initially basic  chunks  are analyzed; then  argumental relations  are recognized; finally  modifier attachment  is performed and the  global parse tree  is built. The approach was proven to work for three  languages  and different  domains . It was implemented in the  IE module  of  FACILE, a EU project for multilingual text classification and IE .\n", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'full parsing' whose type is Task, 'Information Extraction' whose type is Task, 'rules' whose type is OtherScientificTerm, 'unambiguous structures' whose type is OtherScientificTerm, 'argumental relations' whose type is OtherScientificTerm, 'modifier attachment' whose type is OtherScientificTerm, 'global parse tree' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'It' whose type is Generic, 'IE module' whose type is Method, 'FACILE, a EU project for multilingual text classification and IE' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "An empirical comparison of  CFG filtering techniques  for  LTAG  and  HPSG  is presented. We demonstrate that an approximation of  HPSG  produces a more effective  CFG filter  than that of  LTAG  . We also investigate the reason for that difference. ", "output": "The abstract text contains these key words below: 'CFG filtering techniques' whose type is Method, 'LTAG' whose type is Method, 'HPSG' whose type is Method, 'approximation of  HPSG' whose type is Method, 'HPSG' whose type is Method, 'CFG filter' whose type is Method, 'that' whose type is Generic, 'LTAG' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper proposes a  Hidden Markov Model (HMM)  and an  HMM-based chunk tagger , from which a  named entity (NE) recognition (NER) system  is built to recognize and classify  names ,  times and numerical quantities . Through the  HMM , our system is able to apply and integrate four types of internal and external evidences : 1) simple deterministic internal feature of the  words , such as  capitalization  and digitalization ; 2)  internal semantic feature  of important triggers ; 3)  internal gazetteer feature ; 4)  external macro context feature . In this way, the  NER problem  can be resolved effectively. Evaluation of our  system  on  MUC-6 and MUC-7 English NE tasks  achieves  F-measures  of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other  machine-learning system . Moreover, the  performance  is even consistently better than those based on  handcrafted rules .\n", "output": "The abstract text contains these key words below: 'Hidden Markov Model (HMM)' whose type is Method, 'HMM-based chunk tagger' whose type is Method, 'named entity (NE) recognition (NER) system' whose type is Method, 'names' whose type is OtherScientificTerm, 'times and numerical quantities' whose type is OtherScientificTerm, 'HMM' whose type is Method, 'deterministic internal feature of the  words' whose type is OtherScientificTerm, 'capitalization' whose type is OtherScientificTerm, 'digitalization' whose type is OtherScientificTerm, 'internal gazetteer feature' whose type is OtherScientificTerm, 'external macro context feature' whose type is OtherScientificTerm, 'NER problem' whose type is Task, 'system' whose type is Generic, 'MUC-6 and MUC-7 English NE tasks' whose type is Material, 'F-measures' whose type is Metric, 'machine-learning system' whose type is Method, 'handcrafted rules' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "onstationary chaotic behavior is not an oxymoron. We present two methods for capturing nonstationary chaos, then present a few examples including biological signals, ocean waves and traffic flow. The issue is of practical interest because it is often useful to capture when nonstationary events take place and it is desirable to know over what periods a signal is stationary.", "output": "The abstract text contains these key words below: 'Nonstationary chaotic behavior' whose type is OtherScientificTerm, 'oxymoron' whose type is OtherScientificTerm, 'methods' whose type is Generic, 'nonstationary chaos' whose type is OtherScientificTerm, 'examples' whose type is Generic, 'biological signals' whose type is Material, 'ocean waves' whose type is Material, 'traffic flow' whose type is Material, 'nonstationary events' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We propose a new  phrase-based translation model  and  decoding algorithm  that enables us to evaluate and compare several, previously proposed  phrase-based translation models  . Within our framework, we carry out a large number of experiments to understand better and explain why  phrase-based models  outperform  word-based models  . Our empirical results, which hold for all examined  language pairs  , suggest that the highest levels of performance can be obtained through relatively simple means:  heuristic learning  of  phrase translations  from  word-based alignments  and  lexical weighting  of  phrase translations  . Surprisingly, learning  phrases  longer than three  words  and learning  phrases  from  high-accuracy word-level alignment models  does not have a strong impact on performance. Learning only  syntactically motivated phrases  degrades the performance of our systems. ", "output": "The abstract text contains these key words below: 'phrase-based translation model' whose type is Method, 'decoding algorithm' whose type is Method, 'phrase-based translation models' whose type is Method, 'framework' whose type is Generic, 'phrase-based models' whose type is Method, 'word-based models' whose type is Method, 'means' whose type is Generic, 'heuristic learning  of  phrase translations' whose type is Method, 'word-based alignments' whose type is Method, 'lexical weighting  of  phrase translations' whose type is Method, 'high-accuracy word-level alignment models' whose type is Method, 'systems' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n It is well-known that there are  polysemous words  like  sentence  whose  meaning  or  sense  depends on the context of use. We have recently reported on two new  word-sense disambiguation systems  , one trained on  bilingual material  (the  Canadian Hansards  ) and the other trained on  monolingual material  (  Roget's Thesaurus  and  Grolier's Encyclopedia  ). As this work was nearing completion, we observed a very strong  discourse  effect. That is, if a  polysemous word  such as  sentence  appears two or more times in a  well-written discourse  , it is extremely likely that they will all share the same  sense  . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share  sense  in the same  discourse  is extremely strong (98%). This result can be used as an additional source of  constraint  for improving the performance of the  word-sense disambiguation algorithm  . In addition, it could also be used to help evaluate  disambiguation algorithms  that did not make use of the  discourse constraint  . ", "output": "The abstract text contains these key words below: 'polysemous words' whose type is OtherScientificTerm, 'word-sense disambiguation systems' whose type is Method, 'one' whose type is Generic, 'bilingual material' whose type is Material, 'Canadian Hansards' whose type is Material, 'other' whose type is Generic, 'monolingual material' whose type is Material, 'Roget's Thesaurus' whose type is Material, 'Grolier's Encyclopedia' whose type is Material, 'discourse' whose type is OtherScientificTerm, 'polysemous word' whose type is OtherScientificTerm, 'well-written discourse' whose type is OtherScientificTerm, 'discourse' whose type is OtherScientificTerm, 'constraint' whose type is OtherScientificTerm, 'word-sense disambiguation algorithm' whose type is Method, 'it' whose type is Generic, 'disambiguation algorithms' whose type is Method, 'discourse constraint' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe a set of  supervised machine learning  experiments centering on the construction of  statistical models  of  WH-questions  . These  models  , which are built from  shallow linguistic features  of  questions  , are employed to predict target variables which represent a  user's informational goals  . We report on different aspects of the  predictive performance  of our  models  , including the influence of various  training and testing factors  on  predictive performance  , and examine the relationships among the target variables. ", "output": "The abstract text contains these key words below: 'supervised machine learning' whose type is Method, 'statistical models  of  WH-questions' whose type is Method, 'models' whose type is Generic, 'shallow linguistic features  of  questions' whose type is OtherScientificTerm, 'user's informational goals' whose type is OtherScientificTerm, 'models' whose type is Generic, 'training and testing factors' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present a new model-based bundle adjustment algorithm to recover the 3D model of a scene/object from a sequence of images with unknown motions. Instead of representing scene/object by a collection of isolated 3D features (usually points), our algorithm uses a surface controlled by a small set of parameters. Compared with previous model-based approaches, our approach has the following advantages. First, instead of using the model space as a regular-izer, we directly use it as our search space, thus resulting in a more elegant formulation with fewer unknowns and fewer equations. Second, our algorithm automatically associates tracked points with their correct locations on the surfaces, thereby eliminating the need for a prior 2D-to-3D association. Third, regarding face modeling, we use a very small set of face metrics (meaningful deformations) to parame-terize the face geometry, resulting in a smaller search space and a better posed system. Experiments with both synthetic and real data show that this new algorithm is faster, more accurate and more stable than existing ones.", "output": "The abstract text contains these key words below: 'model-based bundle adjustment algorithm' whose type is Method, '3D model' whose type is Method, 'images' whose type is Material, 'unknown motions' whose type is OtherScientificTerm, 'isolated 3D features' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'surface' whose type is OtherScientificTerm, 'model-based approaches' whose type is Generic, 'approach' whose type is Generic, 'model space' whose type is OtherScientificTerm, 'regular-izer' whose type is OtherScientificTerm, 'it' whose type is Generic, 'search space' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'surfaces' whose type is OtherScientificTerm, 'prior 2D-to-3D association' whose type is OtherScientificTerm, 'face modeling' whose type is Task, 'face metrics' whose type is Metric, 'face geometry' whose type is OtherScientificTerm, 'search space' whose type is OtherScientificTerm, 'posed system' whose type is Method, 'synthetic and real data' whose type is Material, 'algorithm' whose type is Generic, 'ones' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we discuss language model adaptation methods given a word list and a raw corpus. In this situation, the general method is to segment the raw corpus automatically using a word list, correct the output sentences by hand, and build a model from the segmented corpus. In this sentence-by-sentence error correction method, however, the annotator encounters grammatically complicated positions and this results in a decrease of productivity. In this paper, we propose to concentrate on correcting the positions in which the words in the list appear by taking a word as a correction unit. This method allows us to avoid these problems and go directly to capturing the statistical behavior of specific words in the application. In the experiments, we used a variety of methods for preparing a segmented corpus and compared the language models by their speech recognition accuracies. The results showed the advantages of our method.", "output": "The abstract text contains these key words below: 'language model adaptation methods' whose type is Method, 'word list' whose type is OtherScientificTerm, 'raw corpus' whose type is Material, 'method' whose type is Generic, 'raw corpus' whose type is Material, 'word list' whose type is OtherScientificTerm, 'model' whose type is Generic, 'segmented corpus' whose type is Material, 'sentence-by-sentence error correction method' whose type is Method, 'method' whose type is Generic, 'methods' whose type is Generic, 'preparing a segmented corpus' whose type is Task, 'language models' whose type is Method, 'speech recognition accuracies' whose type is Metric, 'method' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We provide a  logical definition  of  Minimalist grammars  , that are  Stabler's formalization  of  Chomsky's minimalist program  . Our  logical definition  leads to a neat relation to  categorial grammar  , (yielding a treatment of  Montague semantics  ), a  parsing-as-deduction  in a  resource sensitive logic  , and a  learning algorithm  from  structured data  (based on a  typing-algorithm  and  type-unification  ). Here we emphasize the connection to  Montague semantics  which can be viewed as a  formal computation  of the  logical form  . ", "output": "The abstract text contains these key words below: 'logical definition  of  Minimalist grammars' whose type is Method, 'Stabler's formalization  of  Chomsky's minimalist program' whose type is Method, 'logical definition' whose type is Method, 'categorial grammar' whose type is Method, 'Montague semantics' whose type is OtherScientificTerm, 'parsing-as-deduction' whose type is Method, 'resource sensitive logic' whose type is OtherScientificTerm, 'learning algorithm' whose type is Method, 'structured data' whose type is Material, 'typing-algorithm' whose type is Method, 'type-unification' whose type is Method, 'Montague semantics' whose type is OtherScientificTerm, 'formal computation  of the  logical form' whose type is OtherScientificTerm, 'logical form' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he perception of transparent objects from images is known to be a very hard problem in vision. Given a single image, it is difficult to even detect the presence of transparent objects in the scene. In this paper, we explore what can be said about transparent objects by a moving observer. We show how features that are imaged through a transparent object behave differently from those that are rigidly attached to the scene. We present a novel model-based approach to recover the shapes and the poses of transparent objects from known motion. The objects can be complex in that they may be composed of multiple layers with different refractive indices. We have conducted numerous simulations to verify the practical feasibility of our algorithm. We have applied it to real scenes that include transparent objects and recovered the shapes of the objects with high accuracy.\n", "output": "The abstract text contains these key words below: 'perception of transparent objects' whose type is Task, 'images' whose type is Material, 'transparent objects' whose type is OtherScientificTerm, 'transparent objects' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'transparent object' whose type is OtherScientificTerm, 'those' whose type is Generic, 'model-based approach' whose type is Method, 'shapes and the poses of transparent objects' whose type is OtherScientificTerm, 'known motion' whose type is OtherScientificTerm, 'objects' whose type is Generic, 'they' whose type is Generic, 'multiple layers' whose type is OtherScientificTerm, 'refractive indices' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'it' whose type is Generic, 'real scenes' whose type is Material, 'transparent objects' whose type is OtherScientificTerm, 'shapes of the objects' whose type is OtherScientificTerm, 'accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We consider the problem of  question-focused sentence retrieval  from complex  news articles  describing  multi-event stories published over time .  Annotators  generated a list of  questions  central to understanding each  story  in our  corpus . Because of the dynamic nature of the  stories , many  questions  are time-sensitive (e.g. \"How many victims have been found?\").  Judges  found  sentences  providing an  answer  to each  question . To address the  sentence retrieval problem , we apply a  stochastic, graph-based method  for comparing the relative importance of the  textual units , which was previously used successfully for  generic summarization . Currently, we present a topic-sensitive version of our  method  and hypothesize that it can outperform a competitive  baseline , which compares the  similarity  of each  sentence  to the input  question  via  IDF-weighted word overlap . In our experiments, the  method  achieves a  TRDR score  that is significantly higher than that of the  baseline .\n", "output": "The abstract text contains these key words below: 'question-focused sentence retrieval' whose type is Task, 'news articles' whose type is Material, 'multi-event stories' whose type is Material, 'sentence retrieval problem' whose type is Task, 'stochastic, graph-based method' whose type is Method, 'generic summarization' whose type is Task, 'method' whose type is Generic, 'it' whose type is Generic, 'baseline' whose type is Generic, 'IDF-weighted word overlap' whose type is OtherScientificTerm, 'method' whose type is Generic, 'TRDR score' whose type is Metric, 'baseline' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n cross-domain learning, there is a more challenging problem that the domain divergence involves more than one dominant factors, e.g., different viewpoints , various resolutions and changing illuminations. Fortunately, an intermediate domain could often be found to build a bridge across them to facilitate the learning problem. In this paper, we propose a Coupled Marginalized Denoising Auto-encoders framework to address the cross-domain problem. Specifically, we design two marginalized denoising auto-encoders, one for the target and the other for source as well as the intermediate one. To better couple the two denoising auto-encoders learning, we incorporate a feature mapping, which tends to transfer knowledge between the intermediate domain and the target one. Furthermore, the maximum margin criterion, e.g., intra-class com-pactness and inter-class penalty, on the output layer is imposed to seek more discriminative features across different domains. Extensive experiments on two tasks have demonstrated the superiority of our method over the state-of-the-art methods.", "output": "The abstract text contains these key words below: 'cross-domain learning' whose type is Task, 'domain divergence' whose type is OtherScientificTerm, 'dominant factors' whose type is Generic, 'viewpoints' whose type is OtherScientificTerm, 'resolutions' whose type is OtherScientificTerm, 'illuminations' whose type is OtherScientificTerm, 'intermediate domain' whose type is OtherScientificTerm, 'learning problem' whose type is Task, 'Coupled Marginalized Denoising Auto-encoders framework' whose type is Method, 'cross-domain problem' whose type is Task, 'marginalized denoising auto-encoders' whose type is Method, 'one' whose type is Generic, 'other' whose type is Generic, 'denoising auto-encoders learning' whose type is Method, 'feature mapping' whose type is Method, 'intermediate domain' whose type is OtherScientificTerm, 'maximum margin criterion' whose type is OtherScientificTerm, 'intra-class com-pactness' whose type is OtherScientificTerm, 'inter-class penalty' whose type is OtherScientificTerm, 'discriminative features' whose type is OtherScientificTerm, 'tasks' whose type is Generic, 'method' whose type is Generic, 'state-of-the-art methods' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose a novel step toward the unsupervised seg-mentation of whole objects by combining \" hints \" of partial scene segmentation offered by multiple soft, binary mattes. These mattes are implied by a set of hypothesized object boundary fragments in the scene. Rather than trying to find or define a single \" best \" segmentation, we generate multiple segmentations of an image. This reflects contemporary methods for unsupervised object discovery from groups of images, and it allows us to define intuitive evaluation met-rics for our sets of segmentations based on the accurate and parsimonious delineation of scene objects. Our proposed approach builds on recent advances in spectral clustering, image matting, and boundary detection. It is demonstrated qualitatively and quantitatively on a dataset of scenes and is suitable for current work in unsupervised object discovery without top-down knowledge.", "output": "The abstract text contains these key words below: 'unsupervised seg-mentation of whole objects' whose type is Task, 'partial scene segmentation' whose type is Task, 'soft, binary mattes' whose type is OtherScientificTerm, 'mattes' whose type is Generic, 'hypothesized object boundary fragments' whose type is OtherScientificTerm, 'contemporary methods' whose type is Generic, 'unsupervised object discovery' whose type is Task, 'delineation of scene objects' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'spectral clustering' whose type is Method, 'image matting' whose type is Method, 'boundary detection' whose type is Method, 'It' whose type is Generic, 'dataset of scenes' whose type is Material, 'unsupervised object discovery' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n In this paper we explore a new  theory of discourse structure  that stresses the role of  purpose  and  processing  in  discourse  . In this theory,  discourse structure  is composed of three separate but interrelated components: the structure of the sequence of  utterances  (called the  linguistic structure  ), a structure of  purposes  (called the  intentional structure  ), and the state of  focus of attention  (called the  attentional state  ). The  linguistic structure  consists of segments of the  discourse  into which the  utterances  naturally aggregate. The  intentional structure  captures the  discourse-relevant purposes  , expressed in each of the  linguistic segments  as well as relationships among them. The  attentional state  is an abstraction of the  focus of attention  of the  participants  as the  discourse  unfolds. The  attentional state  , being dynamic, records the objects, properties, and relations that are salient at each point of the  discourse  . The distinction among these components is essential to provide an adequate explanation of such  discourse phenomena  as  cue phrases  ,  referring expressions  , and  interruptions  . The  theory of attention, intention, and aggregation of utterances  is illustrated in the paper with a number of example  discourses  . Various properties of  discourse  are described, and explanations for the behaviour of  cue phrases  ,  referring expressions  , and  interruptions  are explored. This  theory  provides a framework for describing the processing of  utterances  in a  discourse  .  Discourse processing  requires recognizing how the  utterances  of the  discourse  aggregate into  segments  , recognizing the  intentions  expressed in the  discourse  and the relationships among  intentions  , and tracking the  discourse  through the operation of the mechanisms associated with  attentional state  . This processing description specifies in these  recognition tasks  the role of information from the  discourse  and from the  participants  ' knowledge of the domain. ", "output": "The abstract text contains these key words below: 'theory of discourse structure' whose type is Method, 'discourse' whose type is OtherScientificTerm, 'theory' whose type is Generic, 'discourse structure' whose type is OtherScientificTerm, 'components' whose type is Generic, 'linguistic structure' whose type is OtherScientificTerm, 'intentional structure' whose type is OtherScientificTerm, 'attentional state' whose type is OtherScientificTerm, 'linguistic structure' whose type is OtherScientificTerm, 'discourse' whose type is OtherScientificTerm, 'intentional structure' whose type is OtherScientificTerm, 'discourse-relevant purposes' whose type is OtherScientificTerm, 'attentional state' whose type is OtherScientificTerm, 'discourse' whose type is OtherScientificTerm, 'attentional state' whose type is OtherScientificTerm, 'discourse' whose type is OtherScientificTerm, 'discourse phenomena' whose type is OtherScientificTerm, 'cue phrases' whose type is OtherScientificTerm, 'referring expressions' whose type is OtherScientificTerm, 'interruptions' whose type is OtherScientificTerm, 'theory of attention, intention, and aggregation of utterances' whose type is Method, 'discourses' whose type is OtherScientificTerm, 'discourse' whose type is OtherScientificTerm, 'cue phrases' whose type is OtherScientificTerm, 'referring expressions' whose type is OtherScientificTerm, 'interruptions' whose type is OtherScientificTerm, 'theory' whose type is Generic, 'discourse' whose type is OtherScientificTerm, 'Discourse processing' whose type is Method, 'discourse' whose type is OtherScientificTerm, 'discourse' whose type is OtherScientificTerm, 'discourse' whose type is OtherScientificTerm, 'attentional state' whose type is OtherScientificTerm, 'processing' whose type is Generic, 'recognition tasks' whose type is Task, 'discourse' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n In order to meet the needs of a publication of papers in English, many systems to run off texts have been developed. In this paper, we report a system  FROFF  which can make a fair copy of not only texts but also graphs and tables indispensable to our papers. Its selection of  fonts  , specification of  character  size are dynamically changeable, and the  typing location  can be also changed in lateral or longitudinal directions. Each  character  has its own width and a line length is counted by the sum of each  character  . By using commands or  rules  which are defined to facilitate the construction of format expected or some  mathematical expressions  , elaborate and pretty documents can be successfully obtained. ", "output": "The abstract text contains these key words below: 'system' whose type is Generic, 'FROFF' whose type is Method, 'typing location' whose type is OtherScientificTerm, 'commands' whose type is OtherScientificTerm, 'rules' whose type is OtherScientificTerm, 'mathematical expressions' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n It is often assumed that when  natural language processing  meets the real world, the ideal of aiming for complete and correct interpretations has to be abandoned. However, our experience with  TACITUS  ; especially in the  MUC-3 evaluation  , has shown that principled techniques for  syntactic and pragmatic analysis  can be bolstered with methods for achieving robustness. We describe three techniques for making  syntactic analysis  more robust---an  agenda-based scheduling parser  , a  recovery technique for failed parses  , and a new technique called  terminal substring parsing  . For  pragmatics processing  , we describe how the method of  abductive inference  is inherently robust, in that an interpretation is always possible, so that in the absence of the required  world knowledge  , performance degrades gracefully. Each of these techniques have been evaluated and the results of the evaluations are presented. ", "output": "The abstract text contains these key words below: 'natural language processing' whose type is Task, 'TACITUS' whose type is Method, 'MUC-3 evaluation' whose type is Metric, 'techniques' whose type is Generic, 'syntactic and pragmatic analysis' whose type is Task, 'methods' whose type is Generic, 'robustness' whose type is Metric, 'three techniques' whose type is Generic, 'syntactic analysis' whose type is Task, 'agenda-based scheduling parser' whose type is Method, 'recovery technique' whose type is Method, 'failed parses' whose type is OtherScientificTerm, 'technique' whose type is Generic, 'terminal substring parsing' whose type is Method, 'pragmatics processing' whose type is Task, 'abductive inference' whose type is Method, 'world knowledge' whose type is OtherScientificTerm, 'these techniques' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "urrently, N-gram models are the most common and widely used models for statistical language modeling. In this paper, we investigated an alternative way to build language models, i.e., using artificial neural networks to learn the language model. Our experiment result shows that the neural network can learn a language model that has performance even better than standard statistical methods.", "output": "The abstract text contains these key words below: 'N-gram models' whose type is Method, 'statistical language modeling' whose type is Task, 'language models' whose type is Method, 'artificial neural networks' whose type is Method, 'language model' whose type is Method, 'neural network' whose type is Method, 'language model' whose type is Method, 'statistical methods' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e describe the use of text data scraped from the web to augment language models for Automatic Speech Recognition and Keyword Search for Low Resource Languages. We scrape text from multiple genres including blogs, online news, translated TED talks, and subtitles. Using linearly interpolated language models, we find that blogs and movie subtitles are more relevant for language modeling of conversational telephone speech and obtain large reductions in out-of-vocabulary keywords. Furthermore, we show that the web data can improve Term Error Rate Performance by 3.8% absolute and Maximum Term-Weighted Value in Keyword Search by 0.0076-0.1059 absolute points. Much of the gain comes from the reduction of out-of-vocabulary items.", "output": "The abstract text contains these key words below: 'text data' whose type is Material, 'web' whose type is Material, 'language models' whose type is Method, 'Automatic Speech Recognition' whose type is Task, 'Keyword Search' whose type is Task, 'Low Resource Languages' whose type is Material, 'genres' whose type is Generic, 'blogs' whose type is Material, 'online news' whose type is Material, 'translated TED talks' whose type is Material, 'subtitles' whose type is Material, 'linearly interpolated language models' whose type is Method, 'blogs' whose type is Material, 'movie subtitles' whose type is Material, 'language modeling of conversational telephone speech' whose type is Method, 'out-of-vocabulary keywords' whose type is OtherScientificTerm, 'web data' whose type is Material, 'Term Error Rate Performance' whose type is Metric, 'Maximum Term-Weighted Value' whose type is Metric, 'Keyword Search' whose type is Task, 'reduction of out-of-vocabulary items' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper describes a  computational model  of  word segmentation  and presents simulation results on  realistic acquisition . In particular, we explore the capacity and limitations of  statistical learning mechanisms  that have recently gained prominence in  cognitive psychology  and  linguistics .\n", "output": "The abstract text contains these key words below: 'computational model' whose type is Method, 'word segmentation' whose type is Task, 'realistic acquisition' whose type is Task, 'statistical learning mechanisms' whose type is Method, 'cognitive psychology' whose type is Material, 'linguistics' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper describes an implemented program that takes a  tagged text corpus  and generates a partial list of the  subcategorization frames  in which each  verb  occurs. The completeness of the output list increases monotonically with the total  occurrences  of each  verb  in the  training corpus .  False positive rates  are one to three percent. Five  subcategorization frames  are currently detected and we foresee no impediment to detecting many more. Ultimately, we expect to provide a large  subcategorization dictionary  to the  NLP community  and to train  dictionaries  for specific  corpora .\n", "output": "The abstract text contains these key words below: 'program' whose type is Generic, 'tagged text corpus' whose type is Material, 'subcategorization frames' whose type is OtherScientificTerm, 'False positive rates' whose type is Metric, 'subcategorization frames' whose type is OtherScientificTerm, 'subcategorization dictionary' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we describe a search procedure for  statistical machine translation (MT)  based on  dynamic programming (DP) . Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible  word reordering  between  source and target language  in order to achieve an efficient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the  Verbmobil task  (German-English, 8000-word vocabulary), which is a  limited-domain spoken-language task .\n", "output": "The abstract text contains these key words below: 'search procedure' whose type is Generic, 'statistical machine translation (MT)' whose type is Task, 'dynamic programming (DP)' whose type is Method, 'DP-based solution' whose type is Method, 'traveling salesman problem' whose type is Method, 'technique' whose type is Generic, 'word reordering' whose type is OtherScientificTerm, 'search algorithm' whose type is Method, 'Verbmobil task' whose type is Material, 'German-English' whose type is Material, 'limited-domain spoken-language task' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose a unified variational formulation for joint motion estimation and segmentation with explicit occlusion handling. This is done by a multi-label representation of the flow field, where each label corresponds to a parametric representation of the motion. We use a convex formulation of the multi-label Potts model with label costs and show that the asymmetric map-uniqueness criterion can be integrated into our formulation by means of convex constraints. Explicit occlusion handling eliminates errors otherwise created by the regularization. As occlusions can occur only at object boundaries, a large number of objects may be required. By using a fast primal-dual algorithm we are able to handle several hundred motion segments. Results are shown on several classical motion segmentation and optical flow examples.", "output": "The abstract text contains these key words below: 'unified variational formulation' whose type is Method, 'joint motion estimation and segmentation' whose type is Task, 'explicit occlusion handling' whose type is Method, 'multi-label representation of the flow field' whose type is Method, 'parametric representation of the motion' whose type is Task, 'convex formulation' whose type is Method, 'multi-label Potts model' whose type is Method, 'asymmetric map-uniqueness criterion' whose type is Metric, 'formulation' whose type is Generic, 'convex constraints' whose type is OtherScientificTerm, 'Explicit occlusion handling' whose type is Method, 'regularization' whose type is OtherScientificTerm, 'occlusions' whose type is OtherScientificTerm, 'object boundaries' whose type is OtherScientificTerm, 'primal-dual algorithm' whose type is Method, 'motion segments' whose type is OtherScientificTerm, 'classical motion segmentation' whose type is Task, 'optical flow' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The  translation  of  English text  into  American Sign Language (ASL) animation  tests the limits of  traditional MT architectural designs . A new  semantic representation  is proposed that uses  virtual reality 3D scene modeling software  to produce  spatially complex ASL phenomena  called \" classifier predicates .\" The model acts as an  interlingua  within a new  multi-pathway MT architecture design  that also incorporates  transfer  and  direct approaches  into a single system.\n", "output": "The abstract text contains these key words below: 'translation  of  English text  into  American Sign Language (ASL) animation' whose type is Task, 'MT architectural designs' whose type is Method, 'semantic representation' whose type is Method, 'virtual reality 3D scene modeling software' whose type is Method, 'spatially complex ASL phenomena' whose type is OtherScientificTerm, 'classifier predicates' whose type is OtherScientificTerm, 'model' whose type is Generic, 'interlingua' whose type is OtherScientificTerm, 'multi-pathway MT architecture design' whose type is Method, 'transfer' whose type is OtherScientificTerm, 'direct approaches' whose type is Generic, 'system' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper proposes to use a  convolution kernel  over  parse trees  to model  syntactic structure information  for  relation extraction . Our study reveals that the  syntactic structure features  embedded in a  parse tree  are very effective for  relation extraction  and these features can be well captured by the  convolution tree kernel . Evaluation on the  ACE 2003 corpus  shows that the  convolution kernel  over  parse trees  can achieve comparable performance with the previous best-reported feature-based methods on the 24  ACE relation subtypes . It also shows that our method significantly outperforms the previous two  dependency tree kernels  on the 5  ACE relation major types .\n", "output": "The abstract text contains these key words below: 'convolution kernel  over  parse trees' whose type is Method, 'syntactic structure information' whose type is OtherScientificTerm, 'relation extraction' whose type is Task, 'syntactic structure features' whose type is OtherScientificTerm, 'parse tree' whose type is OtherScientificTerm, 'relation extraction' whose type is Task, 'features' whose type is Generic, 'convolution tree kernel' whose type is Method, 'ACE 2003 corpus' whose type is Material, 'convolution kernel  over  parse trees' whose type is Method, 'feature-based methods' whose type is Method, 'method' whose type is Generic, 'dependency tree kernels' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ecent advances in linear classification have shown that for applications such as document classification, the training can be extremely efficient. However, most of the existing training methods are designed by assuming that data can be stored in the computer memory. These methods cannot be easily applied to data larger than the memory capacity due to the random access to the disk. We propose and analyze a block minimization framework for data larger than the memory size. At each step a block of data is loaded from the disk and handled by certain learning methods. We investigate two implementations of the proposed framework for primal and dual SVMs, respectively. As data cannot fit in memory, many design considerations are very different from those for traditional algorithms. Experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method.", "output": "The abstract text contains these key words below: 'linear classification' whose type is Task, 'applications' whose type is Generic, 'document classification' whose type is Task, 'training methods' whose type is Method, 'computer memory' whose type is OtherScientificTerm, 'methods' whose type is Generic, 'data' whose type is Generic, 'memory capacity' whose type is OtherScientificTerm, 'random access' whose type is OtherScientificTerm, 'disk' whose type is OtherScientificTerm, 'block minimization framework' whose type is Method, 'data' whose type is Generic, 'memory size' whose type is OtherScientificTerm, 'disk' whose type is OtherScientificTerm, 'learning methods' whose type is Method, 'framework' whose type is Generic, 'primal and dual SVMs' whose type is Method, 'method' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents a  formal analysis  for a large class of  words  called  alternative markers  , which includes  other (than)  ,  such (as)  , and  besides  . These  words  appear frequently enough in  dialog  to warrant serious  attention  , yet present  natural language search engines  perform poorly on  queries  containing them. I show that the  performance  of a  search engine  can be improved dramatically by incorporating an approximation of the  formal analysis  that is compatible with the  search engine  's  operational semantics  . The value of this approach is that as the  operational semantics  of  natural language applications  improve, even larger improvements are possible. ", "output": "The abstract text contains these key words below: 'formal analysis' whose type is Method, 'alternative markers' whose type is OtherScientificTerm, 'words' whose type is Generic, 'dialog' whose type is OtherScientificTerm, 'natural language search engines' whose type is Method, 'them' whose type is Generic, 'search engine' whose type is Method, 'approximation of the  formal analysis' whose type is Method, 'formal analysis' whose type is Method, 'search engine' whose type is Method, 'operational semantics' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'operational semantics' whose type is OtherScientificTerm, 'natural language applications' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We consider the problem of computing the  Kullback-Leibler distance , also called the  relative entropy , between a  probabilistic context-free grammar  and a  probabilistic finite automaton . We show that there is a  closed-form (analytical) solution  for one part of the  Kullback-Leibler distance , viz the  cross-entropy . We discuss several applications of the result to the problem of  distributional approximation  of  probabilistic context-free grammars  by means of  probabilistic finite automata .\n", "output": "The abstract text contains these key words below: 'Kullback-Leibler distance' whose type is Method, 'relative entropy' whose type is Method, 'probabilistic context-free grammar' whose type is Method, 'probabilistic finite automaton' whose type is OtherScientificTerm, 'closed-form (analytical) solution' whose type is Method, 'Kullback-Leibler distance' whose type is Method, 'cross-entropy' whose type is Method, 'distributional approximation' whose type is Task, 'probabilistic context-free grammars' whose type is Method, 'probabilistic finite automata' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents a  critical discussion  of the various  approaches  that have been used in the  evaluation of Natural Language systems . We conclude that previous  approaches  have neglected to evaluate  systems  in the context of their use, e.g. solving a  task  requiring  data retrieval . This raises questions about the validity of such  approaches . In the second half of the paper, we report a  laboratory study  using the  Wizard of Oz technique  to identify  NL requirements  for carrying out this  task . We evaluate the demands that  task dialogues  collected using this  technique , place upon a  prototype Natural Language system . We identify three important requirements which arose from the  task  that we gave our subjects: operators specific to the task of  database access , complex  contextual reference  and reference to the  structure  of the  information source . We discuss how these might be satisfied by future  Natural Language systems .\n", "output": "The abstract text contains these key words below: 'approaches' whose type is Generic, 'evaluation of Natural Language systems' whose type is Task, 'Natural Language systems' whose type is Method, 'approaches' whose type is Generic, 'systems' whose type is Generic, 'task' whose type is Generic, 'data retrieval' whose type is Task, 'approaches' whose type is Generic, 'Wizard of Oz technique' whose type is Method, 'NL requirements' whose type is OtherScientificTerm, 'task' whose type is Generic, 'task dialogues' whose type is Material, 'technique' whose type is Generic, 'prototype Natural Language system' whose type is Method, 'task' whose type is Generic, 'task' whose type is Generic, 'database access' whose type is Task, 'contextual reference' whose type is OtherScientificTerm, 'Natural Language systems' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a novel approach for automatically acquiring  English topic signatures . Given a particular  concept , or  word sense , a  topic signature  is a set of  words  that tend to co-occur with it.  Topic signatures  can be useful in a number of  Natural Language Processing (NLP) applications , such as  Word Sense Disambiguation (WSD)  and  Text Summarisation . Our method takes advantage of the different way in which  word senses  are lexicalised in  English  and  Chinese , and also exploits the large amount of  Chinese text  available in  corpora  and on the Web. We evaluated the  topic signatures  on a  WSD task , where we trained a  second-order vector cooccurrence algorithm  on  standard WSD datasets , with promising results.\n", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'automatically acquiring  English topic signatures' whose type is Task, 'concept' whose type is OtherScientificTerm, 'word sense' whose type is OtherScientificTerm, 'topic signature' whose type is OtherScientificTerm, 'Topic signatures' whose type is OtherScientificTerm, 'Natural Language Processing (NLP) applications' whose type is Task, 'Word Sense Disambiguation (WSD)' whose type is Task, 'Text Summarisation' whose type is Task, 'method' whose type is Generic, 'word senses' whose type is OtherScientificTerm, 'English' whose type is Material, 'Chinese' whose type is Material, 'Chinese text' whose type is Material, 'corpora' whose type is Generic, 'Web' whose type is Material, 'topic signatures' whose type is OtherScientificTerm, 'WSD task' whose type is Task, 'second-order vector cooccurrence algorithm' whose type is Method, 'WSD datasets' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "utomated facial expression recognition has received increased attention over the past two decades. Existing works in the field usually do not encode either the temporal evolution or the intensity of the observed facial displays. They also fail to jointly model multidimensional (multi-class) continuous facial behaviour data; binary classifiers-one for each target basic-emotion class-are used instead. In this paper, intrinsic topology of multidimensional continuous facial affect data is first modeled by an ordinal man-ifold. This topology is then incorporated into the Hidden Conditional Ordinal Random Field (H-CORF) framework for dynamic ordinal regression by constraining H-CORF parameters to lie on the ordinal manifold. The resulting model attains simultaneous dynamic recognition and intensity estimation of facial expressions of multiple emotions. To the best of our knowledge, the proposed method is the first one to achieve this on both deliberate as well as spontaneous facial affect data.", "output": "The abstract text contains these key words below: 'Automated facial expression recognition' whose type is Task, 'Existing works' whose type is Generic, 'temporal evolution' whose type is OtherScientificTerm, 'intensity of the observed facial displays' whose type is OtherScientificTerm, 'They' whose type is Generic, 'multidimensional (multi-class) continuous facial behaviour data' whose type is Material, 'binary classifiers' whose type is Method, 'intrinsic topology of multidimensional continuous facial' whose type is OtherScientificTerm, 'ordinal man-ifold' whose type is OtherScientificTerm, 'topology' whose type is Generic, 'Hidden Conditional Ordinal Random Field (H-CORF) framework' whose type is Method, 'dynamic ordinal regression' whose type is Task, 'H-CORF parameters' whose type is OtherScientificTerm, 'ordinal manifold' whose type is OtherScientificTerm, 'model' whose type is Generic, 'simultaneous dynamic recognition' whose type is Task, 'intensity estimation of facial expressions' whose type is Task, 'the proposed method' whose type is Method, 'spontaneous facial affect data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "tructured-light methods actively generate geometric correspondence data between projectors and cameras in order to facilitate robust 3D reconstruction. In this paper, we present Photogeometric Structured Light whereby a standard structured light method is extended to include photometric methods. Photometric processing serves the double purpose of increasing the amount of recovered surface detail and of enabling the structured-light setup to be robustly self-calibrated. Further, our framework uses a photogeometric optimization that supports the simultaneous use of multiple cameras and projectors and yields a single and accurate multi-view 3D model which best complies with photometric and geometric data.", "output": "The abstract text contains these key words below: 'Structured-light methods' whose type is Method, 'geometric correspondence data' whose type is Material, 'robust 3D reconstruction' whose type is Task, 'Photogeometric Structured Light' whose type is Method, 'structured light method' whose type is Method, 'photometric methods' whose type is Method, 'Photometric processing' whose type is Method, 'recovered surface detail' whose type is OtherScientificTerm, 'structured-light setup' whose type is OtherScientificTerm, 'framework' whose type is Generic, 'photogeometric optimization' whose type is Method, 'multi-view 3D model' whose type is Method, 'photometric and geometric data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " new algorithm for solving the three dimensional container packing problem is proposed in this paper. This new algorithm deviates from the traditional approach of wall building and layering. It uses the concept of \" building growing \" from multiple sides of the container. We tested our method using all 760 test cases from the OR-Library. Experimental results indicate that the new algorithm is able to achieve an average packing utilization of more than 87%. This is better than the results reported in the literature.", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'dimensional container packing problem' whose type is Task, 'algorithm' whose type is Generic, 'approach of wall building and layering' whose type is Method, 'It' whose type is Generic, 'method' whose type is Generic, 'OR-Library' whose type is Material, 'algorithm' whose type is Generic, 'average packing utilization' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present a method for detecting 3D objects using multi-modalities. While it is generic, we demonstrate it on the combination of an image and a dense depth map which give complementary object information. It works in real-time, under heavy clutter, does not require a time consuming training stage, and can handle untextured objects. It is based on an efficient representation of templates that capture the different modalities, and we show in many experiments on commodity hardware that our approach significantly outperforms state-of-the-art methods on single modalities.", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'detecting 3D objects' whose type is Task, 'multi-modalities' whose type is OtherScientificTerm, 'it' whose type is Generic, 'it' whose type is Generic, 'image' whose type is OtherScientificTerm, 'dense depth map' whose type is OtherScientificTerm, 'complementary object information' whose type is OtherScientificTerm, 'It' whose type is Generic, 'time consuming training stage' whose type is OtherScientificTerm, 'untextured objects' whose type is OtherScientificTerm, 'It' whose type is Generic, 'templates' whose type is OtherScientificTerm, 'modalities' whose type is OtherScientificTerm, 'commodity hardware' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'state-of-the-art methods' whose type is Generic, 'single modalities' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The major objective of this program is to develop and demonstrate robust, high performance  continuous speech recognition (CSR) techniques  focussed on application in  Spoken Language Systems (SLS)  which will enhance the effectiveness of  military and civilian computer-based systems . A key complementary objective is to define and develop applications of robust  speech recognition and understanding systems , and to help catalyze the transition of  spoken language technology  into  military and civilian systems , with particular focus on application of robust  CSR  to  mobile military command and control . The research effort focusses on developing advanced  acoustic modelling , rapid search, and  recognition-time adaptation techniques  for robust  large-vocabulary CSR , and on applying these techniques to the new  ARPA large-vocabulary CSR corpora  and to military application tasks.\n", "output": "The abstract text contains these key words below: 'continuous speech recognition (CSR) techniques' whose type is Method, 'Spoken Language Systems (SLS)' whose type is Task, 'military and civilian computer-based systems' whose type is Task, 'speech recognition and understanding systems' whose type is Task, 'spoken language technology' whose type is Method, 'military and civilian systems' whose type is Task, 'CSR' whose type is Method, 'mobile military command and control' whose type is Task, 'acoustic modelling' whose type is Method, 'rapid search' whose type is Method, 'recognition-time adaptation techniques' whose type is Method, 'large-vocabulary CSR' whose type is Method, 'techniques' whose type is Generic, 'ARPA large-vocabulary CSR corpora' whose type is Material, 'military application tasks' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e have calculated analytical expressions for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with offline updates over trials in absorbing Markov chains using lookup table representations. We illustrate classes of learning curve behavior in various chains, and show the manner in which TD is sensitive to the choice of its step-size and eligibility trace parameters.", "output": "The abstract text contains these key words below: 'analytical expressions' whose type is OtherScientificTerm, 'temporal difference value estimation algorithms' whose type is Method, 'Markov chains' whose type is OtherScientificTerm, 'lookup table representations' whose type is Method, 'learning curve behavior' whose type is OtherScientificTerm, 'step-size and eligibility trace parameters' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "mage sequence processing techniques are used to study exchange , growth, and transport processes and to tackle key questions in environmental physics and biology. These applications require high accuracy for the estimation of the motion field since the most interesting parameters of the dynamical processes studied are contained in first-order derivatives of the motion field or in dynamical changes of the moving objects. Therefore the performance and optimization of low-level motion estimators is discussed. A tensor method tuned with carefully optimized derivative filters yields reliable and dense displacement vector fields (DVF) with an accuracy of up to a few hundredth pixels/frame for real-world images. The accuracy of the tensor method is verified with computer-generated sequences and a calibrated image sequence. With the improvements in accuracy the motion estimation is now rather limited by imperfections in the CCD sensors, especially the spatial nonuni-formity in the responsivity. With a simple two-point calibration, these effects can efficiently be suppressed. The application of the techniques to the analysis of plant growth, to ocean surface microturbulence in IR image sequences, and to sediment transport is demonstrated.", "output": "The abstract text contains these key words below: 'Image sequence processing techniques' whose type is Method, 'exchange , growth, and transport processes' whose type is Task, 'environmental physics' whose type is Material, 'biology' whose type is Material, 'applications' whose type is Generic, 'accuracy' whose type is Metric, 'estimation of the motion field' whose type is Task, 'dynamical processes' whose type is Method, 'first-order derivatives of the motion field' whose type is OtherScientificTerm, 'dynamical changes of the moving objects' whose type is OtherScientificTerm, 'optimization of low-level motion estimators' whose type is Task, 'tensor method' whose type is Method, 'derivative filters' whose type is Method, 'displacement vector fields (DVF)' whose type is OtherScientificTerm, 'accuracy' whose type is Metric, 'pixels/frame' whose type is Metric, 'real-world images' whose type is Material, 'accuracy' whose type is Metric, 'tensor method' whose type is Method, 'computer-generated sequences' whose type is Material, 'calibrated image sequence' whose type is Material, 'accuracy' whose type is Metric, 'motion estimation' whose type is Task, 'CCD sensors' whose type is OtherScientificTerm, 'spatial nonuni-formity' whose type is OtherScientificTerm, 'responsivity' whose type is OtherScientificTerm, 'two-point calibration' whose type is Method, 'techniques' whose type is Generic, 'analysis of plant growth' whose type is Task, 'ocean surface microturbulence in IR image sequences' whose type is Task, 'sediment transport' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Existing techniques extract  term candidates  by looking for  internal and contextual information  associated with  domain specific terms . The algorithms always face the dilemma that fewer  features  are not enough to distinguish  terms  from  non-terms  whereas more  features  lead to more conflicts among selected  features . This paper presents a novel approach for  term extraction  based on  delimiters  which are much more stable and domain independent. The proposed approach is not as sensitive to  term frequency  as that of previous works. This approach has no strict limit or  hard rules  and thus they can deal with all kinds of  terms . It also requires no prior  domain knowledge  and no additional  training  to adapt to new  domains . Consequently, the proposed approach can be applied to different  domains  easily and it is especially useful for  resource-limited domains . Evaluations conducted on two different  domains  for  Chinese term extraction  show significant improvements over existing techniques which verifies its efficiency and domain independent nature. Experiments on  new term extraction  indicate that the proposed approach can also serve as an effective tool for  domain lexicon expansion .\n", "output": "The abstract text contains these key words below: 'internal and contextual information' whose type is OtherScientificTerm, 'domain specific terms' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'term extraction' whose type is Task, 'delimiters' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'term frequency' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'hard rules' whose type is OtherScientificTerm, 'domain knowledge' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'it' whose type is Generic, 'resource-limited domains' whose type is Material, 'Evaluations' whose type is Generic, 'Chinese term extraction' whose type is Task, 'new term extraction' whose type is Task, 'approach' whose type is Generic, 'tool' whose type is Generic, 'domain lexicon expansion' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe the ongoing construction of a large,  semantically annotated corpus  resource as reliable basis for the large-scale  acquisition of word-semantic information  , e.g. the construction of  domain-independent lexica  . The backbone of the  annotation  are  semantic roles  in the  frame semantics paradigm  . We report experiences and evaluate the  annotated data  from the first project stage. On this basis, we discuss the problems of  vagueness  and  ambiguity  in  semantic annotation  . ", "output": "The abstract text contains these key words below: 'semantically annotated corpus' whose type is Material, 'large-scale  acquisition of word-semantic information' whose type is Task, 'construction of  domain-independent lexica' whose type is Task, 'annotation' whose type is OtherScientificTerm, 'semantic roles' whose type is OtherScientificTerm, 'frame semantics paradigm' whose type is Method, 'annotated data' whose type is Material, 'vagueness' whose type is OtherScientificTerm, 'ambiguity' whose type is OtherScientificTerm, 'semantic annotation' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "One of the claimed benefits of  Tree Adjoining Grammars  is that they have an  extended domain of locality (EDOL) . We consider how this can be exploited to limit the need for  feature structure unification  during  parsing . We compare two wide-coverage  lexicalized grammars of English ,  LEXSYS  and  XTAG , finding that the two  grammars  exploit  EDOL  in different ways. ", "output": "The abstract text contains these key words below: 'Tree Adjoining Grammars' whose type is Method, 'they' whose type is Generic, 'extended domain of locality (EDOL)' whose type is OtherScientificTerm, 'feature structure unification' whose type is Method, 'parsing' whose type is Task, 'lexicalized grammars of English' whose type is Method, 'LEXSYS' whose type is Method, 'XTAG' whose type is Method, 'grammars' whose type is Method, 'EDOL' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents a novel representation for three-dimensional objects in terms of affine-invariant image patches and their spatial relationships. Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction, allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint. The proposed approach does not require a separate segmentation stage and is applicable to cluttered scenes. Preliminary modeling and recognition results are presented.", "output": "The abstract text contains these key words below: 'representation' whose type is Generic, 'three-dimensional objects' whose type is OtherScientificTerm, 'affine-invariant image patches' whose type is OtherScientificTerm, 'spatial relationships' whose type is OtherScientificTerm, 'Multi-view constraints' whose type is OtherScientificTerm, 'normalized representation' whose type is Method, 'matching' whose type is Task, 'reconstruction' whose type is Task, 'acquisition of true three-dimensional affine and Euclidean models' whose type is Task, 'images' whose type is Material, 'approach' whose type is Generic, 'segmentation stage' whose type is Method, 'cluttered scenes' whose type is OtherScientificTerm, 'recognition' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The purpose of this research is to test the efficacy of applying  automated evaluation techniques  , originally devised for the evaluation of  human language learners  , to the  output  of  machine translation (MT) systems  . We believe that these  evaluation techniques  will provide information about both the  human language learning process  , the  translation process  and the  development  of  machine translation systems  . This, the first experiment in a series of experiments, looks at the  intelligibility  of  MT output  . A  language learning experiment  showed that  assessors  can differentiate  native from non-native language essays  in less than 100  words  . Even more illuminating was the factors on which the  assessors  made their decisions. We tested this to see if similar criteria could be elicited from duplicating the experiment using  machine translation output  . Subjects were given a set of up to six extracts of  translated newswire text  . Some of the extracts were  expert human translations  , others were  machine translation outputs  . The subjects were given three minutes per extract to determine whether they believed the sample output to be an  expert human translation  or a  machine translation  . Additionally, they were asked to mark the  word  at which they made this decision. The results of this experiment, along with a preliminary analysis of the factors involved in the decision making process will be presented here. ", "output": "The abstract text contains these key words below: 'automated evaluation techniques' whose type is Method, 'evaluation of  human language learners' whose type is Task, 'machine translation (MT) systems' whose type is Method, 'evaluation techniques' whose type is Generic, 'human language learning process' whose type is Task, 'translation process' whose type is Task, 'machine translation systems' whose type is Task, 'language learning' whose type is Task, 'assessors' whose type is Generic, 'non-native language essays' whose type is Material, 'assessors' whose type is Generic, 'machine translation output' whose type is OtherScientificTerm, 'translated newswire text' whose type is Material, 'expert human translations' whose type is OtherScientificTerm, 'machine translation outputs' whose type is OtherScientificTerm, 'expert human translation' whose type is OtherScientificTerm, 'machine translation' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper we discuss object detection when only a small number of training examples are given. Specifically, we show how to incorporate a simple prior on the distribution of natural images into support vector machines. SVMs are known to be robust to overfitting; however, a few training examples usually do not represent well the structure of the class. Thus the resulting detectors are not robust and highly depend on the choice of the training examples. We incorporate the prior on natural images by requiring that the separating hyperplane will not only yield a wide margin , but also that the corresponding positive half space will have a low probability to contain natural images (the background). Our experiments on real data sets show that the resulting detector is more robust to the choice of training examples, and substantially improves both linear and kernel SVM when trained on 10 positive and 10 negative examples.", "output": "The abstract text contains these key words below: 'object detection' whose type is Task, 'prior on the distribution of natural images' whose type is OtherScientificTerm, 'support vector machines' whose type is Method, 'SVMs' whose type is Method, 'overfitting' whose type is OtherScientificTerm, 'detectors' whose type is Method, 'prior on natural images' whose type is OtherScientificTerm, 'hyperplane' whose type is OtherScientificTerm, 'real data sets' whose type is Material, 'detector' whose type is Method, 'linear and kernel SVM' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper describes a method for  utterance classification  that does not require  manual transcription  of  training data  . The method combines  domain independent acoustic models  with off-the-shelf  classifiers  to give  utterance classification performance  that is surprisingly close to what can be achieved using conventional  word-trigram recognition  requiring  manual transcription  . In our method,  unsupervised training  is first used to train a  phone n-gram model  for a particular  domain  ; the  output  of  recognition  with this  model  is then passed to a  phone-string classifier  . The  classification accuracy  of the method is evaluated on three different  spoken language system domains  . ", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'utterance classification' whose type is Task, 'manual transcription' whose type is Material, 'method' whose type is Generic, 'domain independent acoustic models' whose type is Method, 'classifiers' whose type is Method, 'utterance classification' whose type is Task, 'word-trigram recognition' whose type is Method, 'manual transcription' whose type is Material, 'method' whose type is Generic, 'unsupervised training' whose type is Method, 'phone n-gram model' whose type is Method, 'domain' whose type is Generic, 'model' whose type is Generic, 'phone-string classifier' whose type is Method, 'classification accuracy' whose type is Metric, 'method' whose type is Generic, 'spoken language system domains' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we investigate the problem of automatically predicting  segment boundaries  in  spoken multiparty dialogue  . We extend prior work in two ways. We first apply approaches that have been proposed for  predicting top-level topic shifts  to the problem of  identifying subtopic boundaries  . We then explore the impact on  performance  of using  ASR output  as opposed to  human transcription  . Examination of the effect of  features  shows that  predicting top-level and predicting subtopic boundaries  are two distinct tasks: (1) for predicting  subtopic boundaries  , the  lexical cohesion-based approach  alone can achieve competitive results, (2) for  predicting top-level boundaries  , the  machine learning approach  that combines  lexical-cohesion and conversational features  performs best, and (3)  conversational cues  , such as  cue phrases  and  overlapping speech  , are better indicators for the top-level prediction task. We also find that the  transcription errors  inevitable in  ASR output  have a negative impact on models that combine  lexical-cohesion and conversational features  , but do not change the general preference of approach for the two tasks. ", "output": "The abstract text contains these key words below: 'predicting  segment boundaries' whose type is Task, 'spoken multiparty dialogue' whose type is Material, 'approaches' whose type is Generic, 'predicting top-level topic shifts' whose type is Task, 'identifying subtopic boundaries' whose type is Task, 'ASR output' whose type is OtherScientificTerm, 'human transcription' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'predicting top-level and predicting subtopic boundaries' whose type is Task, 'tasks' whose type is Generic, 'predicting  subtopic boundaries' whose type is Task, 'lexical cohesion-based approach' whose type is Method, 'predicting top-level boundaries' whose type is Task, 'machine learning approach' whose type is Method, 'lexical-cohesion and conversational features' whose type is OtherScientificTerm, 'conversational cues' whose type is OtherScientificTerm, 'cue phrases' whose type is OtherScientificTerm, 'overlapping speech' whose type is OtherScientificTerm, 'indicators' whose type is Generic, 'top-level prediction task' whose type is Generic, 'transcription errors' whose type is OtherScientificTerm, 'ASR output' whose type is OtherScientificTerm, 'models' whose type is Generic, 'lexical-cohesion and conversational features' whose type is OtherScientificTerm, 'tasks' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We address appropriate  user modeling  in order to generate  cooperative responses  to each  user  in  spoken dialogue systems  . Unlike previous studies that focus on  user  's  knowledge  or typical kinds of  users  , the  user model  we propose is more comprehensive. Specifically, we set up three dimensions of  user models  :  skill level  to the system,  knowledge level  on the  target domain  and the degree of  hastiness  . Moreover, the  models  are automatically derived by  decision tree learning  using real  dialogue data  collected by the system. We obtained reasonable  classification accuracy  for all dimensions.  Dialogue strategies  based on the  user modeling  are implemented in  Kyoto city bus information system  that has been developed at our laboratory. Experimental evaluation shows that the  cooperative responses  adaptive to  individual users  serve as good guidance for  novice users  without increasing the  dialogue duration  for  skilled users  . ", "output": "The abstract text contains these key words below: 'user modeling' whose type is Method, 'cooperative responses' whose type is OtherScientificTerm, 'spoken dialogue systems' whose type is Task, 'studies' whose type is Generic, 'user model' whose type is Method, 'user models' whose type is Method, 'models' whose type is Generic, 'decision tree learning' whose type is Method, 'real  dialogue data' whose type is Material, 'system' whose type is Generic, 'classification accuracy' whose type is Metric, 'Dialogue strategies' whose type is Method, 'user modeling' whose type is Method, 'Kyoto city bus information system' whose type is Method, 'cooperative responses' whose type is OtherScientificTerm, 'dialogue duration' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e study the number of hidden layers required by a multilayer neu-ral network with threshold units to compute a function f from n d to {O, I}. In dimension d = 2, Gibson characterized the functions computable with just one hidden layer, under the assumption that there is no \"multiple intersection point\" and that f is only defined on a compact set. We consider the restriction of f to the neighborhood of a multiple intersection point or of infinity, and give necessary and sufficient conditions for it to be locally computable with one hidden layer. We show that adding these conditions to Gib-son's assumptions is not sufficient to ensure global computability with one hidden layer, by exhibiting a new non-local configuration, the \"critical cycle\", which implies that f is not computable with one hidden layer.", "output": "The abstract text contains these key words below: 'number of hidden layers' whose type is OtherScientificTerm, 'multilayer neu-ral network' whose type is Method, 'threshold units' whose type is OtherScientificTerm, 'hidden layer' whose type is OtherScientificTerm, 'hidden layer' whose type is OtherScientificTerm, 'global computability' whose type is OtherScientificTerm, 'hidden layer' whose type is OtherScientificTerm, 'non-local configuration' whose type is OtherScientificTerm, '\"critical cycle\"' whose type is OtherScientificTerm, 'hidden layer' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he problem of predicting image or video interestingness from their low-level feature representations has received increasing interest. As a highly subjective visual attribute, annotating the interesting-ness value of training data for learning a prediction model is challenging. To make the annotation less subjective and more reliable, recent studies employ crowdsourcing tools to collect pairwise comparisons \u2013 relying on majority voting to prune the annotation outliers/errors. In this paper, we propose a more principled way to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem, tackling both the outlier detection and interestingness prediction tasks jointly. Extensive experiments on both image and video interestingness benchmark datasets demonstrate that our new approach significantly outperforms state-of-the-art alternatives.", "output": "The abstract text contains these key words below: 'predicting image or video interestingness' whose type is Task, 'low-level feature representations' whose type is Method, 'subjective visual attribute' whose type is OtherScientificTerm, 'interesting-ness value' whose type is OtherScientificTerm, 'prediction model' whose type is Method, 'crowdsourcing tools' whose type is Method, 'pairwise comparisons' whose type is OtherScientificTerm, 'majority voting' whose type is Method, 'annotation outliers/errors' whose type is OtherScientificTerm, 'way' whose type is Generic, 'annotation outliers' whose type is OtherScientificTerm, 'interestingness prediction task' whose type is Task, 'unified robust learning' whose type is Method, 'rank problem' whose type is Task, 'outlier detection' whose type is Task, 'interestingness prediction tasks' whose type is Task, 'image and video interestingness benchmark datasets' whose type is Material, 'approach' whose type is Generic, 'state-of-the-art alternatives' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Lyric-based song sentiment classification  seeks to assign songs appropriate  sentiment labels  such as light-hearted heavy-hearted. Four problems render  vector space model (VSM)-based text classification approach  ineffective: 1) Many  words  within  song lyrics  actually contribute little to  sentiment ; 2)  Nouns  and  verbs  used to express  sentiment  are ambiguous; 3)  Negations  and  modifiers  around the  sentiment keywords  make particular contributions to  sentiment ; 4)  Song lyric  is usually very short. To address these problems, the  sentiment vector space model (s-VSM)  is proposed to represent  song lyric document . The preliminary experiments prove that the  s-VSM model  outperforms the  VSM model  in the  lyric-based song sentiment classification task . \n", "output": "The abstract text contains these key words below: 'Lyric-based song sentiment classification' whose type is Task, 'vector space model (VSM)-based text classification approach' whose type is Method, 'song lyrics' whose type is Material, 'sentiment' whose type is OtherScientificTerm, 'Negations' whose type is OtherScientificTerm, 'modifiers' whose type is OtherScientificTerm, 'sentiment keywords' whose type is OtherScientificTerm, 'sentiment' whose type is OtherScientificTerm, 'Song lyric' whose type is Material, 'sentiment vector space model (s-VSM)' whose type is Method, 'song lyric document' whose type is Material, 's-VSM model' whose type is Method, 'VSM model' whose type is Method, 'lyric-based song sentiment classification task' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper describes  FERRET  , an  interactive question-answering (Q/A) system  designed to address the challenges of integrating  automatic Q/A  applications into real-world environments.  FERRET  utilizes a novel approach to  Q/A  known as  predictive questioning  which attempts to identify the  questions  (and  answers  ) that  users  need by analyzing how a  user  interacts with a system while gathering information related to a particular scenario. ", "output": "The abstract text contains these key words below: 'FERRET' whose type is Method, 'interactive question-answering (Q/A) system' whose type is Method, 'integrating  automatic Q/A  applications into real-world environments' whose type is Task, 'FERRET' whose type is Method, 'approach' whose type is Generic, 'Q/A' whose type is Method, 'predictive questioning' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "s it possible to use out-of-domain acoustic training data to improve a speech recognizer's performance on a speciic, independent application? In our experiments, we use Wallstreet Journal (WSJ) data to train a recognizer, which is adapted and evaluated in the Phonebook domain. Apart from their common language (US English), the two corpora diier in many important respects: microphone vs. telephone channel, continuous speech vs. isolated words, mismatch i n s p e a k i n g r a t e. This paper deals with two questions. First, starting from the WSJ-trained recognizer, how much adaptation data (taken from the Phonebook training corpus) is necessary to achieve a reasonable recognition performance in spite of the high degree of mismatch? Second, is it possible to improve the recognition performance of a Phonebook-trained baseline acoustic model by using additional out-of-domain training data? The paper describes the adaptation and normalization techniques used to bridge the mismatch b e-tween the two corpora.", "output": "The abstract text contains these key words below: 'out-of-domain acoustic training data' whose type is Material, 'speech recognizer' whose type is Method, 'Wallstreet Journal (WSJ) data' whose type is Material, 'recognizer' whose type is Method, 'Phonebook domain' whose type is Material, 'common language (US English)' whose type is OtherScientificTerm, 'microphone vs. telephone channel' whose type is OtherScientificTerm, 'continuous speech' whose type is OtherScientificTerm, 'isolated words' whose type is OtherScientificTerm, 'WSJ-trained recognizer' whose type is Method, 'adaptation data' whose type is Material, 'Phonebook training corpus' whose type is Material, 'recognition' whose type is Task, 'mismatch' whose type is OtherScientificTerm, 'recognition' whose type is Task, 'Phonebook-trained baseline acoustic model' whose type is Method, 'out-of-domain training data' whose type is Material, 'adaptation and normalization techniques' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we use the  information redundancy  in  multilingual input  to correct errors in  machine translation  and thus improve the quality of  multilingual summaries  . We consider the case of  multi-document summarization  , where the input  documents  are in  Arabic  , and the output  summary  is in  English  . Typically, information that makes it to a  summary  appears in many different  lexical-syntactic forms  in the input  documents  . Further, the use of multiple  machine translation systems  provides yet more  redundancy  , yielding different ways to realize that  information  in  English  . We demonstrate how errors in the  machine translations  of the input  Arabic documents  can be corrected by identifying and generating from such  redundancy  , focusing on  noun phrases  . ", "output": "The abstract text contains these key words below: 'information redundancy  in  multilingual input' whose type is OtherScientificTerm, 'machine translation' whose type is Task, 'multilingual summaries' whose type is Task, 'multi-document summarization' whose type is Task, 'Arabic' whose type is Material, 'English' whose type is Material, 'lexical-syntactic forms' whose type is OtherScientificTerm, 'machine translation systems' whose type is Method, 'English' whose type is Material, 'machine translations' whose type is Task, 'Arabic documents' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper considers the problem of reconstructing the motion of a 3D articulated tree from 2D point correspondences subject to some temporal prior. Hitherto, smooth motion has been encouraged using a trajectory basis, yielding a hard combinatorial problem with time complexity growing exponentially in the number of frames. Branch and bound strategies have previously attempted to curb this complexity whilst maintaining global optimality. However, they provide no guarantee of being more efficient than exhaustive search. Inspired by recent work which reconstructs general trajectories using compact high-pass filters, we develop a dynamic programming approach which scales linearly in the number of frames, leveraging the intrinsically local nature of filter interactions. Extension to affine projection enables reconstruction without estimating cameras.", "output": "The abstract text contains these key words below: 'reconstructing the motion of a 3D articulated tree' whose type is Task, '2D point correspondences' whose type is OtherScientificTerm, 'temporal prior' whose type is OtherScientificTerm, 'smooth motion' whose type is OtherScientificTerm, 'trajectory basis' whose type is OtherScientificTerm, 'hard combinatorial problem' whose type is Task, 'time complexity' whose type is Metric, 'Branch and bound strategies' whose type is Method, 'complexity' whose type is Generic, 'global optimality' whose type is OtherScientificTerm, 'they' whose type is Generic, 'exhaustive search' whose type is Method, 'compact high-pass filters' whose type is Method, 'dynamic programming approach' whose type is Method, 'filter interactions' whose type is OtherScientificTerm, 'affine projection' whose type is Method, 'reconstruction' whose type is Task, 'estimating cameras' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " A deterministic parser  is under development which represents a departure from  traditional deterministic parsers  in that it combines both  symbolic and connectionist components . The connectionist component is trained either from  patterns  derived from the  rules  of a  deterministic grammar . The development and evolution of such a  hybrid architecture  has lead to a  parser  which is superior to any  known deterministic parser . Experiments are described and powerful  training techniques  are demonstrated that permit  decision-making  by the  connectionist component  in the  parsing process . This approach has permitted some simplifications to the  rules  of other  deterministic parsers , including the elimination of  rule packets  and priorities. Furthermore,  parsing  is performed more robustly and with more tolerance for error. Data are presented which show how a  connectionist (neural) network  trained with  linguistic rules  can parse both  expected (grammatical) sentences  as well as some novel (ungrammatical or lexically ambiguous) sentences.\n", "output": "The abstract text contains these key words below: 'deterministic parser' whose type is Method, 'deterministic parsers' whose type is Method, 'it' whose type is Generic, 'symbolic and connectionist components' whose type is Method, 'connectionist component' whose type is Method, 'patterns' whose type is OtherScientificTerm, 'rules  of a  deterministic grammar' whose type is Method, 'hybrid architecture' whose type is Method, 'parser' whose type is Method, 'deterministic parser' whose type is Method, 'training techniques' whose type is Method, 'decision-making' whose type is Task, 'connectionist component' whose type is Method, 'parsing process' whose type is Method, 'approach' whose type is Generic, 'rules' whose type is OtherScientificTerm, 'deterministic parsers' whose type is Method, 'parsing' whose type is Task, 'connectionist (neural) network' whose type is Method, 'linguistic rules' whose type is OtherScientificTerm, 'expected (grammatical) sentences' whose type is Material, '(ungrammatical or lexically ambiguous) sentences' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "A  model  is presented to characterize the  class of languages  obtained by adding  reduplication  to  context-free languages . The  model  is a  pushdown automaton  augmented with the ability to check  reduplication  by using the  stack  in a new way. The  class of languages  generated is shown to lie strictly between the  context-free languages  and the  indexed languages . The  model  appears capable of accommodating the sort of  reduplications  that have been observed to occur in  natural languages , but it excludes many of the unnatural  constructions  that other  formal models  have permitted.\n", "output": "The abstract text contains these key words below: 'model' whose type is Generic, 'class of languages' whose type is OtherScientificTerm, 'reduplication' whose type is OtherScientificTerm, 'context-free languages' whose type is Material, 'model' whose type is Method, 'pushdown automaton' whose type is Method, 'reduplication' whose type is OtherScientificTerm, 'stack' whose type is OtherScientificTerm, 'class of languages' whose type is OtherScientificTerm, 'context-free languages' whose type is Material, 'indexed languages' whose type is Material, 'model' whose type is Generic, 'reduplications' whose type is OtherScientificTerm, 'natural languages' whose type is Material, 'it' whose type is Generic, 'formal models' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Statistical machine translation (SMT)  is currently one of the hot spots in  natural language processing  . Over the last few years dramatic improvements have been made, and a number of comparative evaluations have shown, that  SMT  gives competitive results to  rule-based translation systems  , requiring significantly less development time. This is particularly important when building  translation systems  for new  language pairs  or new  domains  . This workshop is intended to give an introduction to  statistical machine translation  with a focus on practical considerations. Participants should be able, after attending this workshop, to set out building an  SMT system  themselves and achieving good  baseline results  in a short time. The tutorial will cover the basics of  SMT  : Theory will be put into practice.  STTK  , a  statistical machine translation tool kit  , will be introduced and used to build a working  translation system  .  STTK  has been developed by the presenter and co-workers over a number of years and is currently used as the basis of  CMU's SMT system  . It has also successfully been coupled with  rule-based and example based machine translation modules  to build a  multi engine machine translation system  . The  source code  of the  tool kit  will be made available. ", "output": "The abstract text contains these key words below: 'Statistical machine translation (SMT)' whose type is Method, 'natural language processing' whose type is Task, 'SMT' whose type is Method, 'rule-based translation systems' whose type is Method, 'translation systems' whose type is Task, 'new  language pairs' whose type is OtherScientificTerm, 'new  domains' whose type is OtherScientificTerm, 'statistical machine translation' whose type is Method, 'SMT system' whose type is Method, 'SMT' whose type is Method, 'STTK' whose type is Method, 'statistical machine translation tool kit' whose type is Method, 'translation system' whose type is Task, 'STTK' whose type is Method, 'SMT system' whose type is Method, 'It' whose type is Generic, 'rule-based and example based machine translation modules' whose type is Method, 'multi engine machine translation system' whose type is Task, 'tool kit' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Coedition  of a  natural language text  and its representation in some  interlingual form  seems the best and simplest way to share  text revision  across  languages . For various reasons,  UNL graphs  are the best candidates in this context. We are developing a  prototype  where, in the simplest  sharing scenario , naive users interact directly with the  text  in their  language (L0) , and indirectly with the associated  graph . The modified  graph  is then sent to the  UNL-L0 deconverter  and the result shown. If is is satisfactory, the errors were probably due to the  graph , not to the  deconverter , and the  graph  is sent to  deconverters  in other  languages . Versions in some other  languages  known by the user may be displayed, so that improvement sharing is visible and encouraging. As new versions are added with appropriate  tags  and  attributes  in the  original multilingual document , nothing is ever lost, and cooperative working on a  document  is rendered feasible. On the internal side, liaisons are established between elements of the  text  and the  graph  by using broadly available resources such as a  LO-English or better a L0-UNL dictionary , a  morphosyntactic parser of L0 , and a  canonical graph2tree transformation . Establishing a \"best\" correspondence between the \" UNL-tree+L0 \" and the \" MS-L0 structure \", a  lattice , may be done using the  dictionary  and trying to align the  tree  and the selected  trajectory  with as few  crossing liaisons  as possible. A central goal of this research is to merge approaches from  pivot MT ,  interactive MT , and  multilingual text authoring .\n", "output": "The abstract text contains these key words below: 'Coedition' whose type is Task, 'natural language text' whose type is Material, 'text revision' whose type is OtherScientificTerm, 'languages' whose type is Material, 'UNL graphs' whose type is OtherScientificTerm, 'graph' whose type is OtherScientificTerm, 'graph' whose type is OtherScientificTerm, 'UNL-L0 deconverter' whose type is Method, 'graph' whose type is OtherScientificTerm, 'deconverter' whose type is Method, 'graph' whose type is OtherScientificTerm, 'deconverters' whose type is Method, 'original multilingual document' whose type is Material, 'liaisons' whose type is OtherScientificTerm, 'resources' whose type is Generic, 'LO-English or better a L0-UNL dictionary' whose type is Material, 'morphosyntactic parser of L0' whose type is Method, 'canonical graph2tree transformation' whose type is OtherScientificTerm, 'UNL-tree+L0' whose type is OtherScientificTerm, 'MS-L0 structure' whose type is OtherScientificTerm, 'lattice' whose type is OtherScientificTerm, 'dictionary' whose type is Material, 'crossing liaisons' whose type is OtherScientificTerm, 'pivot MT' whose type is Task, 'interactive MT' whose type is Task, 'multilingual text authoring' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Previous research has demonstrated the utility of  clustering  in inducing  semantic verb classes  from undisambiguated  corpus data  . We describe a new approach which involves clustering  subcategorization frame (SCF)  distributions using the  Information Bottleneck  and  nearest neighbour  methods. In contrast to previous work, we particularly focus on clustering  polysemic verbs  . A novel  evaluation scheme  is proposed which accounts for the effect of  polysemy  on the  clusters  , offering us a good insight into the potential and limitations of  semantically classifying   undisambiguated SCF data  . ", "output": "The abstract text contains these key words below: 'clustering' whose type is Method, 'inducing  semantic verb classes' whose type is Task, 'undisambiguated  corpus data' whose type is Material, 'approach' whose type is Generic, 'clustering  subcategorization frame (SCF)  distributions' whose type is Task, 'Information Bottleneck  and  nearest neighbour  methods' whose type is Method, 'clustering  polysemic verbs' whose type is Task, 'polysemic verbs' whose type is OtherScientificTerm, 'evaluation scheme' whose type is Generic, 'polysemy' whose type is OtherScientificTerm, 'clusters' whose type is OtherScientificTerm, 'semantically classifying   undisambiguated SCF data' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We argue in favor of the the use of  labeled directed graph  to represent various types of  linguistic structures , and illustrate how this allows one to view  NLP tasks  as  graph transformations . We present a general method for learning such  transformations  from an  annotated corpus  and describe experiments with two applications of the method:  identification of non-local depenencies  (using  Penn Treebank data ) and  semantic role labeling  (using  Proposition Bank data ).\n", "output": "The abstract text contains these key words below: 'labeled directed graph' whose type is Method, 'linguistic structures' whose type is OtherScientificTerm, 'this' whose type is Generic, 'NLP tasks' whose type is Task, 'graph transformations' whose type is Task, 'method' whose type is Method, 'transformations' whose type is Generic, 'annotated corpus' whose type is Material, 'applications' whose type is Generic, 'method' whose type is Generic, 'identification of non-local depenencies' whose type is Task, 'Penn Treebank data' whose type is Material, 'semantic role labeling' whose type is Task, 'Proposition Bank data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we propose a human action recognition system suitable for embedded computer vision applications in security systems, human-computer interaction and intelligent environments. Our system is suitable for embedded computer vision application based on three reasons. Firstly, the system was based on a linear Support Vector Machine (SVM) classifier where classification progress can be implemented easily and quickly in embedded hardware. Secondly , we use compacted motion features easily obtained from videos. We address the limitations of the well known Motion History Image (MHI) and propose a new Hierarchical Motion History Histogram (HMHH) feature to represent the motion information. HMHH not only provides rich motion information, but also remains computationally inexpensive. Finally, we combine MHI and HMHH together and extract a low dimension feature vector to be used in the SVM classifiers. Experimental results show that our system achieves significant improvement on the recognition performance .", "output": "The abstract text contains these key words below: 'human action recognition system' whose type is Method, 'embedded computer vision applications' whose type is Task, 'security systems' whose type is Task, 'human-computer interaction' whose type is Task, 'intelligent environments' whose type is Task, 'system' whose type is Generic, 'embedded computer vision application' whose type is Task, 'system' whose type is Generic, 'linear Support Vector Machine (SVM) classifier' whose type is Method, 'classification progress' whose type is Method, 'embedded hardware' whose type is OtherScientificTerm, 'compacted motion features' whose type is OtherScientificTerm, 'videos' whose type is Material, 'Motion History Image (MHI)' whose type is Method, 'Hierarchical Motion History Histogram (HMHH) feature' whose type is Method, 'motion information' whose type is OtherScientificTerm, 'HMHH' whose type is Method, 'rich motion information' whose type is OtherScientificTerm, 'MHI' whose type is Method, 'HMHH' whose type is Method, 'low dimension feature vector' whose type is OtherScientificTerm, 'SVM classifiers' whose type is Method, 'system' whose type is Generic, 'recognition' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we describe and evaluate a  Question Answering system  that goes beyond answering factoid questions. We focus on  FAQ-like questions and answers  , and build our system around a  noisy-channel architecture  which exploits both a  language model  for  answers  and a  transformation model  for  answer/question terms , trained on a  corpus  of 1 million  question/answer pairs  collected from the Web.\n", "output": "The abstract text contains these key words below: 'Question Answering system' whose type is Method, 'FAQ-like questions and answers' whose type is Material, 'system' whose type is Generic, 'noisy-channel architecture' whose type is Method, 'language model' whose type is Method, 'transformation model' whose type is Method, 'Web' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "A purely functional implementation of  LR-parsers  is given, together with a simple  correctness proof . It is presented as a generalization of the  recursive descent parser . For  non-LR grammars  the time-complexity of our  parser  is cubic if the functions that constitute the  parser  are implemented as  memo-functions , i.e. functions that memorize the results of previous invocations.  Memo-functions  also facilitate a simple way to construct a very compact representation of the  parse forest . For  LR(0) grammars , our algorithm is closely related to the  recursive ascent parsers  recently discovered by Kruse-man Aretz [1] and Roberts [2].  Extended CF grammars  ( grammars  with  regular expressions  at the right hand side) can be parsed with a simple modification of the  LR-parser  for normal  CF grammars .\n", "output": "The abstract text contains these key words below: 'LR-parsers' whose type is Method, 'correctness proof' whose type is Method, 'It' whose type is Generic, 'recursive descent parser' whose type is Method, 'non-LR grammars' whose type is Method, 'time-complexity' whose type is Metric, 'parser' whose type is Method, 'parser' whose type is Method, 'memo-functions' whose type is Method, 'Memo-functions' whose type is Method, 'parse forest' whose type is OtherScientificTerm, 'LR(0) grammars' whose type is Method, 'algorithm' whose type is Generic, 'recursive ascent parsers' whose type is OtherScientificTerm, 'Extended CF grammars' whose type is Method, 'grammars' whose type is Method, 'regular expressions' whose type is OtherScientificTerm, 'LR-parser' whose type is Method, 'CF grammars' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n The  TAP-XL Automated Analyst's Assistant  is an application designed to help an  English  -speaking analyst write a  topical report  , culling information from a large inflow of  multilingual, multimedia data  . It gives users the ability to spend their time finding more data relevant to their task, and gives them translingual reach into other  languages  by leveraging  human language technology  . ", "output": "The abstract text contains these key words below: 'TAP-XL Automated Analyst's Assistant' whose type is Task, 'multilingual, multimedia data' whose type is Material, 'It' whose type is Generic, 'human language technology' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e previously presented a framework for segmentation of complex scenes using multiple physical hypotheses for simple image regions. A consequence of that framework was a proposal for a new approach to the segmentation of complex scenes into regions corresponding to coherent surfaces rather than merely regions of similar color. Herein we present an implementation of this new approach and show example segmentations for scenes containing multi-colored piece-wise uniform objects. Using our approach we are able to intelligently segment scenes with objects of greater complexity than previous physics-based segmentation algorithms. The results show that by using general physical models we obtain segmentations that correspond more closely to coherent surfaces in the scene than segmentations found using only color.", "output": "The abstract text contains these key words below: 'framework' whose type is Generic, 'segmentation of complex scenes' whose type is Task, 'physical hypotheses' whose type is OtherScientificTerm, 'simple image regions' whose type is OtherScientificTerm, 'framework' whose type is Generic, 'approach' whose type is Generic, 'segmentation of complex scenes' whose type is Task, 'coherent surfaces' whose type is OtherScientificTerm, 'regions of similar color' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'segmentations' whose type is OtherScientificTerm, 'scenes' whose type is Material, 'multi-colored piece-wise uniform objects' whose type is Material, 'approach' whose type is Generic, 'complexity' whose type is Metric, 'physics-based segmentation algorithms' whose type is Method, 'physical models' whose type is Method, 'coherent surfaces' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper concerns the  discourse understanding process  in  spoken dialogue systems  . This process enables the system to understand  user utterances  based on the  context  of a  dialogue  . Since multiple  candidates  for the  understanding  result can be obtained for a  user utterance  due to the  ambiguity  of  speech understanding  , it is not appropriate to decide on a single  understanding  result after each  user utterance  . By holding multiple  candidates  for  understanding  results and resolving the  ambiguity  as the  dialogue  progresses, the  discourse understanding accuracy  can be improved. This paper proposes a method for resolving this  ambiguity  based on  statistical information  obtained from  dialogue corpora  . Unlike conventional methods that use  hand-crafted rules  , the proposed method enables easy design of the  discourse understanding process  . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple  candidates  for  understanding  results is effective. ", "output": "The abstract text contains these key words below: 'discourse understanding process' whose type is Task, 'spoken dialogue systems' whose type is Method, 'system' whose type is Generic, 'user utterances' whose type is Material, 'user utterance' whose type is Material, 'ambiguity  of  speech understanding' whose type is OtherScientificTerm, 'user utterance' whose type is OtherScientificTerm, 'ambiguity' whose type is Generic, 'discourse understanding accuracy' whose type is Metric, 'method' whose type is Generic, 'ambiguity' whose type is Generic, 'statistical information' whose type is OtherScientificTerm, 'dialogue corpora' whose type is Material, 'methods' whose type is Generic, 'hand-crafted rules' whose type is OtherScientificTerm, 'method' whose type is Generic, 'discourse understanding process' whose type is Task, 'system' whose type is Generic, 'method' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The following describes recent work on the  Lincoln CSR system . Some new variations in  semiphone modeling  have been tested. A very simple improved  duration model  has reduced the  error rate  by about 10% in both  triphone and semiphone systems . A new  training strategy  has been tested which, by itself, did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique. Finally, the  recognizer  has been modified to use  bigram back-off language models . The system was then transferred from the  RM task  to the  ATIS CSR task  and a limited number of development tests performed. Evaluation test results are presented for both the  RM and ATIS CSR tasks .\n", "output": "The abstract text contains these key words below: 'Lincoln CSR system' whose type is Method, 'semiphone modeling' whose type is Method, 'duration model' whose type is Method, 'error rate' whose type is Metric, 'triphone and semiphone systems' whose type is Method, 'training strategy' whose type is Method, 'rapid adaptation technique' whose type is Method, 'recognizer' whose type is Method, 'bigram back-off language models' whose type is Method, 'system' whose type is Generic, 'RM task' whose type is Task, 'ATIS CSR task' whose type is Task, 'RM and ATIS CSR tasks' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper discusses the application of  Unification Categorial Grammar (UCG)  to the framework of  Isomorphic Grammars  for  Machine Translation  pioneered by Landsbergen. The  Isomorphic Grammars approach to MT  involves developing the  grammars  of the  Source and Target languages  in parallel, in order to ensure that  SL  and  TL  expressions which stand in the  translation relation  have  isomorphic derivations  . The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited, obviating the need for answers to  semantic questions  that we do not yet have.  Semantic  and other information may still be incorporated, but as constraints on the  translation relation  , not as levels of  textual representation  . After introducing this approach to  MT system  design, and the basics of  monolingual UCG  , we will show how the two can be integrated, and present an example from an implemented  bi-directional English-Spanish fragment  . Finally we will present some outstanding problems with the approach. ", "output": "The abstract text contains these key words below: 'Unification Categorial Grammar (UCG)' whose type is Method, 'Isomorphic Grammars' whose type is Method, 'Machine Translation' whose type is Task, 'Isomorphic Grammars approach' whose type is Method, 'MT' whose type is Task, 'translation relation' whose type is OtherScientificTerm, 'isomorphic derivations' whose type is OtherScientificTerm, 'translation relation' whose type is OtherScientificTerm, 'textual representation' whose type is Method, 'approach' whose type is Generic, 'MT system  design' whose type is Task, 'monolingual UCG' whose type is Task, 'two' whose type is Generic, 'bi-directional English-Spanish fragment' whose type is OtherScientificTerm, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper we specialize the projective unifocal, bifo-cal, and trifocal tensors to the affine case, and show how the tensors obtained relate to the registered tensors encountered in previous work. This enables us to obtain an affine specialization of known projective relations connecting points and lines across two or three views. In the simpler case of affine cameras we give neccessary and sufficient constraints on the components of the trifocal tensor, together with a simple geometric interpretation. Finally, we show how the estimation of the tensors from point correspondences is achieved through factorization, and discuss the estimation from line correspondences.", "output": "The abstract text contains these key words below: 'projective unifocal, bifo-cal, and trifocal tensors' whose type is Method, 'affine case' whose type is OtherScientificTerm, 'tensors' whose type is Generic, 'registered tensors' whose type is OtherScientificTerm, 'affine specialization' whose type is OtherScientificTerm, 'affine cameras' whose type is Method, 'trifocal tensor' whose type is Method, 'geometric interpretation' whose type is OtherScientificTerm, 'estimation of the tensors' whose type is Task, 'tensors' whose type is Generic, 'point correspondences' whose type is OtherScientificTerm, 'factorization' whose type is Method, 'estimation' whose type is Generic, 'line correspondences' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n A method for  error correction  of  ill-formed input  is described that acquires  dialogue patterns  in typical usage and uses these  patterns  to predict new inputs.  Error correction  is done by strongly biasing  parsing  toward expected  meanings  unless clear evidence from the input shows the current  sentence  is not expected. A  dialogue acquisition and tracking algorithm  is presented along with a description of its  implementation  in a  voice interactive system  . A series of tests are described that show the power of the  error correction methodology  when  stereotypic dialogue  occurs. ", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'error correction' whose type is Task, 'ill-formed input' whose type is Material, 'dialogue patterns' whose type is OtherScientificTerm, 'patterns' whose type is Generic, 'Error correction' whose type is Task, 'parsing' whose type is Task, 'dialogue acquisition and tracking algorithm' whose type is Method, 'voice interactive system' whose type is Method, 'error correction methodology' whose type is Method, 'stereotypic dialogue' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper introduces a method for  computational analysis of move structures  in  abstracts  of  research articles  . In our approach,  sentences  in a given  abstract  are analyzed and labeled with a specific  move  in light of various  rhetorical functions  . The method involves automatically gathering a large number of  abstracts  from the  Web  and building a  language model  of  abstract moves  . We also present a prototype  concordancer  ,  CARE  , which exploits the  move-tagged abstracts  for  digital learning  . This system provides a promising approach to  Web-based computer-assisted academic writing  . ", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'computational analysis of move structures' whose type is Task, 'abstracts  of  research articles' whose type is Material, 'approach' whose type is Generic, 'rhetorical functions' whose type is OtherScientificTerm, 'method' whose type is Generic, 'abstracts' whose type is Material, 'Web' whose type is Material, 'language model' whose type is Method, 'abstract moves' whose type is Material, 'prototype  concordancer' whose type is Method, 'CARE' whose type is Method, 'move-tagged abstracts' whose type is Material, 'digital learning' whose type is Task, 'system' whose type is Generic, 'approach' whose type is Generic, 'Web-based computer-assisted academic writing' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " CriterionSM Online Essay Evaluation Service  includes a capability that labels  sentences  in student  writing  with  essay-based discourse elements  (e.g.,  thesis statements  ). We describe a new system that enhances  Criterion  's capability, by evaluating multiple aspects of  coherence  in  essays  . This system identifies  features  of  sentences  based on  semantic similarity measures  and  discourse structure  . A  support vector machine  uses these  features  to capture  breakdowns in coherence  due to relatedness to the  essay question  and relatedness between  discourse elements  .  Intra-sentential quality  is evaluated with  rule-based heuristics  . Results indicate that the system yields higher performance than a  baseline  on all three aspects. ", "output": "The abstract text contains these key words below: 'CriterionSM Online Essay Evaluation Service' whose type is Task, 'essay-based discourse elements' whose type is OtherScientificTerm, 'thesis statements' whose type is OtherScientificTerm, 'system' whose type is Generic, 'Criterion  's capability' whose type is OtherScientificTerm, 'coherence  in  essays' whose type is Metric, 'system' whose type is Generic, 'features' whose type is OtherScientificTerm, 'semantic similarity measures' whose type is Metric, 'discourse structure' whose type is OtherScientificTerm, 'support vector machine' whose type is Method, 'features' whose type is OtherScientificTerm, 'breakdowns in coherence' whose type is OtherScientificTerm, 'discourse elements' whose type is OtherScientificTerm, 'Intra-sentential quality' whose type is Metric, 'rule-based heuristics' whose type is Method, 'system' whose type is Generic, 'baseline' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe a novel approach to  statistical machine translation  that combines  syntactic information  in the  source language  with recent advances in  phrasal translation  . This method requires a  source-language   dependency parser  ,  target language   word segmentation  and an  unsupervised word alignment component  . We align a  parallel corpus  , project the  source dependency parse  onto the target  sentence  , extract  dependency treelet translation pairs  , and train a  tree-based ordering model  . We describe an efficient  decoder  and show that using these  tree-based models  in combination with conventional  SMT models  provides a promising approach that incorporates the power of  phrasal SMT  with the linguistic generality available in a  parser  . ", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'statistical machine translation' whose type is Task, 'syntactic information' whose type is OtherScientificTerm, 'phrasal translation' whose type is Task, 'method' whose type is Generic, 'source-language   dependency parser' whose type is Method, 'target language   word segmentation' whose type is Method, 'unsupervised word alignment component' whose type is Method, 'parallel corpus' whose type is Material, 'source dependency parse' whose type is OtherScientificTerm, 'dependency treelet translation pairs' whose type is OtherScientificTerm, 'tree-based ordering model' whose type is Method, 'decoder' whose type is Method, 'tree-based models' whose type is Method, 'SMT models' whose type is Method, 'approach' whose type is Generic, 'phrasal SMT' whose type is Method, 'linguistic generality' whose type is OtherScientificTerm, 'parser' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose a novel approach to associate objects across multiple PTZ cameras that can be used to perform camera handoff in wide-area surveillance scenarios. While previous approaches relied on geometric, appearance, or correlation-based information for establishing correspondences between static cameras, they each have well-known limitations and are not extendable to wide-area settings with PTZ cameras. In our approach, the slave camera only passively follows the target (by loose registration with the master) and bootstraps itself from its own incoming imagery , thus effectively circumventing the problems faced by previous approaches and avoiding the need to perform any model transfer. Towards this goal, we also propose a novel Multiple Instance Learning (MIL) formulation for the problem based on the logistic softmax function of covariance-based region features within a MAP estimation framework. We demonstrate our approach with multiple PTZ camera sequences in typical outdoor surveillance settings and show a comparison with state-of-the-art approaches.", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'PTZ cameras' whose type is OtherScientificTerm, 'camera handoff in wide-area surveillance scenarios' whose type is Task, 'approaches' whose type is Generic, 'geometric, appearance, or correlation-based information' whose type is OtherScientificTerm, 'static cameras' whose type is OtherScientificTerm, 'wide-area settings' whose type is OtherScientificTerm, 'PTZ cameras' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'slave camera' whose type is OtherScientificTerm, 'approaches' whose type is Generic, 'model transfer' whose type is OtherScientificTerm, 'Multiple Instance Learning (MIL) formulation' whose type is Method, 'logistic softmax function of covariance-based region features' whose type is OtherScientificTerm, 'MAP estimation framework' whose type is Method, 'approach' whose type is Generic, 'multiple PTZ camera sequences' whose type is OtherScientificTerm, 'outdoor surveillance settings' whose type is OtherScientificTerm, 'state-of-the-art approaches' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we will describe a  search tool  for a huge set of  ngrams . The tool supports  queries  with an arbitrary number of  wildcards . It takes a fraction of a second for a search, and can provide the  fillers  of the  wildcards . The system runs on a single Linux PC with reasonable size  memory  (less than 4GB) and  disk space  (less than 400GB). This system can be a very useful tool for  linguistic knowledge discovery  and other  NLP tasks .\n", "output": "The abstract text contains these key words below: 'search tool' whose type is Method, 'ngrams' whose type is OtherScientificTerm, 'tool' whose type is Generic, 'It' whose type is Generic, 'system' whose type is Generic, 'memory' whose type is OtherScientificTerm, 'disk space' whose type is OtherScientificTerm, 'system' whose type is Generic, 'tool' whose type is Generic, 'linguistic knowledge discovery' whose type is Task, 'NLP tasks' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "One of the major problems one is faced with when decomposing  words  into their  constituent parts  is  ambiguity : the  generation  of multiple  analyses  for one  input word , many of which are implausible. In order to deal with  ambiguity , the  MORphological PArser MORPA  is provided with a  probabilistic context-free grammar (PCFG) , i.e. it combines a  \"conventional\" context-free morphological grammar  to filter out  ungrammatical segmentations  with a  probability-based scoring function  which determines the likelihood of each successful  parse . Consequently, remaining  analyses  can be ordered along a scale of plausibility. Test performance data will show that a  PCFG  yields good results in  morphological parsing .  MORPA  is a fully implemented  parser  developed for use in a  text-to-speech conversion system .\n", "output": "The abstract text contains these key words below: 'ambiguity' whose type is OtherScientificTerm, 'generation' whose type is Task, 'ambiguity' whose type is OtherScientificTerm, 'MORphological PArser MORPA' whose type is Method, 'probabilistic context-free grammar (PCFG)' whose type is Method, 'it' whose type is Generic, '\"conventional\" context-free morphological grammar' whose type is Method, 'ungrammatical segmentations' whose type is OtherScientificTerm, 'probability-based scoring function' whose type is Method, 'parse' whose type is Task, 'PCFG' whose type is Method, 'morphological parsing' whose type is Task, 'MORPA' whose type is Method, 'parser' whose type is Method, 'text-to-speech conversion system' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Sentence boundary detection  in  speech  is important for enriching  speech recognition  output, making it easier for humans to read and downstream modules to process. In previous work, we have developed  hidden Markov model (HMM) and maximum entropy (Maxent) classifiers  that integrate textual and prosodic  knowledge sources  for detecting  sentence boundaries . In this paper, we evaluate the use of a  conditional random field (CRF)  for this task and relate results with this model to our prior work. We evaluate across two corpora (conversational  telephone speech  and  broadcast news speech ) on both  human transcriptions  and  speech recognition  output. In general, our  CRF  model yields a lower error rate than the  HMM and Max-ent models  on the  NIST sentence boundary detection task  in  speech , although it is interesting to note that the best results are achieved by  three-way voting  among the  classifiers . This probably occurs because each  model  has different strengths and weaknesses for modeling the  knowledge sources .\n", "output": "The abstract text contains these key words below: 'Sentence boundary detection' whose type is Task, 'speech' whose type is Material, 'speech recognition  output' whose type is OtherScientificTerm, 'it' whose type is Generic, 'hidden Markov model (HMM) and maximum entropy (Maxent) classifiers' whose type is Method, 'textual and prosodic  knowledge sources' whose type is Material, 'detecting  sentence boundaries' whose type is Task, 'conditional random field (CRF)' whose type is Method, 'task' whose type is Generic, 'model' whose type is Generic, 'corpora' whose type is Generic, 'conversational  telephone speech' whose type is Material, 'broadcast news speech' whose type is Material, 'human transcriptions' whose type is OtherScientificTerm, 'speech recognition  output' whose type is OtherScientificTerm, 'CRF  model' whose type is Method, 'error rate' whose type is Metric, 'HMM and Max-ent models' whose type is Method, 'NIST sentence boundary detection task' whose type is Material, 'speech' whose type is Material, 'three-way voting' whose type is Method, 'classifiers' whose type is Method, 'model' whose type is Generic, 'knowledge sources' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " model-based approach to on-line cursive handwriting analysis and recognition is presented and evaluated. In this model, on-line handwriting is considered as a modulation of a simple cycloidal pen motion, described by two coupled oscillations with a constant linear drift along the line of the writing. By slow modulations of the amplitudes and phase lags of the two oscillators, a general pen trajectory can be efficiently encoded. These parameters are then quantized into a small number of values without altering the writing intelligibility. A general procedure for the estimation and quantization of these cycloidal motion parameters for arbitrary handwriting is presented. The result is a discrete motor control representation of the continuous pen motion, via the quantized levels of the model parameters. This motor control representation enables successful word spotting and matching of cursive scripts. Our experiments clearly indicate the potential of this dynamic representation for complete cursive handwriting recognition.", "output": "The abstract text contains these key words below: 'model-based approach' whose type is Method, 'on-line cursive handwriting analysis and recognition' whose type is Task, 'model' whose type is Generic, 'on-line handwriting' whose type is Task, 'cycloidal pen motion' whose type is Task, 'constant linear drift' whose type is OtherScientificTerm, 'pen trajectory' whose type is OtherScientificTerm, 'writing intelligibility' whose type is OtherScientificTerm, 'cycloidal motion parameters' whose type is OtherScientificTerm, 'arbitrary handwriting' whose type is Material, 'discrete motor control representation' whose type is Method, 'continuous pen motion' whose type is OtherScientificTerm, 'motor control representation' whose type is Method, 'word spotting' whose type is Task, 'matching of cursive scripts' whose type is Task, 'dynamic representation' whose type is Method, 'cursive handwriting recognition' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Towards deep analysis of  compositional classes of paraphrases , we have examined a  class-oriented framework  for collecting  paraphrase examples , in which  sentential paraphrases  are collected for each  paraphrase class  separately by means of  automatic candidate generation  and  manual judgement . Our preliminary experiments on building a  paraphrase corpus  have so far been producing promising results, which we have evaluated according to  cost-efficiency ,  exhaustiveness , and  reliability .\n", "output": "The abstract text contains these key words below: 'compositional classes of paraphrases' whose type is Task, 'class-oriented framework' whose type is Method, 'paraphrase examples' whose type is Material, 'sentential paraphrases' whose type is Material, 'automatic candidate generation' whose type is Method, 'manual judgement' whose type is Method, 'paraphrase corpus' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we propose a partially-blurred-image classification and analysis framework for automatically detecting images containing blurred regions and recognizing the blur types for those regions without needing to perform blur kernel estimation and image deblurring. We develop several blur features modeled by image color, gradient, and spectrum information, and use feature parameter training to robustly classify blurred images. Our blur detection is based on image patches, making region-wise training and classification in one image efficient. Extensive experiments show that our method works satisfactorily on challenging image data, which establishes a technical foundation for solving several computer vision problems, such as motion analysis and image restoration, using the blur information.", "output": "The abstract text contains these key words below: 'partially-blurred-image classification and analysis framework' whose type is Method, 'automatically detecting images' whose type is Task, 'images' whose type is Material, 'blurred regions' whose type is OtherScientificTerm, 'regions' whose type is Generic, 'blur kernel estimation' whose type is Task, 'image deblurring' whose type is Task, 'blur features' whose type is OtherScientificTerm, 'image color' whose type is OtherScientificTerm, 'gradient' whose type is OtherScientificTerm, 'spectrum information' whose type is OtherScientificTerm, 'feature parameter training' whose type is Method, 'blurred images' whose type is Material, 'blur detection' whose type is Method, 'image patches' whose type is OtherScientificTerm, 'region-wise training and classification' whose type is Method, 'method' whose type is Generic, 'image data' whose type is Material, 'computer vision problems' whose type is Task, 'motion analysis' whose type is Task, 'image restoration' whose type is Task, 'blur information' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents a method for blind estimation of reverberation times in reverberant enclosures. The proposed algorithm is based on a statistical model of short-term log-energy sequences for echo-free speech. Given a speech utterance recorded in a reverberant room, it computes a Maximum Likelihood estimate of the room full-band reverberation time. The estimation method is shown to require little data and to perform satisfactorily. The method has been successfully applied to robust automatic speech recognition in reverberant environments by model selection. For this application, the reverberation time is first estimated from the reverberated speech utterance to be recognized. The estimation is then used to select the best acoustic model out of a library of models trained in various artificial re-verberant conditions. Speech recognition experiments in simulated and real reverberant environments show the efficiency of our approach which outperforms standard channel normaliza-tion techniques.", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'blind estimation of reverberation times' whose type is Task, 'reverberant enclosures' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'statistical model of short-term log-energy sequences' whose type is Method, 'echo-free speech' whose type is Material, 'Maximum Likelihood estimate of the room full-band reverberation time' whose type is Method, 'estimation method' whose type is Generic, 'method' whose type is Generic, 'robust automatic speech recognition' whose type is Task, 'reverberant environments' whose type is OtherScientificTerm, 'model selection' whose type is Method, 'application' whose type is Generic, 'reverberation time' whose type is OtherScientificTerm, 'reverberated speech utterance' whose type is OtherScientificTerm, 'estimation' whose type is Generic, 'acoustic model' whose type is Method, 'models' whose type is Generic, 'artificial re-verberant conditions' whose type is OtherScientificTerm, 'Speech recognition' whose type is Task, 'simulated and real reverberant environments' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'channel normaliza-tion techniques' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "A proposal to deal with  French tenses  in the framework of  Discourse Representation Theory  is presented, as it has been implemented for a fragment at the  IMS . It is based on the  theory of tenses  of H. Kamp and Ch. Rohrer. Instead of using  operators  to express the  meaning  of the  tenses  the Reichenbachian point of view is adopted and refined such that the impact of the  tenses  with respect to the  meaning  of the  text  is understood as contribution to the integration of the  events  of a  sentence  in the  event structure  of the preceeding  text . Thereby a  system of relevant times  provided by the preceeding  text  and by the  temporal adverbials  of the  sentence  being processed is used. This system consists of one or more  reference times  and  temporal perspective times , the  speech time  and the  location time . The special interest of our proposal is to establish a plausible choice of anchors for the  new event  out of the  system of relevant times  and to update this  system of temporal coordinates  correctly. The problem of choice is largely neglected in the literature. In opposition to the approach of Kamp and Rohrer the exact  meaning  of the  tenses  is fixed by the  resolution component  and not in the process of  syntactic analysis . ", "output": "The abstract text contains these key words below: 'proposal' whose type is Generic, 'French tenses' whose type is OtherScientificTerm, 'Discourse Representation Theory' whose type is Method, 'it' whose type is Generic, 'IMS' whose type is Task, 'It' whose type is Generic, 'theory of tenses' whose type is OtherScientificTerm, 'operators' whose type is Generic, 'meaning  of the  tenses' whose type is OtherScientificTerm, 'event structure' whose type is OtherScientificTerm, 'system of relevant times' whose type is Method, 'preceeding  text' whose type is OtherScientificTerm, 'temporal adverbials' whose type is OtherScientificTerm, 'system' whose type is Generic, 'reference times' whose type is OtherScientificTerm, 'temporal perspective times' whose type is OtherScientificTerm, 'speech time' whose type is OtherScientificTerm, 'location time' whose type is OtherScientificTerm, 'system of relevant times' whose type is Method, 'system of temporal coordinates' whose type is Method, 'meaning  of the  tenses' whose type is OtherScientificTerm, 'resolution component' whose type is Method, 'syntactic analysis' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we want to show how the  morphological component  of an existing  NLP-system for Dutch (Dutch Medical Language Processor - DMLP)  has been extended in order to produce output that is compatible with the  language independent modules  of the  LSP-MLP system (Linguistic String Project - Medical Language Processor)  of the New York University. The former can take advantage of the  language independent developments  of the latter, while focusing on  idiosyncrasies  for  Dutch . This general strategy will be illustrated by a practical application, namely the highlighting of relevant information in a  patient discharge summary (PDS)  by means of modern  HyperText Mark-Up Language (HTML) technology . Such an application can be of use for medical administrative purposes in a hospital environment.\n", "output": "The abstract text contains these key words below: 'morphological component' whose type is Method, 'NLP-system for Dutch (Dutch Medical Language Processor - DMLP)' whose type is Method, 'language independent modules' whose type is Method, 'LSP-MLP system (Linguistic String Project - Medical Language Processor)' whose type is Method, 'former' whose type is Generic, 'latter' whose type is Generic, 'idiosyncrasies' whose type is OtherScientificTerm, 'Dutch' whose type is Material, 'application' whose type is Generic, 'highlighting of relevant information' whose type is Task, 'relevant information' whose type is Generic, 'patient discharge summary (PDS)' whose type is OtherScientificTerm, 'HyperText Mark-Up Language (HTML) technology' whose type is Method, 'application' whose type is Generic, 'medical administrative purposes' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper defines a  generative probabilistic model  of  parse trees , which we call  PCFG-LA . This  model  is an extension of  PCFG  in which  non-terminal symbols  are augmented with  latent variables . Finegrained  CFG rules  are automatically induced from a  parsed corpus  by  training  a  PCFG-LA model  using an  EM-algorithm . Because exact  parsing  with a  PCFG-LA  is  NP-hard , several  approximations  are described and empirically compared. In experiments using the  Penn WSJ corpus , our automatically trained  model  gave a  performance  of 86.6% (F1,  sentences  < 40  words ), which is comparable to that of an  unlexicalized PCFG parser  created using extensive  manual feature selection .\n", "output": "The abstract text contains these key words below: 'generative probabilistic model  of  parse trees' whose type is Method, 'PCFG-LA' whose type is Method, 'model' whose type is Generic, 'PCFG' whose type is Method, 'non-terminal symbols' whose type is OtherScientificTerm, 'latent variables' whose type is OtherScientificTerm, 'Finegrained  CFG rules' whose type is OtherScientificTerm, 'parsed corpus' whose type is Material, 'PCFG-LA model' whose type is Method, 'EM-algorithm' whose type is Method, 'exact  parsing' whose type is Task, 'PCFG-LA' whose type is Method, 'Penn WSJ corpus' whose type is Material, 'model' whose type is Generic, 'F1' whose type is Metric, 'unlexicalized PCFG parser' whose type is Method, 'manual feature selection' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The task of  machine translation (MT) evaluation  is closely related to the task of  sentence-level semantic equivalence classification  . This paper investigates the utility of applying standard  MT evaluation methods (BLEU, NIST, WER and PER)  to building  classifiers  to predict  semantic equivalence  and  entailment  . We also introduce a novel  classification method  based on  PER  which leverages  part of speech information  of the  words  contributing to the  word matches and non-matches  in the  sentence  . Our results show that  MT evaluation techniques  are able to produce useful  features  for  paraphrase classification  and to a lesser extent  entailment  . Our  technique  gives a substantial improvement in  paraphrase classification accuracy  over all of the other  models  used in the experiments. ", "output": "The abstract text contains these key words below: 'machine translation (MT) evaluation' whose type is Task, 'sentence-level semantic equivalence classification' whose type is Task, 'MT evaluation methods' whose type is Metric, 'BLEU' whose type is Metric, 'NIST' whose type is Metric, 'WER' whose type is Metric, 'PER' whose type is Metric, 'classifiers' whose type is Method, 'semantic equivalence' whose type is Task, 'entailment' whose type is Task, 'classification method' whose type is Method, 'PER' whose type is Metric, 'part of speech information' whose type is OtherScientificTerm, 'word matches and non-matches' whose type is Task, 'MT evaluation techniques' whose type is Method, 'features' whose type is OtherScientificTerm, 'paraphrase classification' whose type is Task, 'entailment' whose type is Task, 'technique' whose type is Generic, 'paraphrase classification' whose type is Task, 'paraphrase classification accuracy' whose type is Metric, 'models' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "PC based speech coders operating at bit rates below 3.0 kbits/sec are usually associated with buzzy or metallic artefacts in the synthetic speech. These are mainly attributable to the simplifying assumptions made about the excitation source, which are usually required to maintain such low bit rates. In this paper a new LPC vocoder is presented which splits the LPC excitation into two frequency bands using a variable cutoff frequency. The lower band is responsible for representing the voiced parts of speech, whilst the upper band represents unvoiced speech. In doing so the coder's performance during both mixed voicing speech and speech containing acoustic noise is greatly improved, producing soft natural sounding speech. The paper also describes new parameter determination and quantisation techniques vital to the operation of this coder at such low bit rates.", "output": "The abstract text contains these key words below: 'LPC based speech coders' whose type is Method, 'bit rates' whose type is OtherScientificTerm, 'buzzy or metallic artefacts' whose type is OtherScientificTerm, 'synthetic speech' whose type is Material, 'excitation source' whose type is OtherScientificTerm, 'low bit rates' whose type is OtherScientificTerm, 'LPC vocoder' whose type is Method, 'LPC excitation' whose type is OtherScientificTerm, 'frequency bands' whose type is OtherScientificTerm, 'variable cutoff frequency' whose type is OtherScientificTerm, 'voiced parts of speech' whose type is Material, 'unvoiced speech' whose type is Material, 'coder' whose type is Generic, 'mixed voicing speech' whose type is Material, 'speech containing acoustic noise' whose type is Material, 'soft natural sounding speech' whose type is Material, 'parameter determination' whose type is Method, 'quantisation techniques' whose type is Method, 'coder' whose type is Generic, 'low bit rates' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This article deals with the  interpretation  of  conceptual operations  underlying the communicative use of  natural language (NL)  within the  Structured Inheritance Network (SI-Nets) paradigm . The operations are reduced to  functions  of a  formal language , thus changing the level of abstraction of the operations to be performed on  SI-Nets . In this sense, operations on  SI-Nets  are not merely isomorphic to single epistemological objects, but can be viewed as a simulation of processes on a different level, that pertaining to the  conceptual system  of  NL . For this purpose, we have designed a version of  KL-ONE  which represents the  epistemological level , while the new experimental language,  KL-Conc , represents the  conceptual level . KL-Conc would seem to be a more natural and intuitive way of interacting with  SI-Nets . ", "output": "The abstract text contains these key words below: 'conceptual operations' whose type is Task, 'natural language (NL)' whose type is Material, 'Structured Inheritance Network (SI-Nets) paradigm' whose type is Method, 'operations' whose type is Generic, 'formal language' whose type is OtherScientificTerm, 'operations' whose type is Generic, 'SI-Nets' whose type is Method, 'operations' whose type is Generic, 'SI-Nets' whose type is Method, 'epistemological objects' whose type is OtherScientificTerm, 'conceptual system' whose type is Method, 'NL' whose type is Material, 'KL-ONE' whose type is Method, 'epistemological level' whose type is OtherScientificTerm, 'experimental language' whose type is Generic, 'KL-Conc' whose type is Method, 'conceptual level' whose type is OtherScientificTerm, 'KL-Conc' whose type is Method, 'SI-Nets' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Techniques for automatically training  modules of a  natural language generator  have recently been proposed, but a fundamental concern is whether the  quality  of  utterances  produced with  trainable components  can compete with  hand-crafted template-based or rule-based approaches  . In this paper We experimentally evaluate a  trainable sentence planner  for a  spoken dialogue system  by eliciting  subjective human judgments  . In order to perform an exhaustive comparison, we also evaluate a  hand-crafted template-based generation component  , two  rule-based sentence planners  , and two  baseline sentence planners  . We show that the  trainable sentence planner  performs better than the  rule-based systems  and the  baselines  , and as well as the  hand-crafted system  . ", "output": "The abstract text contains these key words below: 'Techniques' whose type is Generic, 'automatically training  modules' whose type is Method, 'natural language generator' whose type is Method, 'quality  of  utterances' whose type is Metric, 'utterances' whose type is OtherScientificTerm, 'trainable components' whose type is Method, 'hand-crafted template-based or rule-based approaches' whose type is Method, 'trainable sentence planner' whose type is Method, 'spoken dialogue system' whose type is Method, 'subjective human judgments' whose type is OtherScientificTerm, 'hand-crafted template-based generation component' whose type is Method, 'rule-based sentence planners' whose type is Method, 'baseline sentence planners' whose type is Method, 'trainable sentence planner' whose type is Method, 'rule-based systems' whose type is Method, 'baselines' whose type is Generic, 'hand-crafted system' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "tterance Verification (UV) is a critical function of an Automatic Speech Recognition (ASR) System working on real applications where spontaneous speech, out-of-vocabulary (OOV) words and acoustic noises are present. In this paper we present a new UV procedure with two major features: a) Confidence tests are applied to decoded string hypotheses obtained from using word and garbage models that represent OOV words and noises. Thus the ASR system is designed to deal with what we refer to as Word Spotting and Noise Spotting capabilities. b) The UV procedure is based on three different confidence tests, two based on acoustic measures and one founded on linguistic information, applied in a hierarchical structure. Experimental results from a real telephone application on a natural number recognition task show an 50% reduction in recognition errors with a moderate 12% rejection rate of correct utterances and a low 1.5% rate of false acceptance.", "output": "The abstract text contains these key words below: 'Utterance Verification (UV)' whose type is Method, 'Automatic Speech Recognition (ASR) System' whose type is Method, 'spontaneous speech' whose type is OtherScientificTerm, 'out-of-vocabulary (OOV) words' whose type is OtherScientificTerm, 'acoustic noises' whose type is OtherScientificTerm, 'UV procedure' whose type is Method, 'Confidence tests' whose type is Method, 'decoded string hypotheses' whose type is OtherScientificTerm, 'OOV words' whose type is OtherScientificTerm, 'noises' whose type is OtherScientificTerm, 'ASR system' whose type is Method, 'Word Spotting' whose type is Task, 'Noise Spotting capabilities' whose type is Task, 'UV procedure' whose type is Method, 'confidence tests' whose type is Method, 'two' whose type is Generic, 'acoustic measures' whose type is Metric, 'one' whose type is Generic, 'linguistic information' whose type is OtherScientificTerm, 'hierarchical structure' whose type is OtherScientificTerm, 'telephone application' whose type is Task, 'natural number recognition task' whose type is Task, 'recognition errors' whose type is Metric, 'rejection rate' whose type is Metric, 'false acceptance' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Recognition of proper nouns  in  Japanese text  has been studied as a part of the more general problem of  morphological analysis  in  Japanese text processing  ([1] [2]). It has also been studied in the framework of  Japanese information extraction  ([3]) in recent years. Our approach to the Multi-lingual Evaluation Task (MET) for  Japanese text  is to consider the given task as a  morphological analysis problem  in  Japanese . Our  morphological analyzer  has done all the necessary work for the  recognition and classification of proper names, numerical and temporal expressions, i.e. Named Entity (NE) items  in the  Japanese text . The  analyzer  is called \"Amorph\". Amorph recognizes  NE items  in two stages:  dictionary lookup  and  rule application . First, it uses several kinds of  dictionaries  to segment and tag  Japanese character strings . Second, based on the information resulting from the  dictionary lookup stage , a set of  rules  is applied to the  segmented strings  in order to identify  NE items . When a  segment  is found to be an  NE item , this information is added to the  segment  and it is used to generate the final output.\n", "output": "The abstract text contains these key words below: 'Recognition of proper nouns' whose type is Task, 'proper nouns' whose type is OtherScientificTerm, 'Japanese text' whose type is Material, 'morphological analysis' whose type is Task, 'Japanese text processing' whose type is Task, 'It' whose type is Generic, 'Japanese information extraction' whose type is Task, 'approach' whose type is Generic, 'Multi-lingual Evaluation Task (MET)' whose type is Task, 'Japanese text' whose type is Material, 'task' whose type is Generic, 'morphological analysis problem' whose type is Task, 'Japanese' whose type is Material, 'morphological analyzer' whose type is Method, 'recognition and classification of proper names, numerical and temporal expressions' whose type is Task, 'proper names, numerical and temporal expressions' whose type is OtherScientificTerm, 'Named Entity (NE) items' whose type is OtherScientificTerm, 'Japanese text' whose type is Material, 'analyzer' whose type is Generic, '\"Amorph\"' whose type is Method, 'Amorph' whose type is Method, 'NE items' whose type is OtherScientificTerm, 'dictionary lookup' whose type is Method, 'rule application' whose type is Method, 'it' whose type is Generic, 'dictionaries' whose type is OtherScientificTerm, 'Japanese character strings' whose type is OtherScientificTerm, 'dictionary lookup stage' whose type is Method, 'rules' whose type is OtherScientificTerm, 'NE items' whose type is OtherScientificTerm, 'NE item' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he co-occurrence pattern, a combination of binary or local features, is more discriminative than individual features and has shown its advantages in object, scene, and action recognition. We discuss two types of co-occurrence patterns that are complementary to each other, the conjunction (AND) and disjunction (OR) of binary features. The necessary condition of identifying discriminative co-occurrence patterns is firstly provided. Then we propose a novel data mining method to efficiently discover the optimal co-occurrence pattern with minimum empirical error, despite the noisy training dataset. This mining procedure of AND and OR patterns is readily integrated to boosting, which improves the generalization ability over the conventional boosting decision trees and boosting decision stumps. Our versatile experiments on object, scene, and action cat-egorization validate the advantages of the discovered dis-criminative co-occurrence patterns.", "output": "The abstract text contains these key words below: 'co-occurrence pattern' whose type is OtherScientificTerm, 'binary or local features' whose type is OtherScientificTerm, 'object, scene, and action recognition' whose type is Task, 'co-occurrence patterns' whose type is OtherScientificTerm, 'conjunction (AND) and disjunction (OR) of binary features' whose type is OtherScientificTerm, 'identifying discriminative co-occurrence patterns' whose type is Task, 'discriminative co-occurrence patterns' whose type is OtherScientificTerm, 'data mining method' whose type is Method, 'optimal co-occurrence pattern' whose type is OtherScientificTerm, 'minimum empirical error' whose type is OtherScientificTerm, 'noisy training dataset' whose type is Material, 'mining procedure' whose type is Generic, 'AND and OR patterns' whose type is OtherScientificTerm, 'boosting' whose type is Method, 'generalization ability' whose type is Metric, 'boosting decision trees' whose type is Method, 'boosting decision stumps' whose type is Method, 'object, scene, and action cat-egorization' whose type is Task, 'dis-criminative co-occurrence patterns' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "While  paraphrasing  is critical both for  interpretation and generation of natural language  , current systems use manual or semi-automatic methods to collect  paraphrases  . We present an  unsupervised learning algorithm  for  identification of paraphrases  from a  corpus of multiple English translations  of the same  source text  . Our approach yields  phrasal and single word lexical paraphrases  as well as  syntactic paraphrases  . ", "output": "The abstract text contains these key words below: 'paraphrasing' whose type is Method, 'interpretation and generation of natural language' whose type is Task, 'systems' whose type is Generic, 'manual or semi-automatic methods' whose type is Method, 'paraphrases' whose type is OtherScientificTerm, 'unsupervised learning algorithm' whose type is Method, 'identification of paraphrases' whose type is Task, 'corpus of multiple English translations' whose type is Material, 'approach' whose type is Generic, 'phrasal and single word lexical paraphrases' whose type is OtherScientificTerm, 'syntactic paraphrases' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We propose a solution to the challenge of the  CoNLL 2008 shared task  that uses a  generative history-based latent variable model  to predict the most likely  derivation  of a  synchronous dependency parser  for both  syntactic and semantic dependencies . The submitted  model  yields 79.1%  macro-average F1 performance , for the joint task, 86.9%  syntactic dependencies LAS  and 71.0%  semantic dependencies F1 . A larger  model  trained after the deadline achieves 80.5%  macro-average F1 , 87.6%  syntactic dependencies LAS , and 73.1%  semantic dependencies F1 .\n", "output": "The abstract text contains these key words below: 'CoNLL 2008 shared task' whose type is Task, 'generative history-based latent variable model' whose type is Method, 'synchronous dependency parser' whose type is Task, 'syntactic and semantic dependencies' whose type is OtherScientificTerm, 'model' whose type is Generic, 'macro-average F1 performance' whose type is Metric, 'task' whose type is Generic, 'syntactic dependencies LAS' whose type is Metric, 'semantic dependencies F1' whose type is Metric, 'model' whose type is Generic, 'macro-average F1' whose type is Metric, 'syntactic dependencies LAS' whose type is Metric, 'semantic dependencies F1' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We suggest a new goal and  evaluation criterion  for  word similarity measures . The new criterion \u2013  meaning-entailing substitutability  \u2013 fits the needs of  semantic-oriented NLP applications  and can be evaluated directly (independent of an application) at a good level of  human agreement . Motivated by this  semantic criterion  we analyze the empirical quality of  distributional word feature vectors  and its impact on  word similarity results , proposing an objective measure for evaluating  feature vector quality . Finally, a novel  feature weighting and selection function  is presented, which yields superior  feature vectors  and better  word similarity performance .", "output": "The abstract text contains these key words below: 'evaluation criterion' whose type is Metric, 'word similarity measures' whose type is Metric, 'criterion' whose type is Generic, 'meaning-entailing substitutability' whose type is Metric, 'semantic-oriented NLP applications' whose type is Task, 'human agreement' whose type is Metric, 'semantic criterion' whose type is Metric, 'distributional word feature vectors' whose type is OtherScientificTerm, 'word similarity' whose type is Task, 'measure' whose type is Generic, 'feature vector quality' whose type is Metric, 'feature weighting and selection function' whose type is Method, 'feature vectors' whose type is OtherScientificTerm, 'word similarity' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ue to the capacity of pan-tilt-zoom (PTZ) cameras to simultaneously cover a panoramic area and maintain high resolution imagery, researches in automated surveillance systems with multiple PTZ cameras have become increasingly important. Most existing algorithms require the prior knowledge of intrinsic parameters of the PTZ camera to infer the relative positioning and orientation among multiple PTZ cameras. To overcome this limitation, we propose a novel mapping algorithm that derives the relative positioning and orientation between two PTZ cameras based on a unified polynomial model. This reduces the dependence on the knowledge of intrinsic parameters of PTZ camera and relative positions. Experimental results demonstrate that our proposed algorithm presents substantially reduced computational complexity and improved flexibility at the cost of slightly decreased pixel accuracy, as compared with the work of Chen and Wang. This slightly decreased pixel accuracy can be compensated by consistent labeling approaches without added cost for the application of automated surveillance systems along with changing configurations and a larger number of PTZ cameras.", "output": "The abstract text contains these key words below: 'pan-tilt-zoom (PTZ) cameras' whose type is OtherScientificTerm, 'panoramic area' whose type is OtherScientificTerm, 'high resolution imagery' whose type is OtherScientificTerm, 'automated surveillance systems' whose type is Task, 'PTZ cameras' whose type is OtherScientificTerm, 'algorithms' whose type is Generic, 'prior knowledge of intrinsic parameters of the PTZ camera' whose type is OtherScientificTerm, 'relative positioning' whose type is OtherScientificTerm, 'orientation' whose type is OtherScientificTerm, 'PTZ cameras' whose type is OtherScientificTerm, 'mapping algorithm' whose type is Method, 'relative positioning' whose type is OtherScientificTerm, 'orientation' whose type is OtherScientificTerm, 'PTZ cameras' whose type is OtherScientificTerm, 'unified polynomial model' whose type is Method, 'PTZ camera' whose type is OtherScientificTerm, 'relative positions' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'computational complexity' whose type is Metric, 'flexibility' whose type is Metric, 'pixel accuracy' whose type is Metric, 'pixel accuracy' whose type is Metric, 'consistent labeling approaches' whose type is Method, 'automated surveillance systems' whose type is Task, 'PTZ cameras' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "A meaningful evaluation methodology can advance the state-of-the-art by encouraging mature, practical applications rather than \"toy\" implementations. Evaluation is also crucial to assessing competing claims and identifying promising technical approaches. While work in  speech recognition (SR)  has a history of evaluation methodologies that permit comparison among various systems, until recently no methodology existed for either developers of  natural language (NL) interfaces  or researchers in  speech understanding (SU)  to evaluate and compare the systems they developed. Recently considerable progress has been made by a number of groups involved in the  DARPA Spoken Language Systems (SLS) program  to agree on a methodology for comparative evaluation of  SLS systems , and that methodology has been put into practice several times in comparative tests of several  SLS systems . These evaluations are probably the only  NL evaluations  other than the series of  Message Understanding Conferences  (Sundheim, 1989; Sundheim, 1991) to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems (Palmer et al., 1989; Neal et al., 1991). This paper describes a practical  \"black-box\" methodology  for automatic evaluation of  question-answering NL systems . While each new application domain will require some development of special resources, the heart of the methodology is domain-independent, and it can be used with either  speech or text input . The particular characteristics of the approach are described in the following section: subsequent sections present its implementation in the  DARPA SLS community , and some problems and directions for future development.\n", "output": "The abstract text contains these key words below: 'evaluation methodology' whose type is Task, 'Evaluation' whose type is Task, 'approaches' whose type is Generic, 'speech recognition (SR)' whose type is Task, 'evaluation methodologies' whose type is Generic, 'natural language (NL) interfaces' whose type is OtherScientificTerm, 'speech understanding (SU)' whose type is Task, 'methodology' whose type is Generic, 'evaluation of  SLS systems' whose type is Task, 'SLS systems' whose type is Method, 'methodology' whose type is Generic, 'SLS systems' whose type is Method, 'evaluations' whose type is Generic, 'NL evaluations' whose type is Task, 'Message Understanding Conferences' whose type is Material, '\"black-box\" methodology' whose type is Metric, 'automatic evaluation of  question-answering NL systems' whose type is Task, 'methodology' whose type is Generic, 'it' whose type is Generic, 'speech or text input' whose type is OtherScientificTerm, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints. Objects are represented as a coherent ensemble of parts that are consistent under 3D viewpoint transformations. Each part is a collection of salient image features. A generative framework is used for learning a model that captures the relative position of parts within each of the discretized viewpoints. Contrary to most of the existing mixture of viewpoints models , our model establishes explicit correspondences of parts across different viewpoints of the object class. Given a new image, detection and classification are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects. Our approach is among the first to propose a generative proba-bilistic framework for 3D object categorization. We test our algorithm on the detection task and the viewpoint classification task by using \" car \" category from both the Savarese et al. 2007 and PASCAL VOC 2006 datasets. We show promising results in both the detection and viewpoint classification tasks on these two challenging datasets.", "output": "The abstract text contains these key words below: 'probabilistic framework' whose type is Method, 'visual models of 3D object categories' whose type is Task, 'appearance information' whose type is OtherScientificTerm, 'geometric constraints' whose type is OtherScientificTerm, '3D viewpoint transformations' whose type is OtherScientificTerm, 'salient image features' whose type is OtherScientificTerm, 'generative framework' whose type is Method, 'model' whose type is Generic, 'discretized viewpoints' whose type is OtherScientificTerm, 'mixture of viewpoints models' whose type is Method, 'model' whose type is Generic, 'viewpoints' whose type is OtherScientificTerm, 'image' whose type is Material, 'detection' whose type is Task, 'classification' whose type is Task, 'position' whose type is OtherScientificTerm, 'viewpoint' whose type is OtherScientificTerm, 'recognition scores' whose type is Metric, 'approach' whose type is Generic, 'generative proba-bilistic framework' whose type is Method, '3D object categorization' whose type is Task, 'algorithm' whose type is Generic, 'detection task' whose type is Task, 'viewpoint classification task' whose type is Task, 'PASCAL VOC 2006 datasets' whose type is Material, 'detection and viewpoint classification tasks' whose type is Task, 'datasets' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper we target at generating generic action proposals in unconstrained videos. Each action proposal corresponds to a temporal series of spatial bounding boxes, i.e., a spatio-temporal video tube, which has a good potential to locate one human action. Assuming each action is performed by a human with meaningful motion, both appearance and motion cues are utilized to measure the ac-tionness of the video tubes. After picking those spatiotem-poral paths of high actionness scores, our action proposal generation is formulated as a maximum set coverage problem , where greedy search is performed to select a set of action proposals that can maximize the overall actionness score. Compared with existing action proposal approaches, our action proposals do not rely on video segmentation and can be generated in nearly real-time. Experimental results on two challenging datasets, MSRII and UCF 101, validate the superior performance of our action proposals as well as competitive results on action detection and search.", "output": "The abstract text contains these key words below: 'generic action proposals' whose type is OtherScientificTerm, 'unconstrained videos' whose type is Material, 'action proposal' whose type is OtherScientificTerm, 'temporal series of spatial bounding boxes' whose type is OtherScientificTerm, 'spatio-temporal video tube' whose type is OtherScientificTerm, 'human action' whose type is OtherScientificTerm, 'appearance and motion cues' whose type is OtherScientificTerm, 'ac-tionness' whose type is Metric, 'video tubes' whose type is Material, 'actionness scores' whose type is Metric, 'action proposal generation' whose type is Task, 'maximum set coverage problem' whose type is Task, 'greedy search' whose type is Method, 'action proposals' whose type is OtherScientificTerm, 'actionness score' whose type is Metric, 'action proposal approaches' whose type is Method, 'action proposals' whose type is OtherScientificTerm, 'video segmentation' whose type is Method, 'datasets' whose type is Generic, 'MSRII' whose type is Material, 'UCF 101' whose type is Material, 'action proposals' whose type is OtherScientificTerm, 'action detection and search' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Words  in  Chinese text  are not naturally separated by  delimiters , which poses a challenge to  standard machine translation (MT) systems . In  MT , the widely used approach is to apply a  Chinese word segmenter  trained from  manually annotated data , using a fixed  lexicon . Such  word segmentation  is not necessarily optimal for  translation . We propose a  Bayesian semi-supervised Chinese word segmentation model  which uses both  monolingual and bilingual information  to derive a  segmentation  suitable for  MT . Experiments show that our method improves a  state-of-the-art MT system  in a small and a  large data environment .\n", "output": "The abstract text contains these key words below: 'Chinese text' whose type is Material, 'machine translation (MT) systems' whose type is Task, 'MT' whose type is Task, 'Chinese word segmenter' whose type is Method, 'manually annotated data' whose type is Material, 'word segmentation' whose type is Method, 'translation' whose type is Task, 'Bayesian semi-supervised Chinese word segmentation model' whose type is Method, 'monolingual and bilingual information' whose type is OtherScientificTerm, 'segmentation' whose type is Task, 'MT' whose type is Task, 'method' whose type is Generic, 'MT system' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Sources of  training data  suitable for  language modeling  of  conversational speech  are limited. In this paper, we show how  training data  can be supplemented with  text  from the  web  filtered to match the  style  and/or  topic  of the target  recognition task  , but also that it is possible to get bigger performance gains from the  data  by using  class-dependent interpolation  of  N-grams  . ", "output": "The abstract text contains these key words below: 'language modeling' whose type is Method, 'conversational speech' whose type is Material, 'recognition task' whose type is Task, 'class-dependent interpolation  of  N-grams' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present results on  addressee identification  in  four-participants face-to-face meetings  using  Bayesian Network  and  Naive Bayes classifiers  . First, we investigate how well the  addressee  of a  dialogue act  can be predicted based on  gaze  ,  utterance  and  conversational context features  . Then, we explore whether information about  meeting context  can aid  classifiers  '  performances  . Both  classifiers  perform the best when  conversational context  and  utterance features  are combined with  speaker's gaze information  . The  classifiers  show little  gain  from information about  meeting context  . ", "output": "The abstract text contains these key words below: 'addressee identification  in  four-participants face-to-face meetings' whose type is Task, 'Bayesian Network' whose type is Method, 'Naive Bayes classifiers' whose type is Method, 'addressee  of a  dialogue act' whose type is OtherScientificTerm, 'gaze' whose type is OtherScientificTerm, 'utterance' whose type is OtherScientificTerm, 'conversational context features' whose type is OtherScientificTerm, 'meeting context' whose type is OtherScientificTerm, 'classifiers' whose type is Method, 'classifiers' whose type is Method, 'conversational context' whose type is OtherScientificTerm, 'utterance features' whose type is OtherScientificTerm, 'speaker's gaze information' whose type is OtherScientificTerm, 'classifiers' whose type is Method, 'meeting context' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "urrent approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate the models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets.", "output": "The abstract text contains these key words below: 'approaches' whose type is Generic, 'object category recognition' whose type is Task, 'datasets' whose type is Generic, 'supervision' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'object category' whose type is OtherScientificTerm, 'image search engines' whose type is Method, 'model' whose type is Generic, 'TSI-pLSA' whose type is Method, 'pLSA' whose type is Method, 'visual words' whose type is OtherScientificTerm, 'spatial information' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'intra-class variability' whose type is OtherScientificTerm, 'unrelated images' whose type is OtherScientificTerm, 'search engines' whose type is Method, 'models' whose type is Generic, 'test sets' whose type is Generic, 'methods' whose type is Generic, 'hand prepared datasets' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "utomatic image annotation is a newly developed and promising technique to provide semantic image retrieval via text descriptions. It concerns a process of automatically labeling the image contents with a pre-defined set of keywords which are exploited to represent the image semantics. A Maximum Entropy Model-based approach to the task of automatic image annotation is proposed in this paper. In the phase of training, a basic visual vocabulary consisting of blob-tokens to describe the image content is generated at first; then the statistical relationship is modeled between the blob-tokens and keywords by a Maximum Entropy Model constructed from the training set of labeled images. In the phase of annotation, for an unlabeled image, the most likely associated keywords are predicted in terms of the blob-token set extracted from the given image. We carried out experiments on a medium-sized image collection with about 5000 images from Corel Photo CDs. The experimental results demonstrated that the annotation performance of this method outperforms some traditional annotation methods by about 8% in mean precision, showing a potential of the Maximum Entropy Model in the task of automatic image annotation.", "output": "The abstract text contains these key words below: 'Automatic image annotation' whose type is Task, 'semantic image retrieval' whose type is Task, 'text descriptions' whose type is Material, 'automatically labeling the image contents' whose type is Task, 'keywords' whose type is OtherScientificTerm, 'image semantics' whose type is OtherScientificTerm, 'Maximum Entropy Model-based approach' whose type is Method, 'automatic image annotation' whose type is Task, 'training' whose type is Task, 'visual vocabulary' whose type is OtherScientificTerm, 'blob-tokens' whose type is OtherScientificTerm, 'image content' whose type is OtherScientificTerm, 'statistical relationship' whose type is Generic, 'blob-tokens' whose type is OtherScientificTerm, 'keywords' whose type is OtherScientificTerm, 'Maximum Entropy Model' whose type is Method, 'annotation' whose type is Task, 'keywords' whose type is OtherScientificTerm, 'blob-token set' whose type is OtherScientificTerm, 'medium-sized image collection' whose type is Material, 'Corel Photo CDs' whose type is Material, 'annotation' whose type is Task, 'method' whose type is Generic, 'annotation methods' whose type is Method, 'mean precision' whose type is Metric, 'Maximum Entropy Model' whose type is Method, 'automatic image annotation' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " mixed-signal paradigm is presented for high-resolution parallel inner-product computation in very high dimensions, suitable for efficient implementation of kernels in image processing. At the core of the externally digital architecture is a high-density, low-power analog array performing binary-binary partial matrix-vector multiplication. Full digital resolution is maintained even with low-resolution analog-to-digital conversion, owing to random statistics in the analog summation of binary products. A random modulation scheme produces near-Bernoulli statistics even for highly correlated inputs. The approach is validated with real image data, and with experimental results from a CID/DRAM analog array prototype in 0.5 \u00a2 m CMOS.", "output": "The abstract text contains these key words below: 'mixed-signal paradigm' whose type is Method, 'high-resolution parallel inner-product computation' whose type is Task, 'kernels' whose type is Method, 'image processing' whose type is Task, 'externally digital architecture' whose type is Method, 'high-density, low-power analog array' whose type is OtherScientificTerm, 'binary-binary partial matrix-vector multiplication' whose type is Method, 'Full digital resolution' whose type is Metric, 'low-resolution analog-to-digital conversion' whose type is Method, 'random statistics' whose type is OtherScientificTerm, 'analog summation of binary products' whose type is OtherScientificTerm, 'random modulation scheme' whose type is Method, 'near-Bernoulli statistics' whose type is OtherScientificTerm, 'highly correlated inputs' whose type is Material, 'approach' whose type is Generic, 'real image data' whose type is Material, 'CID/DRAM analog array prototype' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper introduces a  system for categorizing unknown words . The  system  is based on a  multi-component architecture  where each  component  is responsible for identifying one class of  unknown words . The focus of this paper is the  components  that identify  names  and  spelling errors . Each  component  uses a  decision tree architecture  to combine multiple types of  evidence  about the  unknown word . The  system  is evaluated using data from  live closed captions  - a genre replete with a wide variety of  unknown words .\n", "output": "The abstract text contains these key words below: 'system' whose type is Generic, 'categorizing unknown words' whose type is Task, 'system' whose type is Generic, 'multi-component architecture' whose type is Method, 'component' whose type is Generic, 'unknown words' whose type is OtherScientificTerm, 'components' whose type is Generic, 'names' whose type is OtherScientificTerm, 'spelling errors' whose type is OtherScientificTerm, 'component' whose type is Generic, 'decision tree architecture' whose type is Method, 'unknown word' whose type is OtherScientificTerm, 'system' whose type is Generic, 'live closed captions' whose type is Material, 'unknown words' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he robust principal component analysis (robust PCA) problem has been considered in many machine learning applications, where the goal is to decompose the data matrix to a low rank part plus a sparse residual. While current approaches are developed by only considering the low rank plus sparse structure, in many applications, side information of row and/or column entities may also be given, and it is still unclear to what extent could such information help robust PCA. Thus, in this paper, we study the problem of robust PCA with side information, where both prior structure and features of entities are exploited for recovery. We propose a convex problem to incorporate side information in robust PCA and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions. In particular , our guarantee suggests that a substantial amount of low rank matrices, which cannot be recovered by standard robust PCA, become re-coverable by our proposed method. The result theoretically justifies the effectiveness of features in robust PCA. In addition, we conduct synthetic experiments as well as a real application on noisy image classification to show that our method also improves the performance in practice by exploiting side information.", "output": "The abstract text contains these key words below: 'robust principal component analysis (robust PCA) problem' whose type is Method, 'machine learning applications' whose type is Task, 'data matrix' whose type is OtherScientificTerm, 'low rank part' whose type is OtherScientificTerm, 'sparse residual' whose type is OtherScientificTerm, 'approaches' whose type is Generic, 'low rank plus sparse structure' whose type is OtherScientificTerm, 'side information' whose type is OtherScientificTerm, 'information' whose type is Generic, 'robust PCA' whose type is Method, 'robust PCA' whose type is Method, 'side information' whose type is OtherScientificTerm, 'prior structure' whose type is OtherScientificTerm, 'features of entities' whose type is OtherScientificTerm, 'recovery' whose type is Task, 'convex problem' whose type is Task, 'side information' whose type is OtherScientificTerm, 'robust PCA' whose type is Method, 'low rank matrix' whose type is OtherScientificTerm, 'method' whose type is Generic, 'low rank matrices' whose type is OtherScientificTerm, 'robust PCA' whose type is Method, 'method' whose type is Generic, 'features' whose type is OtherScientificTerm, 'robust PCA' whose type is Method, 'noisy image classification' whose type is Task, 'method' whose type is Generic, 'side information' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We directly investigate a subject of much recent debate: do  word sense disambigation models  help  statistical machine translation   quality  ? We present empirical results casting doubt on this common, but unproved, assumption. Using a state-of-the-art  Chinese word sense disambiguation model  to choose  translation candidates  for a typical  IBM statistical MT system  , we find that  word sense disambiguation  does not yield significantly better  translation quality  than the  statistical machine translation system  alone.  Error analysis  suggests several key factors behind this surprising finding, including inherent limitations of current  statistical MT architectures  . ", "output": "The abstract text contains these key words below: 'word sense disambigation models' whose type is Method, 'statistical machine translation   quality' whose type is Metric, 'Chinese word sense disambiguation model' whose type is Method, 'translation candidates' whose type is OtherScientificTerm, 'IBM statistical MT system' whose type is Method, 'word sense disambiguation' whose type is Method, 'translation quality' whose type is Metric, 'statistical machine translation system' whose type is Method, 'Error analysis' whose type is Method, 'statistical MT architectures' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Manual acquisition  of  semantic constraints  in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on  cooccurrence patterns  in a large  corpus . To a large extent, these statistics reflect  semantic constraints  and thus are used to disambiguate  anaphora references  and  syntactic ambiguities . The scheme was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve  references  of the  pronoun \"it\"  in  sentences  that were randomly selected from the  corpus . The results of the experiment show that in most of the cases the  cooccurrence statistics  indeed reflect the  semantic constraints  and thus provide a basis for a useful  disambiguation tool .\n", "output": "The abstract text contains these key words below: 'Manual acquisition  of  semantic constraints' whose type is Task, 'cooccurrence patterns' whose type is OtherScientificTerm, 'semantic constraints' whose type is OtherScientificTerm, 'anaphora references' whose type is OtherScientificTerm, 'syntactic ambiguities' whose type is OtherScientificTerm, 'linguistic tools' whose type is Method, 'cooccurrence statistics' whose type is OtherScientificTerm, 'semantic constraints' whose type is OtherScientificTerm, 'disambiguation tool' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n  Graph unification  remains the most expensive part of  unification-based grammar parsing  . We focus on one speed-up element in the design of  unification algorithms  : avoidance of  copying  of  unmodified subgraphs  . We propose a method of attaining such a design through a method of  structure-sharing  which avoids  log(d) overheads  often associated with  structure-sharing of graphs  without any use of costly  dependency pointers  . The proposed scheme eliminates  redundant copying  while maintaining the  quasi-destructive scheme's ability  to avoid  over copying  and  early copying  combined with its ability to handle  cyclic structures  without algorithmic additions. ", "output": "The abstract text contains these key words below: 'Graph unification' whose type is Task, 'unification-based grammar parsing' whose type is Task, 'speed-up element' whose type is Method, 'unification algorithms' whose type is Method, 'copying  of  unmodified subgraphs' whose type is OtherScientificTerm, 'unmodified subgraphs' whose type is OtherScientificTerm, 'method' whose type is Generic, 'structure-sharing' whose type is Method, 'log(d) overheads' whose type is OtherScientificTerm, 'structure-sharing of graphs' whose type is OtherScientificTerm, 'dependency pointers' whose type is Method, 'scheme' whose type is Generic, 'redundant copying' whose type is OtherScientificTerm, 'quasi-destructive scheme's ability' whose type is OtherScientificTerm, 'over copying' whose type is OtherScientificTerm, 'early copying' whose type is OtherScientificTerm, 'cyclic structures' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "An extension to the  GPSG grammatical formalism  is proposed, allowing  non-terminals  to consist of finite sequences of  category labels , and allowing  schematic variables  to range over such sequences. The extension is shown to be sufficient to provide a strongly adequate  grammar  for  crossed serial dependencies , as found in e.g.  Dutch subordinate clauses . The structures induced for such  constructions  are argued to be more appropriate to data involving  conjunction  than some previous proposals have been. The extension is shown to be parseable by a simple extension to an existing  parsing method  for  GPSG . ", "output": "The abstract text contains these key words below: 'extension' whose type is Generic, 'GPSG grammatical formalism' whose type is Method, 'non-terminals' whose type is OtherScientificTerm, 'schematic variables' whose type is OtherScientificTerm, 'extension' whose type is Generic, 'grammar' whose type is Method, 'crossed serial dependencies' whose type is OtherScientificTerm, 'Dutch subordinate clauses' whose type is OtherScientificTerm, 'constructions' whose type is Task, 'extension' whose type is Generic, 'extension' whose type is Generic, 'parsing method' whose type is Method, 'GPSG' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents a research on the Czech talking head system. It gives an overview of methods used for visual speech animation, parameterization of a human face and a tongue, necessary data sources and a synthesis method. A 3D animation model is used for a pseudo-muscular animation schema to create such animation of visual speech which is usable for a lipreading. An extension of animation schema is presented to reach more precise deformations mainly in a lip area. Furthermore , a problem of forming articulatory trajectories is formulated to solve labial coarticulation effects. It is used for the synthesis method based on a selection of articulatory targets and interpolation technique.", "output": "The abstract text contains these key words below: 'Czech talking head system' whose type is Task, 'methods' whose type is Generic, 'visual speech animation' whose type is Task, 'synthesis method' whose type is Method, '3D animation model' whose type is Method, 'pseudo-muscular animation schema' whose type is Method, 'animation of visual speech' whose type is Task, 'lipreading' whose type is Task, 'animation schema' whose type is Method, 'forming articulatory trajectories' whose type is Task, 'labial coarticulation effects' whose type is OtherScientificTerm, 'It' whose type is Generic, 'synthesis method' whose type is Method, 'selection of articulatory targets' whose type is OtherScientificTerm, 'interpolation technique' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper I will argue for a  model of grammatical processing  that is based on  uniform processing  and  knowledge sources . The main  feature  of this model is to view  parsing  and  generation  as two strongly interleaved tasks performed by a single  parametrized deduction  process. It will be shown that this view supports flexible and efficient  natural language processing .\n", "output": "The abstract text contains these key words below: 'model of grammatical processing' whose type is Method, 'uniform processing' whose type is Method, 'knowledge sources' whose type is Material, 'model' whose type is Generic, 'parsing' whose type is Task, 'generation' whose type is Task, 'tasks' whose type is Generic, 'parametrized deduction  process' whose type is Method, 'natural language processing' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e derive a convex optimization problem for the task of segmenting sequential data, which explicitly treats presence of outliers. We describe two algorithms for solving this problem, one exact and one a top-down novel approach , and we derive a consistency results for the case of two segments and no outliers. Robustness to outliers is evaluated on two real-world tasks related to speech segmentation. Our algorithms outperform baseline seg-mentation algorithms.", "output": "The abstract text contains these key words below: 'convex optimization problem' whose type is Task, 'segmenting sequential data' whose type is Task, 'outliers' whose type is OtherScientificTerm, 'algorithms' whose type is Generic, 'problem' whose type is Generic, 'Robustness' whose type is Metric, 'outliers' whose type is OtherScientificTerm, 'real-world tasks' whose type is Task, 'speech segmentation' whose type is Task, 'algorithms' whose type is Generic, 'baseline seg-mentation algorithms' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " new algorithm is proposed for novel view generation in one-to-one teleconferencing applications. Given the video streams acquired by two cameras placed on either side of a computer monitor , the proposed algorithm synthesises images from a virtual camera in arbitrary position (typically located within the monitor) to facilitate eye contact. Our technique is based on an improved, dynamic-programming, stereo algorithm for efficient novel-view generation. The two main contributions of this paper are: i) a new type of three-plane graph for dense-stereo dynamic-programming, that encourages correct occlusion labeling; ii) a compact geometric derivation for novel-view synthesis by direct projection of the minimum-cost surface. Furthermore, this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts (flicker); and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space. Examples are given that demonstrate the robustness of the new algorithm to spatial and temporal artefacts for long stereo video streams. These include demonstrations of synthesis of cyclopean views of extended conversational sequences. We further demonstrate synthesis from a freely translating virtual camera.", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'novel view generation' whose type is Task, 'one-to-one teleconferencing applications' whose type is Task, 'video streams' whose type is Material, 'cameras' whose type is OtherScientificTerm, 'computer monitor' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'images' whose type is Material, 'virtual camera' whose type is OtherScientificTerm, 'arbitrary position' whose type is OtherScientificTerm, 'eye contact' whose type is OtherScientificTerm, 'technique' whose type is Generic, 'dynamic-programming, stereo algorithm' whose type is Method, 'novel-view generation' whose type is Task, 'three-plane graph' whose type is Method, 'dense-stereo dynamic-programming' whose type is Method, 'occlusion labeling' whose type is Task, 'compact geometric derivation' whose type is OtherScientificTerm, 'novel-view synthesis' whose type is Task, 'direct projection of the minimum-cost surface' whose type is Method, 'algorithm' whose type is Generic, 'temporal maintenance of a background model' whose type is Task, 'rendering of occlusions' whose type is Task, 'temporal artefacts (flicker)' whose type is OtherScientificTerm, 'cost aggregation algorithm' whose type is Method, 'three-dimensional matching cost space' whose type is OtherScientificTerm, 'robustness' whose type is Metric, 'algorithm' whose type is Generic, 'spatial and temporal artefacts' whose type is OtherScientificTerm, 'long stereo video streams' whose type is Material, 'synthesis of cyclopean views of extended conversational sequences' whose type is Task, 'cyclopean views' whose type is OtherScientificTerm, 'extended conversational sequences' whose type is Material, 'synthesis' whose type is Task, 'translating virtual camera' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Pipelined Natural Language Generation (NLG) systems  have grown increasingly complex as  architectural modules  were added to support  language functionalities  such as  referring expressions ,  lexical choice , and  revision . This has given rise to discussions about the relative placement of these new  modules  in the overall  architecture . Recent work on another aspect of  multi-paragraph text ,  discourse markers , indicates it is time to consider where a  discourse marker insertion algorithm  fits in. We present examples which suggest that in a  pipelined NLG architecture , the best approach is to strongly tie it to a  revision component . Finally, we evaluate the approach in a working  multi-page system .\n", "output": "The abstract text contains these key words below: 'Pipelined Natural Language Generation (NLG) systems' whose type is Method, 'architectural modules' whose type is Method, 'language functionalities' whose type is OtherScientificTerm, 'referring expressions' whose type is OtherScientificTerm, 'lexical choice' whose type is OtherScientificTerm, 'revision' whose type is OtherScientificTerm, 'modules' whose type is Generic, 'overall  architecture' whose type is Generic, 'multi-paragraph text' whose type is Material, 'discourse markers' whose type is OtherScientificTerm, 'discourse marker insertion algorithm' whose type is Method, 'pipelined NLG architecture' whose type is Method, 'approach' whose type is Generic, 'it' whose type is Generic, 'revision component' whose type is Method, 'approach' whose type is Generic, 'multi-page system' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they \" attract \" people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or \" repel \" people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as \" dark matter \" , emanating \" dark energy \" that affects people's trajectories in the video. To detect \" dark matter \" and infer their \" dark energy \" field, we extend the La-grangian mechanics. People are treated as particle-agents with latent intents to approach \" dark matter \" and thus satisfy their needs, where their motions are subject to a composite \" dark energy \" field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended \" dark matter \" while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people's trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people's trajectories in unobserved parts of the video footage.", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'localizing functional objects' whose type is Task, 'surveillance videos' whose type is Material, 'domain knowledge' whose type is OtherScientificTerm, 'semantic object classes' whose type is OtherScientificTerm, 'Functional objects' whose type is OtherScientificTerm, 'discriminative appearance and shape' whose type is OtherScientificTerm, 'they' whose type is Generic, 'functional objects' whose type is OtherScientificTerm, 'La-grangian mechanics' whose type is Method, 'functional objects' whose type is OtherScientificTerm, 'Bayesian framework' whose type is Method, 'people's trajectories and intents' whose type is OtherScientificTerm, 'constraint map of the scene' whose type is OtherScientificTerm, 'locations of functional objects' whose type is OtherScientificTerm, 'data-driven Markov Chain Monte Carlo (MCMC) process' whose type is Method, 'inference' whose type is Task, 'videos of public squares and courtyards' whose type is Material, 'localizing functional objects' whose type is Task, 'predicting people's trajectories' whose type is Task, 'video footage' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Methods developed for  spelling correction  for  languages  like  English  (see the review by Kukich (Kukich, 1992)) are not readily applicable to  agglutinative languages . This poster presents an approach to  spelling correction  in  agglutinative languages  that is based on  two-level morphology  and a  dynamic-programming based search algorithm . After an overview of our approach, we present results from experiments with  spelling correction  in  Turkish .\n", "output": "The abstract text contains these key words below: 'Methods' whose type is Generic, 'spelling correction' whose type is Task, 'languages' whose type is OtherScientificTerm, 'English' whose type is OtherScientificTerm, 'agglutinative languages' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'spelling correction' whose type is Task, 'agglutinative languages' whose type is OtherScientificTerm, 'two-level morphology' whose type is OtherScientificTerm, 'dynamic-programming based search algorithm' whose type is Method, 'approach' whose type is Generic, 'spelling correction' whose type is Task, 'Turkish' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "any practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions. For example, nucleotides in a DNA sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions. In all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the DNA strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic. These dependencies are not naturally captured by the typical Dirichlet-multinomial formulation. Here, we leverage a logistic stick-breaking representation and recent innovations in P\u00f3lya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods, enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead.", "output": "The abstract text contains these key words below: 'modeling problems' whose type is Task, 'discrete data' whose type is Material, 'multinomial or categorical distributions' whose type is Method, 'nucleotides in a DNA sequence' whose type is Material, 'text documents' whose type is Material, 'multinomial distributions' whose type is Method, 'nucleotide' whose type is OtherScientificTerm, 'DNA strand' whose type is OtherScientificTerm, 'preceding nucleotides' whose type is OtherScientificTerm, 'Dirichlet-multinomial formulation' whose type is Method, 'logistic stick-breaking representation' whose type is Method, 'P\u00f3lya-gamma augmentation' whose type is Task, 'multinomial distribution' whose type is Method, 'latent variables' whose type is OtherScientificTerm, 'jointly Gaussian likelihoods' whose type is OtherScientificTerm, 'Bayesian inference techniques' whose type is Method, 'Gaussian models' whose type is Method, 'minimal overhead' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we present a  fully automated extraction system , named  IntEx , to identify  gene and protein interactions  in  biomedical text . Our approach is based on first splitting  complex sentences  into  simple clausal structures  made up of  syntactic roles . Then, tagging  biological entities  with the help of  biomedical and linguistic ontologies . Finally, extracting  complete interactions  by analyzing the matching contents of  syntactic roles  and their linguistically significant combinations. Our  extraction system  handles  complex sentences  and extracts  multiple and nested interactions  specified in a  sentence . Experimental evaluations with two other state of the art  extraction systems  indicate that the  IntEx system  achieves better  performance  without the labor intensive  pattern engineering requirement .\n", "output": "The abstract text contains these key words below: 'fully automated extraction system' whose type is Method, 'IntEx' whose type is Method, 'gene and protein interactions' whose type is Task, 'biomedical text' whose type is Material, 'approach' whose type is Generic, 'syntactic roles' whose type is OtherScientificTerm, 'biological entities' whose type is OtherScientificTerm, 'biomedical and linguistic ontologies' whose type is OtherScientificTerm, 'syntactic roles' whose type is OtherScientificTerm, 'extraction system' whose type is Method, 'multiple and nested interactions' whose type is OtherScientificTerm, 'extraction systems' whose type is Method, 'IntEx system' whose type is Method, 'pattern engineering requirement' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present the first known  empirical test  of an increasingly common speculative claim, by evaluating a representative  Chinese-to-English SMT model  directly on  word sense disambiguation performance  , using standard  WSD evaluation methodology  and  datasets  from the  Senseval-3 Chinese lexical sample task  . Much effort has been put in designing and evaluating dedicated  word sense disambiguation (WSD) models  , in particular with the  Senseval  series of workshops. At the same time, the recent improvements in the  BLEU scores  of  statistical machine translation (SMT)  suggests that  SMT models  are good at predicting the right  translation  of the  words  in  source language sentences  . Surprisingly however, the  WSD   accuracy  of  SMT models  has never been evaluated and compared with that of the dedicated  WSD models  . We present controlled experiments showing the  WSD   accuracy  of current typical  SMT models  to be significantly lower than that of all the dedicated  WSD models  considered. This tends to support the view that despite recent speculative claims to the contrary, current  SMT models  do have limitations in comparison with dedicated  WSD models  , and that  SMT  should benefit from the better predictions made by the  WSD models  . ", "output": "The abstract text contains these key words below: 'Chinese-to-English SMT model' whose type is Method, 'word sense disambiguation' whose type is Task, 'WSD evaluation methodology' whose type is Method, 'Senseval-3 Chinese lexical sample task' whose type is Material, 'dedicated  word sense disambiguation (WSD) models' whose type is Method, 'Senseval  series of workshops' whose type is Material, 'BLEU scores' whose type is Metric, 'statistical machine translation (SMT)' whose type is Task, 'SMT models' whose type is Method, 'translation' whose type is Task, 'WSD   accuracy' whose type is Metric, 'SMT models' whose type is Method, 'that' whose type is Generic, 'dedicated  WSD models' whose type is Method, 'WSD   accuracy' whose type is Metric, 'SMT models' whose type is Method, 'that' whose type is Generic, 'dedicated  WSD models' whose type is Method, 'SMT models' whose type is Method, 'dedicated  WSD models' whose type is Method, 'SMT' whose type is Method, 'WSD models' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e investigate the problem of learning to predict moves in the board game of Go from game records of expert players. In particular, we obtain a probability distribution over legal moves for professional play in a given position. This distribution has numerous applications in computer Go, including serving as an efficient stand-alone Go player. It would also be effective as a move selector and move sorter for game tree search and as a training tool for Go players. Our method has two major components: a) a pattern extraction scheme for efficiently harvesting patterns of given size and shape from expert game records and b) a Bayesian learning algorithm (in two variants) that learns a distribution over the values of a move given a board position based on the local pattern context. The system is trained on 181,000 expert games and shows excellent prediction performance as indicated by its ability to perfectly predict the moves made by professional Go players in 34% of test positions.", "output": "The abstract text contains these key words below: 'board game of Go' whose type is Task, 'game records of expert players' whose type is Material, 'probability distribution' whose type is OtherScientificTerm, 'distribution' whose type is Generic, 'computer Go' whose type is OtherScientificTerm, 'stand-alone Go player' whose type is OtherScientificTerm, 'It' whose type is Generic, 'move selector' whose type is Method, 'move sorter' whose type is Method, 'game tree search' whose type is Method, 'training tool' whose type is Task, 'Go players' whose type is OtherScientificTerm, 'method' whose type is Generic, 'pattern extraction scheme' whose type is Method, 'Bayesian learning algorithm' whose type is Method, 'local pattern context' whose type is OtherScientificTerm, 'system' whose type is Generic, 'expert games' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we reported experiments of  unsupervised automatic acquisition  of  Italian and English verb subcategorization frames (SCFs)  from  general and domain corpora . The proposed technique operates on  syntactically shallow-parsed corpora  on the basis of a limited number of  search heuristics  not relying on any previous  lexico-syntactic knowledge  about  SCFs . Although preliminary, reported results are in line with  state-of-the-art lexical acquisition systems . The issue of whether  verbs  sharing similar  SCFs distributions  happen to share  similar semantic properties  as well was also explored by clustering  verbs  that share  frames  with the same  distribution  using the  Minimum Description Length Principle (MDL) . First experiments in this direction were carried out on  Italian verbs  with encouraging results.\n", "output": "The abstract text contains these key words below: 'unsupervised automatic acquisition  of  Italian and English verb subcategorization frames (SCFs)' whose type is Task, 'Italian and English verb subcategorization frames (SCFs)' whose type is OtherScientificTerm, 'general and domain corpora' whose type is Material, 'technique' whose type is Generic, 'syntactically shallow-parsed corpora' whose type is Material, 'search heuristics' whose type is Method, 'lexico-syntactic knowledge' whose type is OtherScientificTerm, 'SCFs' whose type is OtherScientificTerm, 'lexical acquisition systems' whose type is Method, 'SCFs distributions' whose type is OtherScientificTerm, 'similar semantic properties' whose type is OtherScientificTerm, 'Minimum Description Length Principle (MDL)' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Despite much recent progress on accurate  semantic role labeling , previous work has largely used  independent classifiers , possibly combined with separate  label sequence models  via  Viterbi decoding . This stands in stark contrast to the linguistic observation that a  core argument frame  is a joint structure, with strong  dependencies  between  arguments . We show how to build a  joint model  of  argument frames , incorporating novel  features  that model these interactions into  discriminative log-linear models . This system achieves an  error reduction  of 22% on all  arguments  and 32% on  core arguments  over a state-of-the art independent  classifier  for  gold-standard parse trees  on  PropBank .\n", "output": "The abstract text contains these key words below: 'semantic role labeling' whose type is Task, 'independent classifiers' whose type is Method, 'label sequence models' whose type is Method, 'Viterbi decoding' whose type is Method, 'core argument frame' whose type is OtherScientificTerm, 'joint model  of  argument frames' whose type is Method, 'features' whose type is OtherScientificTerm, 'discriminative log-linear models' whose type is Method, 'system' whose type is Generic, 'error reduction' whose type is Metric, 'independent  classifier' whose type is Method, 'gold-standard parse trees' whose type is OtherScientificTerm, 'PropBank' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n We present an efficient algorithm for  chart-based phrase structure parsing  of  natural language  that is tailored to the problem of extracting specific information from  unrestricted texts  where many of the  words  are unknown and much of the  text  is irrelevant to the task. The  parser  gains algorithmic efficiency through a  reduction  of its  search space  . As each new  edge  is added to the  chart  , the algorithm checks only the topmost of the  edges  adjacent to it, rather than all such  edges  as in conventional treatments. The resulting  spanning edges  are insured to be the correct ones by carefully controlling the order in which  edges  are introduced so that every final  constituent  covers the longest possible  span  . This is facilitated through the use of  phrase boundary heuristics  based on the placement of  function words  , and by  heuristic rules  that permit certain kinds of  phrases  to be deduced despite the presence of  unknown words  . A further  reduction in the search space  is achieved by using  semantic  rather than  syntactic categories  on the  terminal and non-terminal edges  , thereby reducing the amount of  ambiguity  and thus the number of  edges  , since only  edges  with a valid  semantic  interpretation are ever introduced. ", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'chart-based phrase structure parsing' whose type is Task, 'natural language' whose type is Material, 'parser' whose type is Method, 'search space' whose type is OtherScientificTerm, 'edge' whose type is OtherScientificTerm, 'chart' whose type is OtherScientificTerm, 'edges' whose type is OtherScientificTerm, 'edges' whose type is OtherScientificTerm, 'spanning edges' whose type is OtherScientificTerm, 'edges' whose type is OtherScientificTerm, 'phrase boundary heuristics' whose type is Method, 'function words' whose type is OtherScientificTerm, 'heuristic rules' whose type is Method, 'unknown words' whose type is OtherScientificTerm, 'reduction in the search space' whose type is OtherScientificTerm, 'semantic' whose type is OtherScientificTerm, 'syntactic categories' whose type is OtherScientificTerm, 'terminal and non-terminal edges' whose type is OtherScientificTerm, 'edges' whose type is OtherScientificTerm, 'edges' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe a method for identifying systematic  patterns  in  translation data  using  part-of-speech tag sequences  . We incorporate this analysis into a  diagnostic tool  intended for  developers  of  machine translation systems  , and demonstrate how our application can be used by  developers  to explore  patterns  in  machine translation output  . ", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'systematic  patterns  in  translation data' whose type is OtherScientificTerm, 'part-of-speech tag sequences' whose type is Material, 'analysis' whose type is Generic, 'diagnostic tool' whose type is Method, 'machine translation systems' whose type is Method, 'application' whose type is Generic, 'patterns  in  machine translation output' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we propose a new approach to generate oriented object proposals (OOPs) to reduce the detection error caused by various orientations of the object. To this end, we propose to efficiently locate object regions according to pixelwise object probability, rather than measuring the objectness from a set of sampled windows. We formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different shapes (i.e., sizes and orientations) can be produced by locating the local maximum likelihoods. The new approach has three main advantages. First, it helps the object detector handle objects of different orientations. Second, as the shapes of the proposals may vary to fit the objects, the resulting proposals are tighter than the sampling windows with fixed sizes. Third, it avoids massive window sampling, and thereby reducing the number of proposals while maintaining a high recall. Experiments on the PASCAL VOC 2007 dataset show that the proposed OOP outperforms the state-of-the-art fast methods. Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios. Generating OOPs is very fast and takes only 0.5s per image.", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'oriented object proposals (OOPs)' whose type is Task, 'detection error' whose type is Metric, 'orientations of the object' whose type is OtherScientificTerm, 'object regions' whose type is OtherScientificTerm, 'pixelwise object probability' whose type is OtherScientificTerm, 'objectness' whose type is OtherScientificTerm, 'proposal generation problem' whose type is Task, 'generative proba-bilistic model' whose type is Method, 'object proposals' whose type is OtherScientificTerm, 'shapes' whose type is OtherScientificTerm, 'sizes' whose type is OtherScientificTerm, 'orientations' whose type is OtherScientificTerm, 'local maximum likelihoods' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'it' whose type is Generic, 'object detector' whose type is Method, 'orientations' whose type is OtherScientificTerm, 'shapes of the proposals' whose type is OtherScientificTerm, 'sampling windows' whose type is OtherScientificTerm, 'it' whose type is Generic, 'massive window sampling' whose type is Method, 'number of proposals' whose type is OtherScientificTerm, 'recall' whose type is Metric, 'PASCAL VOC 2007 dataset' whose type is Material, 'OOP' whose type is Method, 'state-of-the-art fast methods' whose type is Generic, 'rotation invariant property' whose type is OtherScientificTerm, 'class-specific object detector' whose type is Method, 'proposal generation methods' whose type is Method, 'object rotation scenarios' whose type is Material, 'general scenarios' whose type is Material, 'OOPs' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose an exact, general and efficient coarse-to-fine energy minimization strategy for semantic video segmenta-tion. Our strategy is based on a hierarchical abstraction of the supervoxel graph that allows us to minimize an energy defined at the finest level of the hierarchy by minimizing a series of simpler energies defined over coarser graphs. The strategy is exact, i.e., it produces the same solution as minimizing over the finest graph. It is general, i.e., it can be used to minimize any energy function (e.g., unary, pairwise, and higher-order terms) with any existing energy minimization algorithm (e.g., graph cuts and belief propagation). It also gives significant speedups in inference for several datasets with varying degrees of spatio-temporal continuity. We also discuss the strengths and weaknesses of our strategy relative to existing hierarchical approaches, and the kinds of image and video data that provide the best speedups.", "output": "The abstract text contains these key words below: 'coarse-to-fine energy minimization strategy' whose type is Method, 'semantic video segmenta-tion' whose type is Task, 'strategy' whose type is Generic, 'hierarchical abstraction of the supervoxel graph' whose type is Task, 'hierarchy' whose type is OtherScientificTerm, 'coarser graphs' whose type is OtherScientificTerm, 'strategy' whose type is Generic, 'it' whose type is Generic, 'finest graph' whose type is OtherScientificTerm, 'It' whose type is Generic, 'it' whose type is Generic, 'energy function' whose type is OtherScientificTerm, 'energy minimization algorithm' whose type is Method, 'graph cuts' whose type is Method, 'belief propagation' whose type is Method, 'It' whose type is Generic, 'inference' whose type is Task, 'datasets' whose type is Generic, 'spatio-temporal continuity' whose type is OtherScientificTerm, 'strategy' whose type is Generic, 'hierarchical approaches' whose type is Method, 'image and video data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e address the problem of estimating location information of an image using principles from automated representation learning. We pursue a hierarchical sparse coding approach that learns features useful in discriminating images across locations, by initializing it with a geometric prior corresponding to transformations between image appearance space and their corresponding location grouping space using the notion of parallel transport on manifolds. We then extend this approach to account for the availability of heterogeneous data modalities such as geo-tags and videos pertaining to different locations, and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations. We evaluate our approach on several standard datasets such as im2gps, San Francisco and MediaEval2010, and obtain state-of-the-art results.", "output": "The abstract text contains these key words below: 'estimating location information' whose type is Task, 'image' whose type is Material, 'automated representation learning' whose type is Method, 'hierarchical sparse coding approach' whose type is Method, 'features' whose type is OtherScientificTerm, 'it' whose type is Generic, 'geometric prior' whose type is OtherScientificTerm, 'image appearance space' whose type is OtherScientificTerm, 'location grouping space' whose type is OtherScientificTerm, 'parallel transport on manifolds' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'heterogeneous data modalities' whose type is OtherScientificTerm, 'geo-tags' whose type is OtherScientificTerm, 'videos' whose type is OtherScientificTerm, 'transferring knowledge' whose type is Task, 'grouping of data' whose type is Task, 'approach' whose type is Method, 'datasets' whose type is Generic, 'im2gps' whose type is Material, 'San Francisco' whose type is Material, 'MediaEval2010' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we introduce a  generative probabilistic optical character recognition (OCR) model  that describes an end-to-end process in the  noisy channel framework  , progressing from generation of  true text  through its transformation into the  noisy output  of an  OCR system  . The  model  is designed for use in  error correction  , with a focus on  post-processing  the  output  of black-box  OCR systems  in order to make it more useful for  NLP tasks  . We present an implementation of the  model  based on  finite-state models  , demonstrate the  model  's ability to significantly reduce  character and word error rate  , and provide evaluation results involving  automatic extraction  of  translation lexicons  from  printed text  . ", "output": "The abstract text contains these key words below: 'generative probabilistic optical character recognition (OCR) model' whose type is Method, 'noisy channel framework' whose type is Method, 'OCR system' whose type is OtherScientificTerm, 'model' whose type is Generic, 'error correction' whose type is Task, 'post-processing' whose type is Task, 'output  of black-box  OCR systems' whose type is OtherScientificTerm, 'OCR systems' whose type is OtherScientificTerm, 'it' whose type is Generic, 'NLP tasks' whose type is Task, 'model' whose type is Generic, 'finite-state models' whose type is Method, 'model' whose type is Generic, 'character and word error rate' whose type is Metric, 'automatic extraction  of  translation lexicons' whose type is Task, 'printed text' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " We give an analysis of  ellipsis resolution  in terms of a straightforward  discourse copying algorithm  that correctly predicts a wide range of phenomena. The treatment does not suffer from problems inherent in  identity-of-relations analyses . Furthermore, in contrast to the approach of Dalrymple et al. [1991], the treatment directly encodes the intuitive distinction between  full NPs  and the  referential elements  that corefer with them through what we term  role linking . The correct  predictions  for several problematic examples of  ellipsis  naturally result. Finally, the analysis extends directly to other  discourse copying phenomena . ", "output": "The abstract text contains these key words below: 'analysis of  ellipsis resolution' whose type is Task, 'discourse copying algorithm' whose type is Method, 'treatment' whose type is Generic, 'identity-of-relations analyses' whose type is Task, 'treatment' whose type is Generic, 'full NPs' whose type is OtherScientificTerm, 'referential elements' whose type is OtherScientificTerm, 'role linking' whose type is Task, 'ellipsis' whose type is OtherScientificTerm, 'analysis' whose type is Generic, 'discourse copying phenomena' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper describes our work on classification of outdoor scenes. First, images are partitioned into regions using one-class classification and patch-based clustering algorithms where one-class classifiers model the regions with relatively uniform color and texture properties, and clustering of patches aims to detect structures in the remaining regions. Next, the resulting regions are clustered to obtain a codebook of region types, and two models are constructed for scene representation: a \" bag of individual regions \" representation where each region is regarded separately , and a \" bag of region pairs \" representation where regions with particular spatial relationships are considered together. Given these representations, scene classification is done using Bayesian classifiers. We also propose a novel region selection algorithm that identifies region types that are frequently found in a particular class of scenes but rarely exist in other classes, and also consistently occur together in the same class of scenes. Experiments on the LabelMe data set showed that the proposed models significantly out-perform a baseline global feature-based approach.", "output": "The abstract text contains these key words below: 'classification of outdoor scenes' whose type is Task, 'one-class classification' whose type is Method, 'patch-based clustering algorithms' whose type is Method, 'one-class classifiers' whose type is Method, 'uniform color and texture properties' whose type is OtherScientificTerm, 'clustering of patches' whose type is Method, 'codebook of region types' whose type is OtherScientificTerm, 'models' whose type is Generic, 'scene representation' whose type is Method, 'spatial relationships' whose type is OtherScientificTerm, 'scene classification' whose type is Task, 'Bayesian classifiers' whose type is Method, 'region selection algorithm' whose type is Method, 'LabelMe data set' whose type is Material, 'models' whose type is Generic, 'baseline global feature-based approach' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "earning video representation is not a trivial task, as video is an information-intensive media where each frame does not exist independently. Locally, a video frame is visually and semantically similar with its adjacent frames. Holistically, a video has its inherent structure\u2014the correlations among video frames. For example, even the frames far from each other may also hold similar semantics. Such context information is therefore important to characterize the intrinsic representation of a video frame. In this paper, we present a novel approach to learn the deep video representation by exploring both local and holistic contexts. Specifically, we propose a triplet sampling mechanism to encode the local temporal relationship of adjacent frames based on their deep representations. In addition, we incorporate the graph structure of the video, as a priori, to holistically preserve the inherent correlations among video frames. Our approach is fully unsupervised and trained in an end-to-end deep convolutional neu-ral network architecture. By extensive experiments, we show that our learned representation can significantly boost several video recognition tasks (retrieval , classification, and highlight detection) over traditional video representations.", "output": "The abstract text contains these key words below: 'Learning video representation' whose type is Task, 'semantics' whose type is OtherScientificTerm, 'context information' whose type is OtherScientificTerm, 'intrinsic representation of a video frame' whose type is Method, 'approach' whose type is Generic, 'deep video representation' whose type is Method, 'local and holistic contexts' whose type is OtherScientificTerm, 'triplet sampling mechanism' whose type is Method, 'local temporal relationship of adjacent frames' whose type is OtherScientificTerm, 'deep representations' whose type is Method, 'graph structure of the video' whose type is OtherScientificTerm, 'priori' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'end-to-end deep convolutional neu-ral network architecture' whose type is Method, 'learned representation' whose type is Generic, 'video recognition tasks' whose type is Task, 'retrieval' whose type is Task, 'classification' whose type is Task, 'highlight detection' whose type is Task, 'video representations' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n Computer programs so far have not fared well in  modeling language acquisition  . For one thing,  learning methodology  applicable in  general domains  does not readily lend itself in the  linguistic domain  . For another,  linguistic representation  used by  language processing systems  is not geared to  learning  . We introduced a new  linguistic representation  , the  Dynamic Hierarchical Phrasal Lexicon (DHPL)  [Zernik88], to facilitate  language acquisition  . From this, a  language learning model  was implemented in the program  RINA  , which enhances its own  lexical hierarchy  by processing examples in context. We identified two tasks: First, how  linguistic concepts  are acquired from  training examples  and organized in a  hierarchy  ; this task was discussed in previous papers [Zernik87]. Second, we show in this paper how a  lexical hierarchy  is used in predicting new  linguistic concepts  . Thus, a  program  does not stall even in the presence of a  lexical unknown  , and a  hypothesis  can be produced for covering that  lexical gap  . ", "output": "The abstract text contains these key words below: 'Computer programs' whose type is Generic, 'language acquisition' whose type is Task, 'learning methodology' whose type is Method, 'general domains' whose type is Material, 'linguistic domain' whose type is Material, 'linguistic representation' whose type is Method, 'language processing systems' whose type is Method, 'linguistic representation' whose type is Method, 'Dynamic Hierarchical Phrasal Lexicon (DHPL)' whose type is OtherScientificTerm, 'language acquisition' whose type is Task, 'language learning model' whose type is Method, 'RINA' whose type is Method, 'lexical hierarchy' whose type is OtherScientificTerm, 'linguistic concepts' whose type is OtherScientificTerm, 'hierarchy' whose type is Generic, 'lexical hierarchy' whose type is OtherScientificTerm, 'linguistic concepts' whose type is OtherScientificTerm, 'program' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "egression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives: (1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively. (2) Rather than learning from only labelled data, the abundant unlabelled data are exploited. (3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.", "output": "The abstract text contains these key words below: 'Regression-based techniques' whose type is Method, 'people counting in crowded scenes' whose type is Task, 'techniques' whose type is Generic, 'data annotation' whose type is Task, 'model training' whose type is Task, 'informative frames' whose type is OtherScientificTerm, 'annotation' whose type is Task, 'labelled data' whose type is Generic, 'abundant unlabelled data' whose type is Material, 'data annotation' whose type is Task, 'unified active and semi-supervised regression framework' whose type is Method, 'transfer learning' whose type is Method, 'geometric structure of crowd patterns' whose type is OtherScientificTerm, 'manifold analysis' whose type is Method, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ithin the EU Network of Excellence PASCAL, a challenge was organized to design a statistical machine learning algorithm that segments words into the smallest meaning-bearing units of language , morphemes. Ideally, these are basic vocabulary units suitable for different tasks, such as speech and text understanding, machine translation, information retrieval, and statistical language modeling. Twelve research groups participated in the challenge and had submitted segmentation results obtained by their algorithms. In this paper, we evaluate the application of these segmen-tation algorithms to large vocabulary speech recognition using statistical n-gram language models based on the proposed word segments instead of entire words. Experiments were done for two ag-glutinative and morphologically rich languages: Finnish and Turk-ish. We also investigate combining various segmentations to improve the performance of the recognizer.", "output": "The abstract text contains these key words below: 'statistical machine learning algorithm' whose type is Method, 'smallest meaning-bearing units of language' whose type is OtherScientificTerm, 'morphemes' whose type is OtherScientificTerm, 'these' whose type is Generic, 'tasks' whose type is Generic, 'speech and text understanding' whose type is Task, 'machine translation' whose type is Task, 'information retrieval' whose type is Task, 'statistical language modeling' whose type is Task, 'segmen-tation algorithms' whose type is Method, 'large vocabulary speech recognition' whose type is Task, 'statistical n-gram language models' whose type is Method, 'ag-glutinative and morphologically rich languages' whose type is Material, 'Finnish' whose type is Material, 'Turk-ish' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n We have developed a  computational model  of the process of describing the layout of an apartment or house, a much-studied  discourse task  first characterized linguistically by Linde (1974). The  model  is embodied in a program,  APT  , that can reproduce segments of actual tape-recorded descriptions, using  organizational and discourse strategies  derived through analysis of our  corpus  . ", "output": "The abstract text contains these key words below: 'computational model' whose type is Generic, 'discourse task' whose type is Task, 'model' whose type is Generic, 'program' whose type is Generic, 'APT' whose type is Method, 'organizational and discourse strategies' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present the first application of the  head-driven statistical parsing model  of Collins (1999) as a  simultaneous language model  and  parser  for  large-vocabulary speech recognition . The model is adapted to an  online left to right chart-parser  for  word lattices , integrating acoustic, n-gram, and parser probabilities. The  parser  uses  structural and lexical dependencies  not considered by  n-gram models , conditioning recognition on more linguistically-grounded relationships. Experiments on the  Wall Street Journal treebank  and lattice corpora show  word error rates  competitive with the  standard n-gram language model  while extracting additional  structural information  useful for  speech understanding .\n", "output": "The abstract text contains these key words below: 'head-driven statistical parsing model' whose type is Method, 'simultaneous language model' whose type is Method, 'parser' whose type is Method, 'large-vocabulary speech recognition' whose type is Task, 'model' whose type is Generic, 'online left to right chart-parser' whose type is Method, 'word lattices' whose type is OtherScientificTerm, 'acoustic, n-gram, and parser probabilities' whose type is OtherScientificTerm, 'parser' whose type is Method, 'structural and lexical dependencies' whose type is OtherScientificTerm, 'n-gram models' whose type is Method, 'Wall Street Journal treebank' whose type is Material, 'lattice corpora' whose type is Material, 'word error rates' whose type is Metric, 'n-gram language model' whose type is Method, 'structural information' whose type is OtherScientificTerm, 'speech understanding' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We apply a  decision tree based approach  to  pronoun resolution  in  spoken dialogue  . Our system deals with  pronouns  with  NP- and non-NP-antecedents  . We present a set of  features  designed for  pronoun resolution  in  spoken dialogue  and determine the most promising  features  . We evaluate the system on twenty  Switchboard dialogues  and show that it compares well to  Byron's (2002) manually tuned system  . ", "output": "The abstract text contains these key words below: 'decision tree based approach' whose type is Method, 'pronoun resolution' whose type is Task, 'spoken dialogue' whose type is Task, 'system' whose type is Generic, 'pronouns' whose type is OtherScientificTerm, 'NP- and non-NP-antecedents' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'pronoun resolution' whose type is Task, 'spoken dialogue' whose type is Task, 'features' whose type is OtherScientificTerm, 'system' whose type is Generic, 'Switchboard dialogues' whose type is Material, 'it' whose type is Generic, 'Byron's (2002) manually tuned system' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we present a  statistical profile  of the  Named Entity task , a specific  information extraction task  for which  corpora  in several  languages  are available. Using the  results  of the  statistical analysis , we propose an  algorithm  for  lower bound estimation  for  Named Entity corpora  and discuss the significance of the  cross-lingual comparisons  provided by the  analysis .\n", "output": "The abstract text contains these key words below: 'statistical profile' whose type is Generic, 'Named Entity task' whose type is Task, 'information extraction task' whose type is Task, 'statistical analysis' whose type is Method, 'algorithm' whose type is Generic, 'lower bound estimation' whose type is Method, 'Named Entity corpora' whose type is Material, 'analysis' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This report describes  Paul  , a  computer text generation system  designed to create  cohesive text  through the use of  lexical substitutions  . Specifically, this system is designed to deterministically choose between  pronominalization  ,  superordinate substitution  , and definite  noun phrase reiteration  . The system identifies a strength of  antecedence recovery  for each of the  lexical substitutions .\n", "output": "The abstract text contains these key words below: 'Paul' whose type is Method, 'computer text generation system' whose type is Method, 'cohesive text' whose type is Material, 'lexical substitutions' whose type is OtherScientificTerm, 'system' whose type is Generic, 'pronominalization' whose type is OtherScientificTerm, 'superordinate substitution' whose type is OtherScientificTerm, 'definite  noun phrase reiteration' whose type is OtherScientificTerm, 'system' whose type is Generic, 'antecedence recovery' whose type is Task, 'lexical substitutions' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Unification  is often the appropriate method for expressing  relations  between  representations  in the form of  feature structures ; however, there are circumstances in which a different approach is desirable. A  declarative formalism  is presented which permits direct  mappings  of one  feature structure  into another, and illustrative examples are given of its application to areas of current interest.\n", "output": "The abstract text contains these key words below: 'Unification' whose type is Method, 'method' whose type is Generic, 'relations  between  representations' whose type is Task, 'feature structures' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'declarative formalism' whose type is Method, 'direct  mappings  of one  feature structure  into another' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper presents necessary and sufficient conditions for the use of  demonstrative expressions  in  English  and discusses implications for current  discourse processing algorithms  . We examine a broad range of  texts  to show how the distribution of  demonstrative forms and functions  is  genre dependent  . This research is part of a larger study of  anaphoric expressions  , the results of which will be incorporated into a  natural language generation system  . ", "output": "The abstract text contains these key words below: 'demonstrative expressions' whose type is OtherScientificTerm, 'English' whose type is Material, 'implications' whose type is Generic, 'discourse processing algorithms' whose type is Method, 'demonstrative forms and functions' whose type is OtherScientificTerm, 'anaphoric expressions' whose type is OtherScientificTerm, 'natural language generation system' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper deals with the problem of generating the fundamental frequency (F0) contour of speech from a text input for text-to-speech synthesis. We have previously introduced a statistical model describing the generating process of speech F0 contours, based on the discrete-time version of the Fujisaki model. One remarkable feature of this model is that it has allowed us to derive an efficient algorithm based on powerful statistical methods for estimating the Fujisaki-model parameters from raw F0 contours. To associate a sequence of the Fujisaki-model parameters with a text input based on statistical learning, this paper proposes extending this model to a context-dependent one. We further propose a parameter training algorithm for the present model based on a decision tree-based context clustering.", "output": "The abstract text contains these key words below: 'fundamental frequency (F0) contour of speech' whose type is OtherScientificTerm, 'text input' whose type is Material, 'text-to-speech synthesis' whose type is Task, 'statistical model' whose type is Method, 'speech F0 contours' whose type is OtherScientificTerm, 'Fujisaki model' whose type is Method, 'remarkable feature' whose type is OtherScientificTerm, 'model' whose type is Generic, 'it' whose type is Generic, 'algorithm' whose type is Generic, 'statistical methods' whose type is Method, 'Fujisaki-model parameters' whose type is OtherScientificTerm, 'raw F0 contours' whose type is OtherScientificTerm, 'Fujisaki-model parameters' whose type is OtherScientificTerm, 'text input' whose type is Material, 'statistical learning' whose type is Method, 'model' whose type is Generic, 'parameter training algorithm' whose type is Method, 'model' whose type is Generic, 'decision tree-based context clustering' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The project presented here is a part of a long term research program aiming at a full  lexicon grammar for Polish (SyntLex) . The main of this project is  computer-assisted acquisition and morpho-syntactic description of verb-noun collocations  in  Polish . We present methodology and resources obtained in three main project phases which are:  dictionary-based acquisition  of  collocation lexicon , feasibility study for  corpus-based lexicon enlargement  phase,  corpus-based lexicon enlargement  and  collocation description . In this paper we focus on the results of the third phase. The presented here  corpus-based approach  permitted us to triple the size the  verb-noun collocation dictionary for Polish . In the paper we describe the  SyntLex Dictionary of Collocations  and announce some future research intended to be a separate project continuation.\n", "output": "The abstract text contains these key words below: 'lexicon grammar for Polish (SyntLex)' whose type is Method, 'computer-assisted acquisition and morpho-syntactic description of verb-noun collocations' whose type is Task, 'Polish' whose type is Material, 'phases' whose type is Generic, 'dictionary-based acquisition  of  collocation lexicon' whose type is Task, 'feasibility study' whose type is Task, 'corpus-based lexicon enlargement  phase' whose type is Task, 'corpus-based lexicon enlargement  and  collocation description' whose type is Task, 'corpus-based approach' whose type is Method, 'verb-noun collocation dictionary' whose type is Material, 'Polish' whose type is Material, 'SyntLex Dictionary of Collocations' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ctive shape models are a powerful and widely used tool to interpret complex image data. By building models of shape variation they enable search algorithms to use a pri-ori knowledge in an efficient and gainful way. However, due to the linearity of PCA, non-linearities like rotations or independently moving sub-parts in the data can deteriorate the resulting model considerably. Although non-linear extensions of active shape models have been proposed and application specific solutions have been used, they still need a certain amount of user interaction during model building. In this paper the task of build-ing/choosing optimal models is tackled in a more generic information theoretic fashion. In particular, we propose an algorithm based on the minimum description length principle to find an optimal subdivision of the data into sub-parts, each adequate for linear modeling. This results in an overall more compact model configuration. Which in turn leads to a better model in terms of modes of variations. The proposed method is evaluated on synthetic data, medical images and hand contours.", "output": "The abstract text contains these key words below: 'Active shape models' whose type is Method, 'complex image data' whose type is Material, 'models of shape variation' whose type is Method, 'search algorithms' whose type is Method, 'pri-ori knowledge' whose type is OtherScientificTerm, 'linearity' whose type is OtherScientificTerm, 'PCA' whose type is Method, 'non-linearities' whose type is OtherScientificTerm, 'rotations' whose type is OtherScientificTerm, 'non-linear extensions of active shape models' whose type is Method, 'active shape models' whose type is Method, 'user interaction' whose type is OtherScientificTerm, 'build-ing/choosing optimal models' whose type is Task, 'algorithm' whose type is Generic, 'minimum description length principle' whose type is Method, 'linear modeling' whose type is Method, 'model' whose type is Generic, 'modes of variations' whose type is OtherScientificTerm, 'method' whose type is Generic, 'synthetic data' whose type is Material, 'medical images' whose type is Material, 'hand contours' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we introduce KAZE features, a novel multiscale 2D feature detection and description algorithm in nonlinear scale spaces. Previous approaches detect and describe features at different scale levels by building or approximating the Gaussian scale space of an image. However, Gaussian blurring does not respect the natural boundaries of objects and smoothes to the same degree both details and noise, reducing localization accuracy and distinctiveness. In contrast, we detect and describe 2D features in a nonlinear scale space by means of nonlinear diffusion filtering. In this way, we can make blurring locally adaptive to the image data, reducing noise but retaining object boundaries, obtaining superior localization accuracy and distinctiviness. The nonlinear scale space is built using efficient Additive Operator Splitting (AOS) techniques and variable con-ductance diffusion. We present an extensive evaluation on benchmark datasets and a practical matching application on deformable surfaces. Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space, but comparable to SIFT, our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods.", "output": "The abstract text contains these key words below: 'KAZE features' whose type is Method, 'multiscale 2D feature detection and description algorithm' whose type is Method, 'nonlinear scale spaces' whose type is OtherScientificTerm, 'Gaussian scale space' whose type is OtherScientificTerm, 'Gaussian blurring' whose type is OtherScientificTerm, 'boundaries of objects' whose type is OtherScientificTerm, 'localization accuracy and distinctiveness' whose type is Metric, '2D features' whose type is Method, 'nonlinear scale space' whose type is OtherScientificTerm, 'nonlinear diffusion filtering' whose type is Method, 'image data' whose type is Material, 'object boundaries' whose type is OtherScientificTerm, 'localization accuracy and distinctiviness' whose type is Metric, 'nonlinear scale space' whose type is OtherScientificTerm, 'Additive Operator Splitting (AOS) techniques' whose type is Method, 'variable con-ductance diffusion' whose type is Method, 'benchmark datasets' whose type is Material, 'matching application on deformable surfaces' whose type is Task, 'features' whose type is Generic, 'SURF' whose type is Method, 'nonlinear scale space' whose type is OtherScientificTerm, 'SIFT' whose type is Method, 'results' whose type is Generic, 'detection' whose type is Task, 'description' whose type is Task, 'state-of-the-art methods' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper describes a particular approach to  parsing  that utilizes recent advances in  unification-based parsing  and in  classification-based knowledge representation . As  unification-based grammatical frameworks  are extended to handle richer descriptions of  linguistic information , they begin to share many of the properties that have been developed in  KL-ONE-like knowledge representation systems . This commonality suggests that some of the  classification-based representation techniques  can be applied to  unification-based linguistic descriptions . This merging supports the integration of  semantic and syntactic information  into the same system, simultaneously subject to the same types of processes, in an efficient manner. The result is expected to be more  efficient parsing  due to the increased organization of knowledge. The use of a  KL-ONE style representation  for  parsing  and  semantic interpretation  was first explored in the  PSI-KLONE system  [2], in which  parsing  is characterized as an inference process called  incremental description refinement .\n", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'parsing' whose type is Task, 'unification-based parsing' whose type is Task, 'classification-based knowledge representation' whose type is Task, 'unification-based grammatical frameworks' whose type is Method, 'linguistic information' whose type is OtherScientificTerm, 'they' whose type is Generic, 'KL-ONE-like knowledge representation systems' whose type is Method, 'classification-based representation techniques' whose type is Method, 'unification-based linguistic descriptions' whose type is OtherScientificTerm, 'semantic and syntactic information' whose type is OtherScientificTerm, 'system' whose type is Generic, 'parsing' whose type is Method, 'KL-ONE style representation' whose type is Method, 'parsing' whose type is Task, 'semantic interpretation' whose type is Task, 'PSI-KLONE system' whose type is Method, 'parsing' whose type is Task, 'inference process' whose type is Method, 'incremental description refinement' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper describes a method of  interactively visualizing and directing the process  of  translating a sentence  . The method allows a  user  to explore a  model  of  syntax-based statistical machine translation (MT)  , to understand the  model  's strengths and weaknesses, and to compare it to other  MT systems  . Using this  visualization method  , we can find and address conceptual and practical problems in an  MT system  . In our demonstration at  ACL  , new  users  of our tool will drive a  syntax-based decoder  for themselves. ", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'interactively visualizing and directing the process  of  translating' whose type is Task, 'method' whose type is Generic, 'model' whose type is Generic, 'syntax-based statistical machine translation (MT)' whose type is Task, 'model' whose type is Generic, 'it' whose type is Generic, 'MT systems' whose type is Method, 'visualization method' whose type is Method, 'MT system' whose type is Method, 'syntax-based decoder' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he construction of causal graphs from non-experimental data rests on a set of constraints that the graph structure imposes on all probability distributions compatible with the graph. These constraints are of two types: conditional inde-pendencies and algebraic constraints, first noted by Verma. While conditional independencies are well studied and frequently used in causal induction algorithms, Verma constraints are still poorly understood, and rarely applied. In this paper we examine a special subset of Verma constraints which are easy to understand, easy to identify and easy to apply ; they arise from \" dormant independencies, \" namely, conditional independencies that hold in interventional distributions. We give a complete algorithm for determining if a dormant independence between two sets of variables is entailed by the causal graph, such that this independence is identifiable , in other words if it resides in an interventional distribution that can be predicted without resorting to interventions. We further show the usefulness of dormant independencies in model testing and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph.", "output": "The abstract text contains these key words below: 'construction of causal graphs' whose type is Task, 'non-experimental data' whose type is Material, 'constraints' whose type is OtherScientificTerm, 'graph structure' whose type is OtherScientificTerm, 'probability distributions' whose type is OtherScientificTerm, 'graph' whose type is OtherScientificTerm, 'constraints' whose type is Generic, 'conditional inde-pendencies' whose type is OtherScientificTerm, 'algebraic constraints' whose type is OtherScientificTerm, 'conditional independencies' whose type is OtherScientificTerm, 'causal induction algorithms' whose type is Method, 'Verma constraints' whose type is OtherScientificTerm, 'Verma constraints' whose type is OtherScientificTerm, 'they' whose type is Generic, 'dormant independencies' whose type is OtherScientificTerm, 'conditional independencies' whose type is OtherScientificTerm, 'interventional distributions' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'dormant independence' whose type is OtherScientificTerm, 'variables' whose type is OtherScientificTerm, 'causal graph' whose type is OtherScientificTerm, 'independence' whose type is OtherScientificTerm, 'it' whose type is Generic, 'interventional distribution' whose type is OtherScientificTerm, 'interventions' whose type is OtherScientificTerm, 'dormant independencies' whose type is OtherScientificTerm, 'model testing' whose type is Task, 'induction' whose type is Task, 'algorithm' whose type is Generic, 'constraints' whose type is Generic, 'dormant independencies' whose type is OtherScientificTerm, 'extraneous edges' whose type is OtherScientificTerm, 'causal graph' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Dividing  sentences  in  chunks of words  is a useful preprocessing step for  parsing ,  information extraction  and  information retrieval . (Ramshaw and Marcus, 1995) have introduced a \"convenient\"  data representation  for  chunking  by converting it to a  tagging task . In this paper we will examine seven different  data representations  for the problem of recognizing  noun phrase chunks . We will show that the  data representation choice  has a minor influence on  chunking performance . However, equipped with the most suitable data representation , our  memory-based learning chunker  was able to improve the best published  chunking results  for a  standard data set .\n", "output": "The abstract text contains these key words below: 'Dividing  sentences  in  chunks of words' whose type is Task, 'parsing' whose type is Task, 'information extraction' whose type is Task, 'information retrieval' whose type is Task, 'data representation' whose type is Method, 'chunking' whose type is Task, 'it' whose type is Generic, 'tagging task' whose type is Task, 'data representations' whose type is Method, 'recognizing  noun phrase chunks' whose type is Task, 'data representation' whose type is Method, 'chunking' whose type is Task, 'data representation' whose type is Method, 'memory-based learning chunker' whose type is Method, 'chunking' whose type is Task, 'data set' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " critical step in encoding sound for neuronal processing occurs when the analog pressure wave is coded into discrete nerve-action potentials. Recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus, so we used visual inspection and automatic speech recognition (ASR) to investigate an offset adaptation (OA) model proposed by Zhang et al. [1]. OA improved phase locking in the auditory nerve (AN) and raised ASR accuracy for features derived from AN fibers (ANFs). We also found that OA is crucial for auditory processing by onset neurons (ONs) in the next neuronal stage, the auditory brainstem. Multi-layer perceptrons (MLPs) performed much better than standard Gaussian mixture models (GMMs) for both our ANF-based and ON-based auditory features. Similar results were previously obtained with MSG (Modulation-filtered Spec-troGram) auditory features[2]. Thus we believe researchers working with novel features should consider trying MLPs.", "output": "The abstract text contains these key words below: 'encoding sound' whose type is Task, 'neuronal processing' whose type is Task, 'analog pressure wave' whose type is OtherScientificTerm, 'discrete nerve-action potentials' whose type is OtherScientificTerm, 'pool models' whose type is Method, 'inner hair cell synapse' whose type is OtherScientificTerm, 'visual inspection' whose type is Method, 'automatic speech recognition (ASR)' whose type is Method, 'offset adaptation (OA) model' whose type is Method, 'OA' whose type is Method, 'phase locking in the auditory nerve (AN)' whose type is Task, 'ASR accuracy' whose type is Metric, 'features' whose type is OtherScientificTerm, 'AN fibers (ANFs)' whose type is OtherScientificTerm, 'OA' whose type is Method, 'auditory processing' whose type is Task, 'onset neurons (ONs)' whose type is OtherScientificTerm, 'auditory brainstem' whose type is OtherScientificTerm, 'Multi-layer perceptrons (MLPs)' whose type is Method, 'Gaussian mixture models (GMMs)' whose type is Method, 'ANF-based and ON-based auditory features' whose type is OtherScientificTerm, 'MSG (Modulation-filtered Spec-troGram) auditory features' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'MLPs' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "martKom is a multimodal dialog system that combines speech, gesture, and mimics input and output. Spontaneous speech understanding is combined with the video-based recognition of natural gestures. One of the major scientific goals of SmartKom is to design new computational methods for the seamless integration and mutual disambiguation of multimodal input and output on a semantic and pragmatic level. SmartKom is based on the situated delegation-oriented dialog paradigm, in which the user delegates a task to a virtual communication assistant, visualized as a lifelike character on a graphical display. We describe the SmartKom architecture, the use of an XML-based markup language for multimodal content, and some of the distinguishing features of the first fully operational SmartKom demonstrator.", "output": "The abstract text contains these key words below: 'SmartKom' whose type is Method, 'multimodal dialog system' whose type is Method, 'speech' whose type is Material, 'gesture' whose type is Material, 'Spontaneous speech understanding' whose type is Task, 'video-based recognition of natural gestures' whose type is Task, 'SmartKom' whose type is Method, 'computational methods' whose type is Method, 'integration and mutual disambiguation of multimodal input and output' whose type is Task, 'semantic and pragmatic level' whose type is OtherScientificTerm, 'SmartKom' whose type is OtherScientificTerm, 'situated delegation-oriented dialog paradigm' whose type is Method, 'virtual communication assistant' whose type is OtherScientificTerm, 'graphical display' whose type is OtherScientificTerm, 'SmartKom architecture' whose type is Method, 'XML-based markup language' whose type is OtherScientificTerm, 'multimodal content' whose type is Material, 'SmartKom demonstrator' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "emantic Web documents that encode facts about entities on the Web have been growing rapidly in size and evolving over time. Creating summaries on lengthy Semantic Web documents for quick identification of the corresponding entity has been of great contemporary interest. In this paper, we explore automatic summa-rization techniques that characterize and enable identification of an entity and create summaries that are human friendly. Specifically, we highlight the importance of diversified (faceted) summaries by combining three dimensions: diversity, uniqueness, and popularity. Our novel diversity-aware entity summarization approach mimics human conceptual clustering techniques to group facts, and picks representative facts from each group to form concise (i.e., short) and comprehensive (i.e., improved coverage through diversity) summaries. We evaluate our approach against the state-of-the-art techniques and show that our work improves both the quality and the efficiency of entity summarization.", "output": "The abstract text contains these key words below: 'Semantic Web documents' whose type is Material, 'Creating summaries' whose type is Task, 'lengthy Semantic Web documents' whose type is Material, 'identification of the corresponding entity' whose type is Task, 'automatic summa-rization techniques' whose type is Method, 'diversified (faceted) summaries' whose type is OtherScientificTerm, 'diversity' whose type is OtherScientificTerm, 'uniqueness' whose type is OtherScientificTerm, 'popularity' whose type is OtherScientificTerm, 'diversity-aware entity summarization approach' whose type is Method, 'human conceptual clustering techniques' whose type is Method, 'approach' whose type is Generic, 'state-of-the-art techniques' whose type is Generic, 'quality' whose type is Metric, 'efficiency' whose type is Metric, 'entity summarization' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper shows how  dictionary word sense definitions  can be analysed by applying a hierarchy of  phrasal patterns . An experimental system embodying this mechanism has been implemented for processing  definitions  from the  Longman Dictionary of Contemporary English . A property of this  dictionary , exploited by the system, is that it uses a  restricted vocabulary  in its  word sense definitions . The structures generated by the experimental system are intended to be used for the  classification  of new  word senses  in terms of the  senses  of  words  in the  restricted vocabulary . Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific  phrasal analysis rules  as determined by a hierarchy of  patterns  in which less specific  patterns  dominate more specific ones. This ensures that reasonable incomplete analyses of the  definitions  are produced when more complete analyses are not possible, resulting in a relatively robust  analysis mechanism . Thus the work reported addresses two  robustness problems  faced by current experimental  natural language processing systems : coping with an incomplete  lexicon  and with incomplete  knowledge  of  phrasal constructions . \n", "output": "The abstract text contains these key words below: 'dictionary word sense definitions' whose type is OtherScientificTerm, 'hierarchy of  phrasal patterns' whose type is Method, 'system' whose type is Generic, 'mechanism' whose type is Generic, 'Longman Dictionary of Contemporary English' whose type is Material, 'dictionary' whose type is Generic, 'system' whose type is Generic, 'it' whose type is Generic, 'restricted vocabulary' whose type is OtherScientificTerm, 'word sense definitions' whose type is OtherScientificTerm, 'classification  of new  word senses' whose type is Task, 'restricted vocabulary' whose type is OtherScientificTerm, 'phrasal analysis rules' whose type is OtherScientificTerm, 'robustness problems' whose type is Task, 'natural language processing systems' whose type is Method, 'incomplete  lexicon' whose type is Material, 'incomplete  knowledge  of  phrasal constructions' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The paper presents a method for  word sense disambiguation  based on  parallel corpora . The method exploits recent advances in  word alignment  and  word clustering  based on  automatic extraction  of  translation equivalents  and being supported by available aligned  wordnets  for the  languages  in the  corpus . The  wordnets  are aligned to the  Princeton Wordnet , according to the principles established by  EuroWordNet . The evaluation of the  WSD system , implementing the method described herein showed very encouraging results. The same system used in a validation mode, can be used to check and spot  alignment errors  in  multilingually aligned wordnets  as  BalkaNet  and  EuroWordNet .\n", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'word sense disambiguation' whose type is Task, 'parallel corpora' whose type is Material, 'method' whose type is Generic, 'word alignment' whose type is Task, 'word clustering' whose type is Task, 'automatic extraction  of  translation equivalents' whose type is Task, 'aligned  wordnets' whose type is Material, 'wordnets' whose type is Material, 'Princeton Wordnet' whose type is Material, 'EuroWordNet' whose type is Material, 'WSD system' whose type is Method, 'method' whose type is Generic, 'system' whose type is Generic, 'alignment errors  in  multilingually aligned wordnets' whose type is OtherScientificTerm, 'multilingually aligned wordnets' whose type is Material, 'BalkaNet' whose type is Material, 'EuroWordNet' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The paper proposes and empirically motivates an integration of  supervised learning  with  unsupervised learning  to deal with human biases in  summarization . In particular, we explore the use of  probabilistic decision tree  within the clustering framework to account for the variation as well as regularity in  human created summaries . The  corpus  of human created extracts is created from a  newspaper corpus  and used as a test set. We build  probabilistic decision trees  of different flavors and integrate each of them with the clustering framework. Experiments with the  corpus  demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either ofthe two is considered alone.\n", "output": "The abstract text contains these key words below: 'supervised learning' whose type is Method, 'unsupervised learning' whose type is Method, 'human biases in  summarization' whose type is Task, 'probabilistic decision tree' whose type is OtherScientificTerm, 'clustering framework' whose type is Method, 'human created summaries' whose type is Material, 'corpus  of human created extracts' whose type is Material, 'newspaper corpus' whose type is Material, 'probabilistic decision trees' whose type is OtherScientificTerm, 'them' whose type is Generic, 'clustering framework' whose type is Method, 'corpus' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we improve an  unsupervised learning method  using the  Expectation-Maximization (EM) algorithm  proposed by Nigam et al. for  text classification problems  in order to apply it to  word sense disambiguation (WSD) problems . The improved method stops the  EM algorithm  at the  optimum iteration number . To estimate that number, we propose two methods. In experiments, we solved 50  noun WSD problems  in the  Japanese Dictionary Task in SENSEVAL2 . The score of our method is a match for the best public score of this task. Furthermore, our methods were confirmed to be effective also for  verb WSD problems .\n", "output": "The abstract text contains these key words below: 'unsupervised learning method' whose type is Method, 'Expectation-Maximization (EM) algorithm' whose type is Method, 'text classification problems' whose type is Task, 'it' whose type is Generic, 'word sense disambiguation (WSD) problems' whose type is Task, 'method' whose type is Generic, 'EM algorithm' whose type is Method, 'optimum iteration number' whose type is OtherScientificTerm, 'number' whose type is Generic, 'noun WSD problems' whose type is Task, 'Japanese Dictionary Task' whose type is Task, 'SENSEVAL2' whose type is Material, 'method' whose type is Generic, 'task' whose type is Generic, 'methods' whose type is Generic, 'verb WSD problems' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ight fields are image-based representations that use densely sampled rays as a scene description. In this paper, we explore geometric structures of 3D lines in ray space for improving light field triangulation and stereo matching. The triangulation problem aims to fill in the ray space with continuous and non-overlapping simplices anchored at sampled points (rays). Such a triangulation provides a piecewise-linear interpolant useful for light field super-resolution. We show that the light field space is largely bi-linear due to 3D line segments in the scene, and direct tri-angulation of these bilinear subspaces leads to large errors. We instead present a simple but effective algorithm to first map bilinear subspaces to line constraints and then apply Constrained Delaunay Triangulation (CDT). Based on our analysis, we further develop a novel line-assisted graph-cut (LAGC) algorithm that effectively encodes 3D line constraints into light field stereo matching. Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform state-of-the-art solutions in accuracy and visual quality.", "output": "The abstract text contains these key words below: 'Light fields' whose type is OtherScientificTerm, 'image-based representations' whose type is Method, 'densely sampled rays' whose type is OtherScientificTerm, 'scene description' whose type is OtherScientificTerm, 'geometric structures of 3D lines' whose type is Task, 'ray space' whose type is OtherScientificTerm, 'light field triangulation' whose type is Task, 'stereo matching' whose type is Task, 'triangulation problem' whose type is Task, 'ray space' whose type is OtherScientificTerm, 'continuous and non-overlapping simplices' whose type is OtherScientificTerm, 'triangulation' whose type is OtherScientificTerm, 'piecewise-linear interpolant' whose type is OtherScientificTerm, 'light field super-resolution' whose type is Task, 'light field space' whose type is OtherScientificTerm, '3D line segments' whose type is OtherScientificTerm, 'bilinear subspaces' whose type is OtherScientificTerm, 'bilinear subspaces' whose type is OtherScientificTerm, 'line constraints' whose type is OtherScientificTerm, 'Constrained Delaunay Triangulation (CDT)' whose type is Method, 'line-assisted graph-cut (LAGC) algorithm' whose type is Method, '3D line constraints' whose type is OtherScientificTerm, 'light field stereo matching' whose type is Task, 'synthetic and real data' whose type is Material, 'triangulation and LAGC algorithms' whose type is Method, 'state-of-the-art solutions' whose type is Generic, 'accuracy' whose type is Metric, 'visual quality' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In the  Chinese language , a  verb  may have its  dependents  on its left, right or on both sides. The  ambiguity resolution  of  right-side dependencies  is essential for  dependency parsing  of  sentences  with two or more  verbs . Previous works on  shift-reduce dependency parsers  may not guarantee the  connectivity  of a  dependency tree  due to their weakness at resolving the  right-side dependencies . This paper proposes a  two-phase shift-reduce dependency parser  based on  SVM learning . The  left-side dependents  and  right-side nominal dependents  are detected in Phase I, and  right-side verbal dependents  are decided in Phase II. In experimental evaluation, our proposed method outperforms previous  shift-reduce dependency parsers  for the  Chine language , showing improvement of  dependency accuracy  by 10.08%.\n", "output": "The abstract text contains these key words below: 'Chinese language' whose type is Material, 'ambiguity resolution  of  right-side dependencies' whose type is Task, 'dependency parsing' whose type is Task, 'shift-reduce dependency parsers' whose type is Method, 'connectivity' whose type is Metric, 'dependency tree' whose type is OtherScientificTerm, 'right-side dependencies' whose type is OtherScientificTerm, 'two-phase shift-reduce dependency parser' whose type is Method, 'SVM learning' whose type is Method, 'left-side dependents' whose type is OtherScientificTerm, 'right-side nominal dependents' whose type is OtherScientificTerm, 'right-side verbal dependents' whose type is OtherScientificTerm, 'method' whose type is Generic, 'shift-reduce dependency parsers' whose type is Method, 'Chine language' whose type is Material, 'dependency accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n In this paper we show how two standard outputs from  information extraction (IE) systems  -  named entity annotations  and  scenario templates  - can be used to enhance access to  text collections  via a standard  text browser  . We describe how this information is used in a  prototype system  designed to support  information workers  ' access to a  pharmaceutical news archive  as part of their  industry watch  function. We also report results of a preliminary,  qualitative user evaluation  of the system, which while broadly positive indicates further work needs to be done on the  interface  to make  users  aware of the increased potential of  IE-enhanced text browsers  . ", "output": "The abstract text contains these key words below: 'outputs' whose type is Generic, 'information extraction (IE) systems' whose type is Method, 'named entity annotations' whose type is OtherScientificTerm, 'scenario templates' whose type is OtherScientificTerm, 'text collections' whose type is Material, 'text browser' whose type is Method, 'prototype system' whose type is Generic, 'pharmaceutical news archive' whose type is Material, 'qualitative user evaluation' whose type is Metric, 'system' whose type is Generic, 'IE-enhanced text browsers' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e describe a general framework for online multiclass learning based on the notion of hypothesis sharing. In our framework sets of classes are associated with hypotheses. Thus, all classes within a given set share the same hypothesis. This framework includes as special cases commonly used constructions for multiclass categorization such as allocating a unique hypothesis for each class and allocating a single common hypothesis for all classes. We generalize the multiclass Perceptron to our framework and derive a unifying mistake bound analysis. Our construction naturally extends to settings where the number of classes is not known in advance but, rather, is revealed along the online learning process. We demonstrate the merits of our approach by comparing it to previous methods on both synthetic and natural datasets.", "output": "The abstract text contains these key words below: 'framework' whose type is Generic, 'online multiclass learning' whose type is Task, 'notion of hypothesis sharing' whose type is OtherScientificTerm, 'framework' whose type is Generic, 'framework' whose type is Generic, 'multiclass categorization' whose type is Task, 'multiclass Perceptron' whose type is Method, 'framework' whose type is Generic, 'unifying mistake bound analysis' whose type is Method, 'online learning process' whose type is Method, 'approach' whose type is Generic, 'it' whose type is Generic, 'methods' whose type is Generic, 'synthetic and natural datasets' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Systemic grammar  has been used for  AI text generation  work in the past, but the  implementations  have tended be ad hoc or inefficient. This paper presents an approach to systemic  text generation  where  AI problem solving techniques  are applied directly to an unadulterated  systemic grammar . This  approach  is made possible by a special relationship between  systemic grammar  and  problem solving : both are organized primarily as choosing from alternatives. The result is simple, efficient  text generation  firmly based in a  linguistic theory . ", "output": "The abstract text contains these key words below: 'Systemic grammar' whose type is Method, 'AI text generation' whose type is Task, 'implementations' whose type is Generic, 'approach' whose type is Generic, 'text generation' whose type is Task, 'AI problem solving techniques' whose type is Method, 'systemic grammar' whose type is Method, 'approach' whose type is Generic, 'systemic grammar' whose type is Method, 'problem solving' whose type is Method, 'text generation' whose type is Task, 'linguistic theory' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "eflections in image sequences consist of several layers superimposed over each other. This phenomenon causes many image processing techniques to fail as they assume the presence of only one layer at each examined site e.g. motion estimation and object recognition. This work presents an automated technique for detecting reflections in image sequences by analyzing motion trajectories of feature points. It models reflection as regions containing two different layers moving over each other. We present a strong detector based on combining a set of weak detectors. We use novel priors, generate sparse and dense detection maps and our results show high detection rate with rejection to pathological motion and occlusion.", "output": "The abstract text contains these key words below: 'Reflections' whose type is OtherScientificTerm, 'Reflections in image sequences' whose type is OtherScientificTerm, 'image sequences' whose type is Material, 'image processing techniques' whose type is Method, 'motion estimation' whose type is Task, 'object recognition' whose type is Task, 'technique' whose type is Generic, 'detecting reflections in image sequences' whose type is Task, 'motion trajectories' whose type is OtherScientificTerm, 'feature points' whose type is OtherScientificTerm, 'It' whose type is Generic, 'reflection' whose type is OtherScientificTerm, 'detector' whose type is Generic, 'detectors' whose type is Generic, 'priors' whose type is OtherScientificTerm, 'sparse and dense detection maps' whose type is OtherScientificTerm, 'detection rate' whose type is Metric, 'pathological motion' whose type is OtherScientificTerm, 'occlusion' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a  text mining method  for finding  synonymous expressions  based on the  distributional hypothesis  in a set of coherent  corpora . This paper proposes a new methodology to improve the  accuracy  of a  term aggregation system  using each author's  text  as a coherent  corpus . Our approach is based on the idea that one person tends to use one  expression  for one  meaning . According to our assumption, most of the  words  with  similar context features  in each author's  corpus  tend not to be  synonymous expressions . Our proposed method improves the  accuracy  of our  term aggregation system , showing that our approach is successful.\n", "output": "The abstract text contains these key words below: 'text mining method' whose type is Method, 'synonymous expressions' whose type is OtherScientificTerm, 'distributional hypothesis' whose type is OtherScientificTerm, 'methodology' whose type is Generic, 'accuracy' whose type is Metric, 'term aggregation system' whose type is Method, 'approach' whose type is Generic, 'similar context features' whose type is OtherScientificTerm, 'synonymous expressions' whose type is OtherScientificTerm, 'method' whose type is Generic, 'accuracy' whose type is Metric, 'term aggregation system' whose type is Method, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We propose a method that automatically generates  paraphrase  sets from  seed sentences  to be used as  reference sets  in objective  machine translation evaluation measures  like  BLEU  and  NIST  . We measured the quality of the  paraphrases  produced in an experiment, i.e., (i) their  grammaticality  : at least 99% correct  sentences  ; (ii) their  equivalence in meaning  : at least 96% correct  paraphrases  either by  meaning equivalence  or  entailment  ; and, (iii) the amount of internal  lexical and syntactical variation  in a set of  paraphrases  : slightly superior to that of  hand-produced sets  . The  paraphrase  sets produced by this method thus seem adequate as  reference sets  to be used for  MT evaluation  . ", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'paraphrase' whose type is OtherScientificTerm, 'machine translation evaluation measures' whose type is Metric, 'BLEU' whose type is Metric, 'NIST' whose type is Metric, 'paraphrases' whose type is OtherScientificTerm, 'grammaticality' whose type is Metric, 'equivalence in meaning' whose type is Metric, 'paraphrases' whose type is OtherScientificTerm, 'meaning equivalence' whose type is Method, 'entailment' whose type is Method, 'internal  lexical and syntactical variation' whose type is Metric, 'paraphrases' whose type is OtherScientificTerm, 'hand-produced sets' whose type is OtherScientificTerm, 'paraphrase' whose type is OtherScientificTerm, 'method' whose type is Generic, 'MT evaluation' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n  Spelling-checkers  have become an integral part of most  text processing software  . From different reasons among which the speed of processing prevails they are usually based on  dictionaries of word forms  instead of  words  . This approach is sufficient for languages with little  inflection  such as  English  , but fails for  highly inflective languages  such as  Czech  ,  Russian  ,  Slovak  or other  Slavonic languages  . We have developed a special method for describing  inflection  for the purpose of building  spelling-checkers  for such languages. The speed of the resulting program lies somewhere in the middle of the scale of existing  spelling-checkers  for  English  and the main  dictionary  fits into the standard  360K floppy  , whereas the number of recognized  word forms  exceeds 6 million (for  Czech  ). Further, a special method has been developed for easy  word classification  . ", "output": "The abstract text contains these key words below: 'Spelling-checkers' whose type is Task, 'text processing software' whose type is Task, 'they' whose type is Generic, 'dictionaries of word forms' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'languages' whose type is Material, 'inflection' whose type is OtherScientificTerm, 'English' whose type is Material, 'highly inflective languages' whose type is Material, 'Czech' whose type is Material, 'Russian' whose type is Material, 'Slovak' whose type is Material, 'Slavonic languages' whose type is Material, 'method' whose type is Generic, 'inflection' whose type is OtherScientificTerm, 'spelling-checkers' whose type is Task, 'languages' whose type is Generic, 'program' whose type is Generic, 'spelling-checkers' whose type is Method, 'English' whose type is Material, 'Czech' whose type is Material, 'method' whose type is Generic, 'word classification' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "A method of  sense resolution  is proposed that is based on  WordNet , an on-line  lexical database  that incorporates  semantic relations  ( synonymy ,  antonymy ,  hyponymy ,  meronymy ,  causal and troponymic entailment ) as  labeled pointers  between  word senses . With  WordNet , it is easy to retrieve sets of  semantically related words , a facility that will be used for  sense resolution  during  text processing , as follows. When a  word  with multiple  senses  is encountered, one of two procedures will be followed. Either, (1)  words  related in  meaning  to the  alternative senses  of the  polysemous word  will be retrieved; new  strings  will be derived by substituting these related  words  into the  context  of the  polysemous word ; a large  textual corpus  will then be searched for these  derived strings ; and that  sense  will be chosen that corresponds to the  derived string  that is found most often in the  corpus . Or, (2) the  context  of the  polysemous word  will be used as a key to search a large  corpus ; all  words  found to occur in that  context  will be noted;  WordNet  will then be used to estimate the  semantic distance  from those  words  to the  alternative senses  of the  polysemous word ; and that  sense  will be chosen that is closest in  meaning  to other  words  occurring in the same  context  If successful, this procedure could have practical applications to problems of  information retrieval ,  mechanical translation ,  intelligent tutoring systems , and elsewhere.\n", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'sense resolution' whose type is Task, 'WordNet' whose type is Material, 'on-line  lexical database' whose type is Material, 'semantic relations' whose type is OtherScientificTerm, 'synonymy' whose type is OtherScientificTerm, 'antonymy' whose type is OtherScientificTerm, 'hyponymy' whose type is OtherScientificTerm, 'meronymy' whose type is OtherScientificTerm, 'causal and troponymic entailment' whose type is OtherScientificTerm, 'WordNet' whose type is Material, 'semantically related words' whose type is OtherScientificTerm, 'sense resolution' whose type is Task, 'text processing' whose type is Task, 'procedures' whose type is Generic, 'alternative senses' whose type is OtherScientificTerm, 'polysemous word' whose type is OtherScientificTerm, 'polysemous word' whose type is OtherScientificTerm, 'polysemous word' whose type is OtherScientificTerm, 'WordNet' whose type is Material, 'semantic distance' whose type is OtherScientificTerm, 'procedure' whose type is Generic, 'information retrieval' whose type is Task, 'mechanical translation' whose type is Task, 'intelligent tutoring systems' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " We present a novel method for  discovering parallel sentences  in  comparable, non-parallel corpora  . We train a  maximum entropy classifier  that, given a pair of  sentences  , can reliably determine whether or not they are  translations  of each other. Using this approach, we extract  parallel data  from large  Chinese, Arabic, and English non-parallel newspaper corpora  . We evaluate the  quality of the extracted data  by showing that it improves the performance of a state-of-the-art  statistical machine translation system  . We also show that a good-quality  MT system  can be built from scratch by starting with a very small  parallel corpus  (100,000  words  ) and exploiting a large  non-parallel corpus  . Thus, our method can be applied with great benefit to  language pairs  for which only scarce  resources  are available. ", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'discovering parallel sentences' whose type is Task, 'comparable, non-parallel corpora' whose type is Material, 'maximum entropy classifier' whose type is Method, 'approach' whose type is Generic, 'parallel data' whose type is Material, 'Chinese, Arabic, and English non-parallel newspaper corpora' whose type is Material, 'it' whose type is Generic, 'statistical machine translation system' whose type is Method, 'MT system' whose type is Method, 'parallel corpus' whose type is Material, 'non-parallel corpus' whose type is Material, 'method' whose type is Generic, 'scarce  resources' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Oral communication  is ubiquitous and carries important information yet it is also time consuming to document. Given the development of  storage media and networks  one could just record and store a  conversation  for documentation. The question is, however, how an interesting information piece would be found in a  large database  . Traditional  information retrieval techniques  use a  histogram  of  keywords  as the  document representation  but  oral communication  may offer additional  indices  such as the time and place of the rejoinder and the attendance. An alternative  index  could be the activity such as discussing, planning, informing, story-telling, etc. This paper addresses the problem of the  automatic detection  of those activities in meeting situation and everyday rejoinders. Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can define subsets of larger  database  and detect those automatically which is shown on a large  database  of  TV shows  .  Emotions  and other  indices  such as the  dominance distribution of speakers  might be available on the  surface  and could be used directly. Despite the small size of the  databases  used some results about the effectiveness of these  indices  can be obtained. ", "output": "The abstract text contains these key words below: 'Oral communication' whose type is Material, 'storage media and networks' whose type is Task, 'conversation' whose type is Material, 'information retrieval techniques' whose type is Method, 'histogram  of  keywords' whose type is Method, 'document representation' whose type is Method, 'oral communication' whose type is Material, 'activity' whose type is OtherScientificTerm, 'discussing' whose type is OtherScientificTerm, 'planning' whose type is OtherScientificTerm, 'informing' whose type is OtherScientificTerm, 'story-telling' whose type is OtherScientificTerm, 'automatic detection' whose type is Task, 'activities' whose type is Generic, 'activities' whose type is Generic, 'database  of  TV shows' whose type is Material, 'Emotions' whose type is Material, 'dominance distribution of speakers' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper proposes the  Hierarchical Directed Acyclic Graph (HDAG) Kernel  for  structured natural language data  . The  HDAG Kernel  directly accepts several levels of both  chunks  and their  relations  , and then efficiently computes the  weighed sum  of the number of common  attribute sequences  of the  HDAGs  . We applied the proposed method to  question classification  and  sentence alignment tasks  to evaluate its performance as a  similarity measure  and a  kernel function  . The results of the experiments demonstrate that the  HDAG Kernel  is superior to other  kernel functions  and  baseline methods  . ", "output": "The abstract text contains these key words below: 'Hierarchical Directed Acyclic Graph (HDAG) Kernel' whose type is Method, 'structured natural language data' whose type is Material, 'HDAG Kernel' whose type is Method, 'HDAGs' whose type is Method, 'method' whose type is Generic, 'question classification  and  sentence alignment tasks' whose type is Task, 'similarity measure' whose type is Metric, 'kernel function' whose type is Method, 'HDAG Kernel' whose type is Method, 'kernel functions' whose type is Method, 'baseline methods' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he problem of blind separation of underdetermined instantaneous mixtures of independent signals is addressed through a method relying on nonstationarity of the original signals. The signals are assumed to be piecewise stationary with varying variances in different epochs. In comparison with previous works, in this paper it is assumed that the signals are not i.i.d. in each epoch, but obey a first-order autoregressive model. This model was shown to be more appropriate for blind separation of natural speech signals. A separation method is proposed that is nearly statistically efficient (approaching the corresponding Cram\u00e9r-Rao lower bound), if the separated signals obey the assumed model. In the case of natural speech signals , the method is shown to have separation accuracy better than the state-of-the-art methods.", "output": "The abstract text contains these key words below: 'blind separation of underdetermined instantaneous mixtures of independent signals' whose type is Task, 'method' whose type is Generic, 'nonstationarity' whose type is OtherScientificTerm, 'original signals' whose type is OtherScientificTerm, 'signals' whose type is Generic, 'signals' whose type is Generic, 'first-order autoregressive model' whose type is Method, 'model' whose type is Generic, 'blind separation of natural speech signals.' whose type is Task, 'separation method' whose type is Method, 'Cram\u00e9r-Rao lower bound)' whose type is OtherScientificTerm, 'assumed model' whose type is Generic, 'natural speech signals' whose type is OtherScientificTerm, 'method' whose type is Generic, 'separation accuracy' whose type is Metric, 'methods' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper a  morphological component  with a limited capability to automatically interpret (and generate)  derived words  is presented. The system combines an extended  two-level morphology  [Trost, 1991a; Trost, 1991b] with a  feature-based word grammar  building on a  hierarchical lexicon .  Polymorphemic stems  not explicitly stored in the  lexicon  are given a  compositional interpretation . That way the system allows to minimize redundancy in the  lexicon  because  derived words  that are transparent need not to be stored explicitly. Also,  words formed ad-hoc  can be recognized correctly. The system is implemented in CommonLisp and has been tested on examples from  German derivation .\n", "output": "The abstract text contains these key words below: 'morphological component' whose type is Method, 'derived words' whose type is OtherScientificTerm, 'system' whose type is Generic, 'two-level morphology' whose type is OtherScientificTerm, 'feature-based word grammar' whose type is Method, 'hierarchical lexicon' whose type is OtherScientificTerm, 'Polymorphemic stems' whose type is OtherScientificTerm, 'compositional interpretation' whose type is OtherScientificTerm, 'system' whose type is Generic, 'derived words' whose type is OtherScientificTerm, 'words formed ad-hoc' whose type is OtherScientificTerm, 'system' whose type is Generic, 'CommonLisp' whose type is Method, 'German derivation' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Reducing  language model (LM) size  is a critical issue when applying a  LM  to realistic applications which have memory constraints. In this paper, three measures are studied for the purpose of  LM pruning . They are probability,  rank , and  entropy . We evaluated the performance of the three  pruning criteria  in a real application of  Chinese text input  in terms of  character error rate (CER) . We first present an empirical comparison, showing that  rank  performs the best in most cases. We also show that the high-performance of  rank  lies in its strong correlation with  error rate . We then present a novel method of combining two criteria in  model pruning . Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately, at the same  CER .\n", "output": "The abstract text contains these key words below: 'Reducing  language model (LM) size' whose type is Task, 'LM' whose type is Method, 'memory constraints.' whose type is OtherScientificTerm, 'LM pruning' whose type is Task, 'rank' whose type is OtherScientificTerm, 'entropy' whose type is OtherScientificTerm, 'pruning criteria' whose type is OtherScientificTerm, 'Chinese text input' whose type is Material, 'character error rate (CER)' whose type is Metric, 'rank' whose type is OtherScientificTerm, 'rank' whose type is OtherScientificTerm, 'error rate' whose type is Metric, 'method' whose type is Generic, 'model pruning' whose type is Task, 'CER' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "To support engaging human users in robust,  mixed-initiative speech dialogue interactions  which reach beyond current capabilities in  dialogue systems  , the  DARPA Communicator program  [1] is funding the development of a  distributed message-passing infrastructure  for  dialogue systems  which all  Communicator  participants are using. In this presentation, we describe the features of and  requirements  for a genuinely useful  software infrastructure  for this purpose. ", "output": "The abstract text contains these key words below: 'mixed-initiative speech dialogue interactions' whose type is Method, 'dialogue systems' whose type is Method, 'distributed message-passing infrastructure' whose type is Method, 'dialogue systems' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we compare the performance of a state-of-the-art  statistical parser  (Bikel, 2004) in parsing  written and spoken language  and in generating  sub-categorization cues  from  written and spoken language . Although  Bikel's parser  achieves a higher  accuracy  for parsing  written language , it achieves a higher  accuracy  when extracting  subcategorization cues  from  spoken language . Our experiments also show that current technology for  extracting subcategorization frames  initially designed for  written texts  works equally well for  spoken language . Additionally, we explore the utility of  punctuation  in helping  parsing  and  extraction  of  subcategorization cues . Our experiments show that  punctuation  is of little help in parsing  spoken language  and extracting  subcategorization cues  from  spoken language . This indicates that there is no need to add  punctuation  in transcribing  spoken corpora  simply in order to help  parsers .\n", "output": "The abstract text contains these key words below: 'statistical parser' whose type is Method, 'parsing  written and spoken language' whose type is Task, 'written and spoken language' whose type is Material, 'generating  sub-categorization cues' whose type is Task, 'written and spoken language' whose type is Material, 'Bikel's parser' whose type is Method, 'accuracy' whose type is Metric, 'parsing  written language' whose type is Task, 'written language' whose type is Material, 'it' whose type is Generic, 'accuracy' whose type is Metric, 'subcategorization cues' whose type is OtherScientificTerm, 'spoken language' whose type is Material, 'technology' whose type is Generic, 'extracting subcategorization frames' whose type is Task, 'written texts' whose type is Material, 'spoken language' whose type is Material, 'punctuation' whose type is OtherScientificTerm, 'parsing' whose type is Method, 'extraction  of  subcategorization cues' whose type is Task, 'punctuation' whose type is OtherScientificTerm, 'parsing  spoken language' whose type is Task, 'spoken language' whose type is Material, 'extracting  subcategorization cues' whose type is Task, 'subcategorization cues' whose type is OtherScientificTerm, 'spoken language' whose type is Material, 'punctuation' whose type is OtherScientificTerm, 'spoken corpora' whose type is Material, 'parsers' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We provide a unified account of  sentence-level and text-level anaphora  within the framework of a  dependency-based grammar model . Criteria for  anaphora resolution  within  sentence boundaries  rephrase major concepts from  GB's binding theory , while those for  text-level anaphora  incorporate an adapted version of a  Grosz-Sidner-style focus model .\n", "output": "The abstract text contains these key words below: 'sentence-level and text-level anaphora' whose type is OtherScientificTerm, 'dependency-based grammar model' whose type is Method, 'Criteria' whose type is Generic, 'anaphora resolution  within  sentence boundaries' whose type is Task, 'GB's binding theory' whose type is Method, 'those' whose type is Generic, 'text-level anaphora' whose type is OtherScientificTerm, 'Grosz-Sidner-style focus model' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "actor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. In factor analysis, the observations are modeled as a linear combination of normally distributed hidden variables. We describe a nonlinear generalization of factor analysis , called \"product analy-sis\", that models the observed variables as a linear combination of products of normally distributed hidden variables. Just as factor analysis can be viewed as unsupervised linear regression on unobserved, normally distributed hidden variables, product analysis can be viewed as unsupervised linear regression on products of unobserved, normally distributed hidden variables. The mapping between the data and the hidden space is nonlinear, so we use an approximate variational technique for inference and learning. Since product analysis is a generalization of factor analysis, product analysis always finds a higher data likelihood than factor analysis. We give results on pattern recognition and illumination-invariant image clustering.", "output": "The abstract text contains these key words below: 'Factor analysis' whose type is Method, 'principal components analysis' whose type is Method, 'linear relationships between observed variables' whose type is OtherScientificTerm, 'high-dimensional data' whose type is Material, 'lower-dimensional hidden space' whose type is OtherScientificTerm, 'factor analysis' whose type is Method, 'linear combination of normally distributed hidden variables' whose type is Method, 'nonlinear generalization of factor analysis' whose type is Method, 'factor analysis' whose type is Method, '\"product analy-sis\"' whose type is Method, 'observed variables' whose type is OtherScientificTerm, 'linear combination of products of normally distributed hidden variables' whose type is OtherScientificTerm, 'factor analysis' whose type is Method, 'unsupervised linear regression' whose type is Method, 'distributed hidden variables' whose type is OtherScientificTerm, 'product analysis' whose type is Method, 'unsupervised linear regression' whose type is Method, 'distributed hidden variables' whose type is OtherScientificTerm, 'hidden space' whose type is OtherScientificTerm, 'approximate variational technique' whose type is Method, 'inference' whose type is Task, 'learning' whose type is Task, 'product analysis' whose type is Method, 'generalization of factor analysis' whose type is Method, 'factor analysis' whose type is Method, 'product analysis' whose type is Method, 'factor analysis' whose type is Method, 'pattern recognition' whose type is Task, 'illumination-invariant image clustering' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "iven an object model and a black-box measure of similarity between the model and candidate targets, we consider visual object tracking as a numerical optimization problem. During normal tracking conditions when the object is visible from frame to frame, local optimization is used to track the local mode of the similarity measure in a parameter space of translation, rotation and scale. However, when the object becomes partially or totally occluded, such local tracking is prone to failure, especially when common prediction techniques like the Kalman filter do not provide a good estimate of object parameters in future frames. To recover from these inevitable tracking failures, we consider object detection as a global optimization problem and solve it via Adaptive Simulated Annealing (ASA), a method that avoids becoming trapped at local modes and is much faster than exhaustive search. As a Monte Carlo approach, ASA stochastically samples the parameter space, in contrast to local deterministic search. We apply cluster analysis on the sampled parameter space to redetect the object and renew the local tracker. Our numerical hybrid local and global mode-seeking tracker is validated on challenging airborne videos with heavy occlusion and large camera motions. Our approach outperforms state-of-the-art trackers on the VIVID benchmark datasets.", "output": "The abstract text contains these key words below: 'object model' whose type is Method, 'black-box measure of similarity' whose type is Metric, 'model' whose type is Generic, 'visual object tracking' whose type is Task, 'numerical optimization problem' whose type is Task, 'local optimization' whose type is Method, 'local mode of the similarity measure' whose type is OtherScientificTerm, 'parameter space of translation, rotation and scale' whose type is OtherScientificTerm, 'local tracking' whose type is Method, 'prediction techniques' whose type is Method, 'Kalman filter' whose type is Method, 'object detection' whose type is Task, 'global optimization problem' whose type is Task, 'it' whose type is Generic, 'Adaptive Simulated Annealing (ASA)' whose type is Method, 'method' whose type is Generic, 'exhaustive search' whose type is Method, 'Monte Carlo approach' whose type is Method, 'ASA' whose type is Method, 'parameter space' whose type is OtherScientificTerm, 'local deterministic search' whose type is Method, 'cluster analysis' whose type is Method, 'sampled parameter space' whose type is OtherScientificTerm, 'local tracker' whose type is Method, 'numerical hybrid local and global mode-seeking tracker' whose type is Method, 'airborne videos' whose type is Material, 'heavy occlusion' whose type is OtherScientificTerm, 'camera motions' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'state-of-the-art trackers' whose type is Generic, 'VIVID benchmark datasets' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Robust  natural language interpretation  requires strong  semantic domain models ,  fail-soft recovery heuristics , and very flexible  control structures . Although  single-strategy parsers  have met with a measure of success, a  multi-strategy approach  is shown to provide a much higher degree of flexibility, redundancy, and ability to bring  task-specific domain knowledge  (in addition to  general linguistic knowledge ) to bear on both  grammatical and ungrammatical input . A  parsing algorithm  is presented that integrates several different  parsing strategies , with  case-frame instantiation  dominating. Each of these  parsing strategies  exploits different  types of knowledge ; and their combination provides a strong framework in which to process  conjunctions ,  fragmentary input , and  ungrammatical structures , as well as less exotic,  grammatically correct input . Several  specific heuristics  for handling  ungrammatical input  are presented within this  multi-strategy framework . ", "output": "The abstract text contains these key words below: 'natural language interpretation' whose type is Task, 'semantic domain models' whose type is Method, 'fail-soft recovery heuristics' whose type is Method, 'control structures' whose type is OtherScientificTerm, 'single-strategy parsers' whose type is Method, 'multi-strategy approach' whose type is Method, 'task-specific domain knowledge' whose type is OtherScientificTerm, 'general linguistic knowledge' whose type is OtherScientificTerm, 'grammatical and ungrammatical input' whose type is OtherScientificTerm, 'parsing algorithm' whose type is Method, 'parsing strategies' whose type is Method, 'case-frame instantiation' whose type is OtherScientificTerm, 'parsing strategies' whose type is Method, 'conjunctions' whose type is OtherScientificTerm, 'fragmentary input' whose type is OtherScientificTerm, 'ungrammatical structures' whose type is OtherScientificTerm, 'exotic,  grammatically correct input' whose type is OtherScientificTerm, 'specific heuristics' whose type is Method, 'ungrammatical input' whose type is OtherScientificTerm, 'multi-strategy framework' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Taiwan Child Language Corpus  contains  scripts  transcribed from about 330 hours of  recordings  of fourteen young children from  Southern Min Chinese  speaking families in Taiwan. The format of the  corpus  adopts the  Child Language Data Exchange System (CHILDES) . The size of the  corpus  is about 1.6 million  words . In this paper, we describe  data collection ,  transcription ,  word segmentation , and  part-of-speech annotation  of this  corpus . Applications of the  corpus  are also discussed.\n", "output": "The abstract text contains these key words below: 'Taiwan Child Language Corpus' whose type is Material, 'corpus' whose type is Generic, 'Child Language Data Exchange System (CHILDES)' whose type is Material, 'corpus' whose type is Generic, 'data collection' whose type is Task, 'transcription' whose type is Task, 'word segmentation' whose type is Task, 'part-of-speech annotation' whose type is Task, 'corpus' whose type is Generic, 'corpus' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We propose a draft scheme of the  model  formalizing the  structure of communicative context  in  dialogue interaction . The relationships between the interacting partners are considered as system of three automata representing the partners of the  dialogue  and environment.\n", "output": "The abstract text contains these key words below: 'model' whose type is Generic, 'structure of communicative context' whose type is OtherScientificTerm, 'dialogue interaction' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present an efficient algorithm for the  redundancy elimination problem  : Given an  underspecified semantic representation (USR)  of a  scope ambiguity  , compute an  USR  with fewer mutually  equivalent readings  . The algorithm operates on  underspecified chart representations  which are derived from  dominance graphs  ; it can be applied to the  USRs  computed by  large-scale grammars  . We evaluate the algorithm on a  corpus  , and show that it reduces the degree of  ambiguity  significantly while taking negligible runtime. ", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'redundancy elimination problem' whose type is Task, 'underspecified semantic representation (USR)' whose type is Method, 'scope ambiguity' whose type is OtherScientificTerm, 'USR' whose type is Method, 'equivalent readings' whose type is Material, 'algorithm' whose type is Generic, 'underspecified chart representations' whose type is Method, 'dominance graphs' whose type is OtherScientificTerm, 'it' whose type is Generic, 'USRs' whose type is Method, 'large-scale grammars' whose type is Method, 'algorithm' whose type is Generic, 'it' whose type is Generic, 'degree of  ambiguity' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we compare two competing approaches to  part-of-speech tagging ,  statistical and constraint-based disambiguation , using  French  as our  test language . We imposed a time limit on our experiment: the amount of time spent on the design of our  constraint system  was about the same as the time we used to train and test the easy-to-implement  statistical model . We describe the two systems and compare the results. The  accuracy  of the  statistical method  is reasonably good, comparable to  taggers  for  English . But the  constraint-based tagger  seems to be superior even with the limited time we allowed ourselves for  rule development .\n", "output": "The abstract text contains these key words below: 'approaches' whose type is Generic, 'part-of-speech tagging' whose type is Task, 'statistical and constraint-based disambiguation' whose type is Task, 'French' whose type is Material, 'constraint system' whose type is Generic, 'statistical model' whose type is Method, 'systems' whose type is Generic, 'accuracy' whose type is Metric, 'statistical method' whose type is Method, 'taggers' whose type is Method, 'English' whose type is Material, 'constraint-based tagger' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Instances of a  word  drawn from different  domains  may have different  sense priors  (the proportions of the different  senses  of a  word ). This in turn affects the accuracy of  word sense disambiguation (WSD) systems  trained and applied on different  domains . This paper presents a method to estimate the  sense priors  of  words  drawn from a new  domain , and highlights the importance of using  well calibrated probabilities  when performing these  estimations . By using  well calibrated probabilities , we are able to estimate the  sense priors  effectively to achieve significant improvements in  WSD accuracy .\n", "output": "The abstract text contains these key words below: 'sense priors' whose type is OtherScientificTerm, 'accuracy' whose type is Metric, 'word sense disambiguation (WSD) systems' whose type is Method, 'method' whose type is Generic, 'sense priors  of  words' whose type is OtherScientificTerm, 'new  domain' whose type is Material, 'well calibrated probabilities' whose type is OtherScientificTerm, 'estimations' whose type is Generic, 'well calibrated probabilities' whose type is OtherScientificTerm, 'sense priors' whose type is OtherScientificTerm, 'WSD accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e study the question of how to make loss-aware predictions in image segmentation settings where the evaluation function is the Intersection-over-Union (IoU) measure that is used widely in evaluating image segmentation systems. Currently, there are two dominant approaches: the first approximates the Expected-IoU (EIoU) score as Expected-Intersection-over-Expected-Union (EIoEU); and the second approach is to compute exact EIoU but only over a small set of high-quality candidate solutions. We begin by asking which approach we should favor for two typical image seg-mentation tasks. Studying this question leads to two new methods that draw ideas from both existing approaches. Our new methods use the EIoEU approximation paired with high quality candidate solutions. Experimentally we show that our new approaches lead to improved performance on both image segmentation tasks.", "output": "The abstract text contains these key words below: 'loss-aware predictions' whose type is Task, 'loss-aware predictions in image segmentation settings' whose type is Task, 'image segmentation settings' whose type is Task, 'evaluation function' whose type is OtherScientificTerm, 'Intersection-over-Union (IoU) measure' whose type is Metric, 'image segmentation systems' whose type is Method, 'dominant approaches' whose type is Generic, 'first' whose type is Generic, 'Expected-IoU (EIoU) score' whose type is OtherScientificTerm, 'Expected-Intersection-over-Expected-Union (EIoEU)' whose type is OtherScientificTerm, 'second approach' whose type is Generic, 'EIoU' whose type is OtherScientificTerm, 'image seg-mentation tasks' whose type is Task, 'methods' whose type is Generic, 'EIoEU' whose type is OtherScientificTerm, 'EIoEU approximation' whose type is Method, 'approaches' whose type is Generic, 'image segmentation tasks' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper proposes an automatic, essentially  domain-independent means of evaluating Spoken Language Systems (SLS)  which combines  software  we have developed for that purpose (the \" Comparator \") and a set of  specifications  for  answer expressions  (the \" Common Answer Specification \", or  CAS ). The  Comparator  checks whether the answer provided by a  SLS  accords with a  canonical answer , returning either true or false. The  Common Answer Specification  determines the  syntax  of  answer expressions , the minimal  content  that must be included in them, the  data  to be included in and excluded from  test corpora , and the  procedures  used by the  Comparator . Though some details of the  CAS  are particular to individual  domains , the  Comparator software  is  domain-independent , as is the  CAS approach .\n", "output": "The abstract text contains these key words below: 'domain-independent means of evaluating Spoken Language Systems (SLS)' whose type is Method, 'software' whose type is Generic, 'Comparator' whose type is Method, 'specifications' whose type is Generic, 'answer expressions' whose type is OtherScientificTerm, 'Common Answer Specification' whose type is Method, 'CAS' whose type is Method, 'Comparator' whose type is Method, 'SLS' whose type is Method, 'Common Answer Specification' whose type is Method, 'syntax  of  answer expressions' whose type is OtherScientificTerm, 'Comparator' whose type is Method, 'CAS' whose type is OtherScientificTerm, 'Comparator software' whose type is Method, 'CAS approach' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we present a  corpus-based supervised word sense disambiguation (WSD) system  for  Dutch  which combines  statistical classification  ( maximum entropy ) with  linguistic information . Instead of building individual  classifiers  per  ambiguous wordform , we introduce a  lemma-based approach . The advantage of this novel method is that it clusters all  inflected forms  of an  ambiguous word  in one  classifier , therefore augmenting the  training material  available to the  algorithm . Testing the  lemma-based model  on the  Dutch Senseval-2 test data , we achieve a significant increase in  accuracy  over the  wordform model . Also, the  WSD system based on lemmas  is smaller and more robust.\n", "output": "The abstract text contains these key words below: 'corpus-based supervised word sense disambiguation (WSD) system' whose type is Method, 'Dutch' whose type is Material, 'statistical classification' whose type is Method, 'maximum entropy' whose type is Method, 'linguistic information' whose type is OtherScientificTerm, 'classifiers' whose type is Method, 'ambiguous wordform' whose type is OtherScientificTerm, 'lemma-based approach' whose type is Method, 'method' whose type is Generic, 'it' whose type is Generic, 'inflected forms' whose type is OtherScientificTerm, 'ambiguous word' whose type is OtherScientificTerm, 'classifier' whose type is Method, 'algorithm' whose type is Generic, 'lemma-based model' whose type is Method, 'Dutch Senseval-2 test data' whose type is Material, 'wordform model' whose type is Method, 'WSD system based on lemmas' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper reports on two contributions to  large vocabulary continuous speech recognition  . First, we present a new paradigm for  speaker-independent (SI) training  of  hidden Markov models (HMM)  , which uses a large amount of  speech  from a few  speakers  instead of the traditional practice of using a little  speech  from many  speakers  . In addition, combination of the  training speakers  is done by averaging the  statistics  of  independently trained models  rather than the usual pooling of all the  speech data  from many  speakers  prior to  training  . With only 12  training speakers  for  SI recognition  , we achieved a 7.5%  word error rate  on a standard  grammar  and  test set  from the  DARPA Resource Management corpus  . This  performance  is comparable to our best condition for this test suite, using 109  training speakers  . Second, we show a significant improvement for  speaker adaptation (SA)  using the new  SI corpus  and a small amount of  speech  from the new (target)  speaker  . A  probabilistic spectral mapping  is estimated independently for each  training (reference) speaker  and the  target speaker  . Each  reference model  is transformed to the  space  of the  target speaker  and combined by  averaging  . Using only 40  utterances  from the  target speaker  for  adaptation  , the  error rate  dropped to 4.1% --- a 45% reduction in error compared to the  SI  result. ", "output": "The abstract text contains these key words below: 'large vocabulary continuous speech recognition' whose type is Task, 'speaker-independent (SI) training  of  hidden Markov models (HMM)' whose type is Task, 'speech' whose type is Material, 'averaging the  statistics  of  independently trained models' whose type is OtherScientificTerm, 'pooling of all the  speech data' whose type is OtherScientificTerm, 'SI recognition' whose type is Task, 'word error rate' whose type is Metric, 'DARPA Resource Management corpus' whose type is Material, 'speaker adaptation (SA)' whose type is Task, 'SI corpus' whose type is Material, 'probabilistic spectral mapping' whose type is Method, 'reference model' whose type is Method, 'adaptation' whose type is Task, 'error rate' whose type is Metric, 'SI' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n Soames 1979 provides some counterexamples to the  theory of natural language presuppositions  that is presented in Gazdar 1979. Soames 1982 provides a theory which explains these counterexamples. Mercer 1987 rejects the solution found in Soames 1982 leaving these counterexamples unexplained. By reappraising these insightful counterexamples, the  inferential theory for natural language presuppositions  described in Mercer 1987, 1988 gives a simple and straightforward explanation for the  presuppositional nature  of these  sentences  . ", "output": "The abstract text contains these key words below: 'natural language presuppositions' whose type is Task, 'natural language presuppositions' whose type is Task, 'presuppositional nature' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Following recent developments in the  automatic evaluation  of  machine translation  and  document summarization  , we present a similar approach, implemented in a measure called  POURPRE  , for  automatically evaluating answers to definition questions  . Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for  scoring system output  is an impediment to progress in the field, which we address with this work. Experiments with the  TREC 2003 and TREC 2004 QA tracks  indicate that  rankings  produced by our metric correlate highly with  official rankings  , and that  POURPRE  outperforms direct application of existing metrics. ", "output": "The abstract text contains these key words below: 'automatic evaluation' whose type is Task, 'automatic evaluation  of  machine translation  and  document summarization' whose type is Task, 'machine translation' whose type is Task, 'document summarization' whose type is Task, 'approach' whose type is Generic, 'measure' whose type is Generic, 'POURPRE' whose type is Metric, 'automatically evaluating answers to definition questions' whose type is Task, 'TREC 2003 and TREC 2004 QA tracks' whose type is Material, 'rankings' whose type is OtherScientificTerm, 'metric' whose type is Generic, 'official rankings' whose type is OtherScientificTerm, 'POURPRE' whose type is Metric, 'metrics' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n In this paper we study a set of problems that are of considerable importance to  Statistical Machine Translation (SMT)  but which have not been addressed satisfactorily by the  SMT research community  . Over the last decade, a variety of  SMT algorithms  have been built and empirically tested whereas little is known about the  computational complexity  of some of the fundamental problems of  SMT  . Our work aims at providing useful insights into the the  computational complexity  of those problems. We prove that while  IBM Models 1-2  are conceptually and computationally simple, computations involving the higher (and more useful)  models  are  hard  . Since it is unlikely that there exists a  polynomial time solution  for any of these  hard problems  (unless  P = NP  and  P#P = P  ), our results highlight and justify the need for developing  polynomial time approximations  for these computations. We also discuss some practical ways of dealing with  complexity  . ", "output": "The abstract text contains these key words below: 'Statistical Machine Translation (SMT)' whose type is Task, 'SMT algorithms' whose type is Method, 'computational complexity' whose type is Metric, 'problems' whose type is Generic, 'SMT' whose type is Task, 'computational complexity' whose type is Metric, 'problems' whose type is Generic, 'IBM Models 1-2' whose type is Method, 'computations' whose type is Generic, 'models' whose type is Generic, 'polynomial time solution' whose type is Method, 'hard problems' whose type is Generic, 'polynomial time approximations' whose type is Method, 'computations' whose type is Generic, 'complexity' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ackground modeling is an important component of many vision systems. Existing work in the area has mostly addressed scenes that consist of static or quasi-static structures. When the scene exhibits a persistent dynamic behavior in time, such an assumption is violated and detection performance deteriorates. In this paper, we propose a new method for the modeling and subtraction of such scenes. Towards the modeling of the dynamic characteristics, optical flow is computed and utilized as a feature in a higher dimensional space. Inherent ambiguities in the computation of features are addressed by using a data-dependent bandwidth for density estimation using kernels. Extensive experiments demonstrate the utility and performance of the proposed approach.", "output": "The abstract text contains these key words below: 'Background modeling' whose type is Task, 'vision systems' whose type is Task, 'static or quasi-static structures' whose type is OtherScientificTerm, 'scene' whose type is Generic, 'persistent dynamic behavior' whose type is OtherScientificTerm, 'detection' whose type is Task, 'method' whose type is Generic, 'modeling and subtraction of such scenes' whose type is Task, 'scenes' whose type is Generic, 'modeling of the dynamic characteristics' whose type is Task, 'optical flow' whose type is OtherScientificTerm, 'feature' whose type is OtherScientificTerm, 'higher dimensional space' whose type is OtherScientificTerm, 'ambiguities' whose type is OtherScientificTerm, 'computation of features' whose type is Task, 'data-dependent bandwidth' whose type is OtherScientificTerm, 'density estimation' whose type is Task, 'kernels' whose type is Method, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a framework for the fast  computation  of  lexical affinity models . The framework is composed of a novel algorithm to efficiently compute the  co-occurrence distribution  between pairs of  terms , an  independence model , and a  parametric affinity model . In comparison with previous  models , which either use arbitrary windows to compute  similarity  between  words  or use  lexical affinity  to create  sequential models , in this paper we focus on  models  intended to capture the  co-occurrence patterns  of any pair of  words  or  phrases  at any distance in the  corpus . The framework is flexible, allowing fast  adaptation  to  applications  and it is scalable. We apply it in combination with a  terabyte corpus  to answer  natural language tests , achieving encouraging results.\n", "output": "The abstract text contains these key words below: 'framework' whose type is Generic, 'fast  computation  of  lexical affinity models' whose type is Task, 'framework' whose type is Generic, 'algorithm' whose type is Generic, 'co-occurrence distribution' whose type is OtherScientificTerm, 'independence model' whose type is Method, 'parametric affinity model' whose type is Method, 'models' whose type is Generic, 'similarity' whose type is OtherScientificTerm, 'lexical affinity' whose type is OtherScientificTerm, 'sequential models' whose type is Method, 'models' whose type is Generic, 'co-occurrence patterns' whose type is OtherScientificTerm, 'framework' whose type is Generic, 'it' whose type is Generic, 'terabyte corpus' whose type is Material, 'natural language tests' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents a novel  ensemble learning approach  to resolving  German pronouns .  Boosting , the method in question, combines the moderately accurate  hypotheses  of several  classifiers  to form a highly accurate one. Experiments show that this approach is superior to a single  decision-tree classifier . Furthermore, we present a  standalone system  that resolves  pronouns  in  unannotated text  by using a fully automatic sequence of  preprocessing modules  that mimics the  manual annotation process . Although the system performs well within a limited  textual domain , further research is needed to make it effective for  open-domain question answering  and  text summarisation .\n", "output": "The abstract text contains these key words below: 'ensemble learning approach' whose type is Method, 'German pronouns' whose type is OtherScientificTerm, 'Boosting' whose type is Method, 'classifiers' whose type is Method, 'approach' whose type is Generic, 'decision-tree classifier' whose type is Method, 'standalone system' whose type is Method, 'pronouns' whose type is OtherScientificTerm, 'unannotated text' whose type is Material, 'preprocessing modules' whose type is Method, 'manual annotation process' whose type is Task, 'system' whose type is Generic, 'textual domain' whose type is Material, 'it' whose type is Generic, 'open-domain question answering' whose type is Task, 'text summarisation' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "achine reading is a relatively new field that features computer programs designed to read flowing text and extract fact assertions expressed by the narrative content. This task involves two core technologies: natural language processing (NLP) and information extraction (IE). In this paper we describe a machine reading system that we have developed within a cognitive architecture. We show how we have integrated into the framework several levels of knowledge for a particular domain , ideas from cognitive semantics and construction grammar, plus tools from prior NLP and IE research. The result is a system that is capable of reading and interpreting complex and fairly idiosyncratic texts in the family history domain. We describe the architecture and performance of the system. After presenting the results from several evaluations that we have carried out, we summarize possible future directions.", "output": "The abstract text contains these key words below: 'Machine reading' whose type is Task, 'field' whose type is Generic, 'computer programs' whose type is Method, 'flowing text' whose type is Material, 'fact assertions' whose type is OtherScientificTerm, 'narrative content' whose type is Material, 'task' whose type is Generic, 'natural language processing (NLP)' whose type is Method, 'information extraction (IE)' whose type is Task, 'machine reading system' whose type is Task, 'cognitive architecture' whose type is Method, 'cognitive semantics' whose type is OtherScientificTerm, 'construction grammar' whose type is Method, 'prior NLP' whose type is Task, 'IE research' whose type is Task, 'system' whose type is Generic, 'idiosyncratic texts' whose type is Material, 'family history domain' whose type is Material, 'system' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper proposes an  annotating scheme  that encodes  honorifics  (respectful words).  Honorifics  are used extensively in  Japanese  , reflecting the social relationship (e.g. social ranks and age) of the  referents  . This  referential information  is vital for resolving  zero pronouns  and improving  machine translation outputs  . Annotating  honorifics  is a complex task that involves identifying a  predicate  with  honorifics  , assigning  ranks  to  referents  of the  predicate  , calibrating the  ranks  , and connecting  referents  with their  predicates  . ", "output": "The abstract text contains these key words below: 'annotating scheme' whose type is Method, 'honorifics' whose type is Material, 'respectful words' whose type is OtherScientificTerm, 'Honorifics' whose type is Material, 'Japanese' whose type is Material, 'referential information' whose type is OtherScientificTerm, 'zero pronouns' whose type is OtherScientificTerm, 'machine translation outputs' whose type is OtherScientificTerm, 'honorifics' whose type is Material, 'honorifics' whose type is Material, 'ranks' whose type is OtherScientificTerm, 'ranks' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper proposes a series of modifications to the  left corner parsing algorithm  for  context-free grammars  . It is argued that the resulting algorithm is both efficient and flexible and is, therefore, a good choice for the  parser  used in a  natural language interface  . ", "output": "The abstract text contains these key words below: 'left corner parsing algorithm' whose type is Method, 'context-free grammars' whose type is Method, 'algorithm' whose type is Generic, 'parser' whose type is Method, 'natural language interface' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "raph-cuts optimization is prevalent in vision and graphics problems. It is thus of great practical importance to parallelize the graph-cuts optimization using to-day's ubiquitous multi-core machines. However, the current best serial algorithm by Boykov and Kolmogorov [4] (called the BK algorithm) still has the superior empirical performance. It is non-trivial to parallelize as expensive synchronization overhead easily offsets the advantage of parallelism. In this paper, we propose a novel adaptive bottom-up approach to parallelize the BK algorithm. We first uniformly partition the graph into a number of regularly-shaped dis-joint subgraphs and process them in parallel, then we incre-mentally merge the subgraphs in an adaptive way to obtain the global optimum. The new algorithm has three benefits: 1) it is more cache-friendly within smaller subgraphs; 2) it keeps balanced workloads among computing cores; 3) it causes little overhead and is adaptable to the number of available cores. Extensive experiments in common applications such as 2D/3D image segmentations and 3D surface fitting demonstrate the effectiveness of our approach.", "output": "The abstract text contains these key words below: 'Graph-cuts optimization' whose type is Method, 'vision and graphics problems' whose type is Task, 'graph-cuts optimization' whose type is Method, 'multi-core machines' whose type is OtherScientificTerm, 'serial algorithm' whose type is Method, 'BK algorithm' whose type is Method, 'synchronization overhead' whose type is OtherScientificTerm, 'parallelism' whose type is OtherScientificTerm, 'adaptive bottom-up approach' whose type is Method, 'BK algorithm' whose type is Method, 'graph' whose type is OtherScientificTerm, 'regularly-shaped dis-joint subgraphs' whose type is OtherScientificTerm, 'them' whose type is Generic, 'subgraphs' whose type is OtherScientificTerm, 'global optimum' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'subgraphs' whose type is OtherScientificTerm, 'balanced workloads' whose type is OtherScientificTerm, 'overhead' whose type is OtherScientificTerm, 'applications' whose type is Generic, '2D/3D image segmentations' whose type is Task, '3D surface fitting' whose type is Task, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "any computer vision applications, such as image classification and video indexing, are usually multi-label classification problems in which an instance can be assigned to more than one category. In this paper, we present a novel multi-label classification approach with hypergraph regu-larization that addresses the correlations among different categories. First, a hypergraph is constructed to capture the correlations among different categories, in which each vertex represents one training instance and each hyperedge for one category contains all the instances belonging to the same category. Then, an improved SVM like learning system incorporating the hypergraph regularization, called Rank-HLapSVM, is proposed to handle the multi-label classification problems. We find that the corresponding optimization problem can be efficiently solved by the dual coordinate descent method. Many promising experimental results on the real datasets including ImageCLEF and Me-diaMill demonstrate the effectiveness and efficiency of the proposed algorithm.", "output": "The abstract text contains these key words below: 'computer vision applications' whose type is Task, 'image classification' whose type is Task, 'video indexing' whose type is Task, 'multi-label classification problems' whose type is Task, 'multi-label classification approach' whose type is Method, 'hypergraph regu-larization' whose type is OtherScientificTerm, 'hypergraph' whose type is OtherScientificTerm, 'SVM like learning system' whose type is Method, 'hypergraph regularization' whose type is OtherScientificTerm, 'Rank-HLapSVM' whose type is Method, 'multi-label classification problems' whose type is Task, 'optimization problem' whose type is Task, 'dual coordinate descent method' whose type is Method, 'real datasets' whose type is Generic, 'ImageCLEF' whose type is Material, 'Me-diaMill' whose type is Material, 'algorithm' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n An attempt has been made to use an  Augmented Transition Network  as a procedural  dialog model  . The development of such a  model  appears to be important in several respects: as a device to represent and to use different  dialog schemata  proposed in empirical  conversation analysis  ; as a device to represent and to use  models of verbal interaction  ; as a device combining knowledge about  dialog schemata  and about  verbal interaction  with knowledge about  task-oriented and goal-directed dialogs  . A standard  ATN  should be further developed in order to account for the  verbal interactions  of  task-oriented dialogs  . ", "output": "The abstract text contains these key words below: 'Augmented Transition Network' whose type is Method, 'dialog model' whose type is Method, 'model' whose type is Generic, 'device' whose type is Generic, 'dialog schemata' whose type is OtherScientificTerm, 'conversation analysis' whose type is Method, 'device' whose type is Generic, 'models' whose type is Generic, 'verbal interaction' whose type is OtherScientificTerm, 'device' whose type is Generic, 'dialog schemata' whose type is OtherScientificTerm, 'verbal interaction' whose type is OtherScientificTerm, 'task-oriented and goal-directed dialogs' whose type is Material, 'ATN' whose type is Method, 'verbal interactions' whose type is OtherScientificTerm, 'task-oriented dialogs' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "mage matching is a fundamental problem in Computer Vision. In the context of feature-based matching, SIFT and its variants have long excelled in a wide array of applications. However, for ultra-wide baselines, as in the case of aerial images captured under large camera rotations, the appearance variation goes beyond the reach of SIFT and RANSAC. In this paper we propose a data-driven, deep learning-based approach that sidesteps local correspondence by framing the problem as a classification task. Furthermore , we demonstrate that local correspondences can still be useful. To do so we incorporate an attention mechanism to produce a set of probable matches, which allows us to further increase performance. We train our models on a dataset of urban aerial imagery consisting of 'same' and 'different' pairs, collected for this purpose, and characterize the problem via a human study with annotations from Amazon Mechanical Turk. We demonstrate that our models outperform the state-of-the-art on ultra-wide baseline matching and approach human accuracy.", "output": "The abstract text contains these key words below: 'Image matching' whose type is Task, 'Computer Vision' whose type is Task, 'feature-based matching' whose type is Task, 'SIFT' whose type is Method, 'ultra-wide baselines' whose type is OtherScientificTerm, 'aerial images' whose type is Material, 'large camera rotations' whose type is OtherScientificTerm, 'appearance variation' whose type is OtherScientificTerm, 'SIFT' whose type is Method, 'RANSAC' whose type is Method, 'data-driven, deep learning-based approach' whose type is Method, 'local correspondence' whose type is OtherScientificTerm, 'problem' whose type is Generic, 'classification task' whose type is Task, 'local correspondences' whose type is OtherScientificTerm, 'attention mechanism' whose type is Method, 'models' whose type is Generic, 'dataset of urban aerial imagery' whose type is Material, 'problem' whose type is Generic, 'human study' whose type is Method, 'annotations from Amazon Mechanical Turk' whose type is Material, 'models' whose type is Generic, 'state-of-the-art' whose type is Generic, 'ultra-wide baseline matching' whose type is Task, 'human accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he features based on Markov random field (MRF) models are usually sensitive to the rotation of image textures. This paper develops an anisotropic circular Gaussian MRF (ACGMRF) model for modelling rotated image textures and retrieving rotation-invariant texture features. To overcome the singularity problem of the least squares estimate (LSE) method, an approximate least squares estimate (ALSE) method is proposed to estimate the parameters of the ACGMRF model. The rotation-invariant features can be obtained from the parameters of the ACGMRF model by the one-dimensional (1-D) discrete Fourier transform (DFT). Significantly improved accuracy can be achieved by applying the rotation-invariant features to classify SAR (synthetic aperture radar) sea ice and Brodatz imagery.", "output": "The abstract text contains these key words below: 'features' whose type is OtherScientificTerm, 'Markov random field (MRF) models' whose type is Method, 'rotation of image textures' whose type is OtherScientificTerm, 'anisotropic circular Gaussian MRF (ACGMRF) model' whose type is Method, 'modelling rotated image textures' whose type is Task, 'retrieving rotation-invariant texture features' whose type is Task, 'singularity problem' whose type is OtherScientificTerm, 'least squares estimate (LSE) method' whose type is Method, 'approximate least squares estimate (ALSE) method' whose type is Method, 'parameters of the ACGMRF model' whose type is OtherScientificTerm, 'ACGMRF model' whose type is Method, 'rotation-invariant features' whose type is OtherScientificTerm, 'parameters of the ACGMRF model' whose type is OtherScientificTerm, 'ACGMRF model' whose type is Method, 'one-dimensional (1-D) discrete Fourier transform (DFT)' whose type is Method, 'accuracy' whose type is Metric, 'rotation-invariant features' whose type is OtherScientificTerm, 'SAR (synthetic aperture radar' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We have implemented a  restricted domain parser  called  Plume . Building on previous work at Carnegie-Mellon University e.g. [4, 5, 8],  Plume's approach to parsing  is based on  semantic caseframe instantiation . This has the advantages of  efficiency  on  grammatical input , and  robustness  in the face of  ungrammatical input . While  Plume  is well adapted to simple  declarative and imperative utterances , it handles  passives ,  relative clauses  and  interrogatives  in an ad hoc manner leading to patchy  syntactic coverage . This paper outlines  Plume  as it currently exists and describes our detailed design for extending  Plume  to handle  passives ,  relative clauses , and  interrogatives  in a general manner. ", "output": "The abstract text contains these key words below: 'restricted domain parser' whose type is Method, 'Plume' whose type is Method, 'Plume's approach' whose type is Method, 'parsing' whose type is Task, 'semantic caseframe instantiation' whose type is OtherScientificTerm, 'grammatical input' whose type is OtherScientificTerm, 'robustness' whose type is Metric, 'ungrammatical input' whose type is OtherScientificTerm, 'Plume' whose type is Method, 'declarative and imperative utterances' whose type is Material, 'it' whose type is Generic, 'passives' whose type is OtherScientificTerm, 'relative clauses' whose type is OtherScientificTerm, 'interrogatives' whose type is OtherScientificTerm, 'patchy  syntactic coverage' whose type is OtherScientificTerm, 'Plume' whose type is Method, 'it' whose type is Generic, 'Plume' whose type is Method, 'passives' whose type is OtherScientificTerm, 'relative clauses' whose type is OtherScientificTerm, 'interrogatives' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present  Minimum Bayes-Risk (MBR) decoding  for  statistical machine translation . This statistical approach aims to minimize  expected loss  of  translation errors  under  loss functions  that measure  translation performance . We describe a hierarchy of  loss functions  that incorporate different levels of  linguistic information  from  word strings ,  word-to-word alignments  from an  MT system , and  syntactic structure  from  parse-trees  of  source and target language sentences . We report the  performance  of the  MBR decoders  on a  Chinese-to-English translation task . Our results show that  MBR decoding  can be used to tune  statistical MT performance  for specific  loss functions .\n", "output": "The abstract text contains these key words below: 'Minimum Bayes-Risk (MBR) decoding' whose type is Method, 'statistical machine translation' whose type is Task, 'statistical approach' whose type is Method, 'expected loss' whose type is OtherScientificTerm, 'translation errors' whose type is OtherScientificTerm, 'loss functions' whose type is OtherScientificTerm, 'translation' whose type is Task, 'loss functions' whose type is OtherScientificTerm, 'linguistic information' whose type is OtherScientificTerm, 'word-to-word alignments' whose type is OtherScientificTerm, 'MT system' whose type is Method, 'syntactic structure' whose type is OtherScientificTerm, 'parse-trees' whose type is OtherScientificTerm, 'MBR decoders' whose type is Method, 'Chinese-to-English translation task' whose type is Task, 'MBR decoding' whose type is Method, 'statistical MT' whose type is Method, 'loss functions' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Automatic summarization  and  information extraction  are two important Internet services.  MUC  and  SUMMAC  play their appropriate roles in the next generation Internet. This paper focuses on the  automatic summarization  and proposes two different models to extract  sentences  for  summary generation  under two tasks initiated by  SUMMAC-1 . For  categorization task ,  positive feature vectors  and  negative feature vectors  are used cooperatively to construct generic, indicative  summaries . For adhoc task, a  text model  based on relationship between  nouns  and  verbs  is used to filter out irrelevant  discourse segment , to rank relevant  sentences , and to generate the  user-directed summaries . The result shows that the  NormF  of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0. 447. The  NormF  of the best summary and that of the fixed summary for  categorization task  are 0.4090 and 0.4023. Our system outperforms the average system in  categorization task  but does a common job in adhoc task.\n", "output": "The abstract text contains these key words below: 'Automatic summarization' whose type is Task, 'information extraction' whose type is Task, 'MUC' whose type is Material, 'SUMMAC' whose type is Material, 'automatic summarization' whose type is Task, 'models' whose type is Generic, 'summary generation' whose type is Task, 'tasks' whose type is Generic, 'SUMMAC-1' whose type is Material, 'categorization task' whose type is Task, 'positive feature vectors' whose type is OtherScientificTerm, 'negative feature vectors' whose type is OtherScientificTerm, 'generic, indicative  summaries' whose type is OtherScientificTerm, 'adhoc task' whose type is Task, 'text model' whose type is Method, 'discourse segment' whose type is OtherScientificTerm, 'user-directed summaries' whose type is OtherScientificTerm, 'NormF' whose type is Metric, 'adhoc tasks' whose type is Task, 'NormF' whose type is Metric, 'categorization task' whose type is Task, 'system' whose type is Generic, 'system' whose type is Generic, 'categorization task' whose type is Task, 'adhoc task' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " bio-inspired model for an analog programmable array processor (APAP), based on studies on the vertebrate retina, has permitted the realization of complex programmable spatio-temporal dynamics in VLSI. This model mimics the way in which images are processed in the visual pathway, rendering a feasible alternative for the implementation of early vision applications in standard technologies. A prototype chip has been designed and fabricated in a 0.5\u00b5m standard CMOS process. Computing power per area and power consumption is amongst the highest reported for a single chip. Design challenges, trade-offs and some experimental results are presented in this paper.", "output": "The abstract text contains these key words below: 'bio-inspired model' whose type is Method, 'analog programmable array processor (APAP)' whose type is Task, 'vertebrate retina' whose type is OtherScientificTerm, 'complex programmable spatio-temporal dynamics' whose type is OtherScientificTerm, 'VLSI' whose type is Task, 'model' whose type is Generic, 'images' whose type is Material, 'visual pathway' whose type is Method, 'vision applications' whose type is Task, 'prototype chip' whose type is OtherScientificTerm, 'CMOS process' whose type is OtherScientificTerm, 'Computing power per area' whose type is Metric, 'power consumption' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper describes three relatively  domain-independent capabilities  recently added to the  Paramax spoken language understanding system  :  non-monotonic reasoning  ,  implicit reference resolution  , and  database query paraphrase  . In addition, we discuss the results of the  February 1992 ATIS benchmark tests  . We describe a variation on the  standard evaluation metric  which provides a more tightly controlled measure of progress. Finally, we briefly describe an experiment which we have done in extending the  n-best speech/language integration architecture  to improving  OCR   accuracy  . ", "output": "The abstract text contains these key words below: 'domain-independent capabilities' whose type is Generic, 'Paramax spoken language understanding system' whose type is Method, 'non-monotonic reasoning' whose type is Task, 'implicit reference resolution' whose type is Task, 'database query paraphrase' whose type is Task, 'February 1992 ATIS benchmark tests' whose type is Material, 'n-best speech/language integration architecture' whose type is Method, 'OCR   accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\nWhen people use  natural language  in natural settings, they often use it ungrammatically, missing out or repeating words, breaking-off and restarting, speaking in fragments, etc.. Their  human listeners  are usually able to cope with these deviations with little difficulty. If a  computer system  wishes to accept  natural language input  from its  users  on a routine basis, it must display a similar indifference. In this paper, we outline a set of  parsing flexibilities  that such a system should provide. We go, on to describe  FlexP  , a  bottom-up pattern-matching parser  that we have designed and implemented to provide these flexibilities for  restricted natural language  input to a limited-domain computer system. ", "output": "The abstract text contains these key words below: 'natural language' whose type is OtherScientificTerm, 'it' whose type is Generic, 'computer system' whose type is Method, 'natural language input' whose type is OtherScientificTerm, 'parsing flexibilities' whose type is OtherScientificTerm, 'system' whose type is Generic, 'FlexP' whose type is Method, 'bottom-up pattern-matching parser' whose type is Method, 'flexibilities' whose type is Generic, 'restricted natural language' whose type is OtherScientificTerm, 'limited-domain computer system' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ough voting in a geometric transformation space allows us to realize spatial verification, but remains sensitive to feature detection errors because of the inflexible quan-tization of single feature correspondences. To handle this problem, we propose a new method, called adaptive dither voting, for robust spatial verification. For each correspondence , instead of hard-mapping it to a single transformation , the method augments its description by using multiple dithered transformations that are deterministically generated by the other correspondences. The method reduces the probability of losing correspondences during transformation quantization, and provides high robustness as regards mismatches by imposing three geometric constraints on the dithering process. We also propose exploiting the non-uniformity of a Hough histogram as the spatial similarity to handle multiple matching surfaces. Extensive experiments conducted on four datasets show the superiority of our method. The method outperforms its state-of-the-art counterparts in both accuracy and scalability, especially when it comes to the retrieval of small, rotated objects.", "output": "The abstract text contains these key words below: 'Hough voting' whose type is Method, 'geometric transformation space' whose type is OtherScientificTerm, 'spatial verification' whose type is Task, 'feature detection errors' whose type is OtherScientificTerm, 'inflexible quan-tization of single feature correspondences' whose type is OtherScientificTerm, 'method' whose type is Generic, 'adaptive dither voting' whose type is Method, 'robust spatial verification' whose type is Task, 'method' whose type is Generic, 'multiple dithered transformations' whose type is OtherScientificTerm, 'method' whose type is Generic, 'transformation quantization' whose type is OtherScientificTerm, 'regards mismatches' whose type is OtherScientificTerm, 'geometric constraints' whose type is OtherScientificTerm, 'dithering process' whose type is OtherScientificTerm, 'non-uniformity' whose type is OtherScientificTerm, 'Hough histogram' whose type is Method, 'spatial similarity' whose type is OtherScientificTerm, 'multiple matching surfaces' whose type is OtherScientificTerm, 'method' whose type is Generic, 'method' whose type is Generic, 'counterparts' whose type is Generic, 'accuracy' whose type is Metric, 'scalability' whose type is Metric, 'retrieval of small, rotated objects' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition , our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its sensitivity to human pose, we show results for pose estimation on the FLIC and MPII datasets that are competitive, or better than approaches using significantly more supervision. Our method can be combined with supervised representations to provide an additional boost in accuracy.", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'visual representation' whose type is Method, 'raw spatiotemporal signals in videos' whose type is Material, 'representation' whose type is Generic, 'supervision from semantic labels' whose type is OtherScientificTerm, 'method' whose type is Generic, 'unsupervised sequential verification task' whose type is Task, 'temporal order' whose type is OtherScientificTerm, 'task' whose type is Generic, 'semantic labels' whose type is OtherScientificTerm, 'visual representation' whose type is Method, 'Convolutional Neural Network (CNN)' whose type is Method, 'representation' whose type is Generic, 'complementary information' whose type is OtherScientificTerm, 'supervised image datasets' whose type is Material, 'ImageNet' whose type is Material, 'method' whose type is Generic, 'human pose' whose type is OtherScientificTerm, 'pre-training' whose type is Method, 'action recognition' whose type is Task, 'our method' whose type is Method, 'learning without external data' whose type is Method, 'benchmark datasets' whose type is Generic, 'UCF101' whose type is Material, 'HMDB51' whose type is Material, 'human pose' whose type is OtherScientificTerm, 'pose estimation' whose type is Task, 'FLIC and MPII datasets' whose type is Material, 'approaches' whose type is Generic, 'supervision' whose type is OtherScientificTerm, 'Our method' whose type is OtherScientificTerm, 'supervised representations' whose type is Method, 'accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n Automatic  evaluation metrics  for  Machine Translation (MT) systems  , such as  BLEU  or  NIST  , are now well established. Yet, they are scarcely used for the assessment of  language pairs  like  English-Chinese  or  English-Japanese  , because of the  word segmentation problem  . This study establishes the equivalence between the standard use of  BLEU  in  word n-grams  and its application at the  character  level. The use of  BLEU  at the  character  level eliminates the  word segmentation problem  : it makes it possible to directly compare commercial systems outputting  unsegmented texts  with, for instance,  statistical MT systems  which usually segment their  outputs  . ", "output": "The abstract text contains these key words below: 'Automatic  evaluation metrics' whose type is Metric, 'Machine Translation (MT) systems' whose type is Task, 'BLEU' whose type is Metric, 'NIST' whose type is Metric, 'they' whose type is Generic, 'assessment of  language pairs' whose type is Task, 'language pairs' whose type is OtherScientificTerm, 'English-Chinese' whose type is OtherScientificTerm, 'English-Japanese' whose type is OtherScientificTerm, 'word segmentation problem' whose type is Task, 'BLEU' whose type is Metric, 'word n-grams' whose type is Method, 'character  level' whose type is OtherScientificTerm, 'BLEU' whose type is Metric, 'character  level' whose type is OtherScientificTerm, 'word segmentation problem' whose type is Task, 'it' whose type is Generic, 'commercial systems' whose type is Method, 'statistical MT systems' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present a new method for detecting interest points using histogram information. Unlike existing interest point detectors, which measure pixel-wise differences in image intensity, our detectors incorporate histogram-based representations , and thus can find image regions that present a distinct distribution in the neighborhood. The proposed detectors are able to capture large-scale structures and distinctive textured patterns, and exhibit strong invariance to rotation, illumination variation, and blur. The experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes, in terms of repeatability and distinctiveness. An extension of our method to space-time interest point detection for action classification is also presented.", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'detecting interest points' whose type is Task, 'histogram information' whose type is OtherScientificTerm, 'interest point detectors' whose type is Method, 'pixel-wise differences in image intensity' whose type is Metric, 'detectors' whose type is Generic, 'histogram-based representations' whose type is Method, 'detectors' whose type is Generic, 'large-scale structures' whose type is OtherScientificTerm, 'distinctive textured patterns' whose type is OtherScientificTerm, 'rotation' whose type is OtherScientificTerm, 'illumination variation' whose type is OtherScientificTerm, 'blur' whose type is OtherScientificTerm, 'histogram-based interest point detectors' whose type is Method, 'matching textured scenes' whose type is Task, 'blur and illumination changes' whose type is OtherScientificTerm, 'repeatability' whose type is Metric, 'distinctiveness' whose type is Metric, 'method' whose type is Generic, 'space-time interest point detection' whose type is Task, 'action classification' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents an  evaluation method  employing a  latent variable model  for  paraphrases  with their  contexts . We assume that the  context  of a  sentence  is indicated by a  latent variable  of the  model  as a  topic  and that the  likelihood  of each  variable  can be inferred. A  paraphrase  is evaluated for whether its  sentences  are used in the same  context . Experimental results showed that the proposed method achieves almost 60%  accuracy  and that there is not a large performance difference between the two  models . The results also revealed an upper bound of  accuracy  of 77% with the  method  when using only  topic information .\n", "output": "The abstract text contains these key words below: 'evaluation method' whose type is Generic, 'latent variable model' whose type is Method, 'paraphrases' whose type is OtherScientificTerm, 'latent variable' whose type is OtherScientificTerm, 'model' whose type is Generic, 'paraphrase' whose type is OtherScientificTerm, 'method' whose type is Generic, 'accuracy' whose type is Metric, 'method' whose type is Generic, 'topic information' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "onditional Random Field models have proved effective for several low-level computer vision problems. Inference in these models involves solving a combinatorial optimization problem, with methods such as graph cuts, belief propagation. Although several methods have been proposed to learn the model parameters from training data, they suffer from various drawbacks. Learning these parameters involves computing the partition function, which is intractable. To overcome this, state-of-the-art structured learning methods frame the problem as one of large margin estimation. Iterative solutions have been proposed to solve the resulting convex optimization problem. Each iteration involves solving an inference problem over all the labels , which limits the efficiency of these structured methods. In this paper we present an efficient large margin piece-wise learning method which is widely applicable. We show how the resulting optimization problem can be reduced to an equivalent convex problem with a small number of constraints , and solve it using an efficient scheme. Our method is both memory and computationally efficient. We show results on publicly available standard datasets.", "output": "The abstract text contains these key words below: 'Conditional Random Field models' whose type is Method, 'low-level computer vision problems' whose type is Task, 'Inference' whose type is Task, 'models' whose type is Generic, 'combinatorial optimization problem' whose type is Task, 'methods' whose type is Generic, 'graph cuts' whose type is Method, 'belief propagation' whose type is Method, 'model parameters' whose type is OtherScientificTerm, 'parameters' whose type is Generic, 'partition function' whose type is OtherScientificTerm, 'structured learning methods' whose type is Method, 'problem' whose type is Generic, 'large margin estimation' whose type is Task, 'Iterative solutions' whose type is Method, 'convex optimization problem' whose type is Task, 'inference problem' whose type is Task, 'structured methods' whose type is Generic, 'large margin piece-wise learning method' whose type is Method, 'optimization problem' whose type is Task, 'convex problem' whose type is Task, 'method' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " new exemplar-based framework unifying image completion , texture synthesis and image inpainting is presented in this work. Contrary to existing greedy techniques, these tasks are posed in the form of a discrete global optimization problem with a well defined objective function. For solving this problem a novel optimization scheme, called Priority-BP, is proposed which carries two very important extensions over standard belief propagation (BP): \" priority-based message scheduling \" and \" dynamic label pruning \". These two extensions work in cooperation to deal with the intolerable computational cost of BP caused by the huge number of existing labels. Moreover, both extensions are generic and can therefore be applied to any MRF energy function as well. The effectiveness of our method is demonstrated on a wide variety of image completion examples.", "output": "The abstract text contains these key words below: 'exemplar-based framework' whose type is Method, 'image completion' whose type is Task, 'texture synthesis' whose type is Task, 'image inpainting' whose type is Task, 'greedy techniques' whose type is Method, 'tasks' whose type is Generic, 'discrete global optimization problem' whose type is Task, 'well defined objective function' whose type is OtherScientificTerm, 'problem' whose type is Generic, 'optimization scheme' whose type is Method, 'Priority-BP' whose type is Method, 'extensions' whose type is Generic, 'belief propagation (BP)' whose type is Method, 'priority-based message scheduling' whose type is Method, 'dynamic label pruning' whose type is Method, 'extensions' whose type is Generic, 'intolerable computational cost of BP' whose type is OtherScientificTerm, 'BP' whose type is Method, 'extensions' whose type is Generic, 'MRF energy function' whose type is Method, 'method' whose type is Generic, 'image completion examples' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n A new approach for  Interactive Machine Translation  where the  author  interacts during the creation or the modification of the  document  is proposed. The explanation of an  ambiguity  or an error for the purposes of correction does not use any concepts of the underlying  linguistic theory  : it is a reformulation of the erroneous or ambiguous  sentence  . The interaction is limited to the analysis step of the  translation process  . This paper presents a new  interactive disambiguation scheme  based on the  paraphrasing  of a  parser  's multiple output. Some examples of  paraphrasing  ambiguous  sentences  are presented. ", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'Interactive Machine Translation' whose type is Task, 'linguistic theory' whose type is Method, 'translation process' whose type is Method, 'interactive disambiguation scheme' whose type is Method, 'paraphrasing' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Past work of generating  referring expressions  mainly utilized attributes of  objects  and  binary relations  between  objects . However, such an approach does not work well when there is no distinctive attribute among  objects . To overcome this limitation, this paper proposes a method utilizing the perceptual groups of  objects  and  n-ary relations  among them. The key is to identify groups of  objects  that are naturally recognized by humans. We conducted psychological experiments with 42 subjects to collect  referring expressions  in such situations, and built a  generation algorithm  based on the results. The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions.\n", "output": "The abstract text contains these key words below: 'referring expressions' whose type is OtherScientificTerm, 'binary relations' whose type is OtherScientificTerm, 'n-ary relations' whose type is OtherScientificTerm, 'referring expressions' whose type is OtherScientificTerm, 'generation algorithm' whose type is Method, 'method' whose type is Generic, 'referring expressions' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper describes a  characters-based Chinese collocation system  and discusses the advantages of it over a traditional  word-based system . Since  wordbreaks  are not conventionally marked in  Chinese text corpora , a  character-based collocation system  has the dual advantages of avoiding  pre-processing distortion  and directly accessing  sub-lexical information . Furthermore,  word-based collocational properties  can be obtained through an auxiliary module of  automatic segmentation .\n", "output": "The abstract text contains these key words below: 'characters-based Chinese collocation system' whose type is Method, 'it' whose type is Generic, 'word-based system' whose type is Method, 'Chinese text corpora' whose type is Material, 'character-based collocation system' whose type is Method, 'avoiding  pre-processing distortion' whose type is Task, 'accessing  sub-lexical information' whose type is Task, 'word-based collocational properties' whose type is OtherScientificTerm, 'auxiliary module of  automatic segmentation' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we propose a novel  Cooperative Model  for  natural language understanding  in a  dialogue system  . We build this based on both  Finite State Model (FSM)  and  Statistical Learning Model (SLM)  .  FSM  provides two strategies for  language understanding  and have a high accuracy but little robustness and flexibility.  Statistical approach  is much more robust but less accurate.  Cooperative Model  incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies. ", "output": "The abstract text contains these key words below: 'Cooperative Model' whose type is Method, 'natural language understanding' whose type is Task, 'dialogue system' whose type is Task, 'this' whose type is Generic, 'Finite State Model (FSM)' whose type is Method, 'Statistical Learning Model (SLM)' whose type is Method, 'FSM' whose type is Method, 'language understanding' whose type is Task, 'Statistical approach' whose type is Method, 'Cooperative Model' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In order to build robust  automatic abstracting systems , there is a need for better  training resources  than are currently available. In this paper, we introduce an  annotation scheme  for scientific articles which can be used to build such a  resource  in a consistent way. The seven categories of the  scheme  are based on  rhetorical moves  of  argumentation . Our experimental results show that the  scheme  is stable, reproducible and intuitive to use.\n", "output": "The abstract text contains these key words below: 'automatic abstracting systems' whose type is Task, 'training resources' whose type is Material, 'annotation scheme' whose type is Method, 'scientific articles' whose type is Material, 'resource' whose type is Generic, 'scheme' whose type is Generic, 'rhetorical moves  of  argumentation' whose type is Method, 'scheme' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a new  part-of-speech tagger  that demonstrates the following ideas: (i) explicit use of both preceding and following  tag contexts  via a  dependency network representation  , (ii) broad use of  lexical features  , including  jointly conditioning on multiple consecutive words  , (iii) effective use of  priors  in  conditional loglinear models  , and (iv) fine-grained modeling of  unknown word features  . Using these ideas together, the resulting  tagger  gives a 97.24%  accuracy  on the  Penn Treebank WSJ  , an  error reduction  of 4.4% on the best previous single automatically learned  tagging  result. ", "output": "The abstract text contains these key words below: 'part-of-speech tagger' whose type is Method, 'tag contexts' whose type is OtherScientificTerm, 'dependency network representation' whose type is Method, 'lexical features' whose type is OtherScientificTerm, 'multiple consecutive words' whose type is OtherScientificTerm, 'priors  in  conditional loglinear models' whose type is OtherScientificTerm, 'fine-grained modeling of  unknown word features' whose type is Method, 'tagger' whose type is Method, 'accuracy' whose type is Metric, 'Penn Treebank WSJ' whose type is Material, 'error' whose type is Metric, 'tagging' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n important area of learning in autonomous agents is the ability to learn domain-speciic models of actions to be used by planning systems. In this paper, we present methods by which an agent learns action models from its own experience and from its observation of a domain expert. These methods diier from previous work in the area in two ways: the use of an action model formalism which is better suited to the needs of a re-active agent, and successful implementation of noise-handling mechanisms. Training instances are generated from experience and observation, and a variant of GOLEM is used to learn action models from these instances. The integrated learning system has been experimentally validated in simulated construction and ooce domains.", "output": "The abstract text contains these key words below: 'learning in autonomous agents' whose type is Task, 'domain-speciic models of actions' whose type is Method, 'planning systems' whose type is Task, 'methods' whose type is Generic, 'action models' whose type is Method, 'domain expert' whose type is OtherScientificTerm, 'methods' whose type is Metric, 'action model formalism' whose type is Method, 're-active agent' whose type is Method, 'noise-handling mechanisms' whose type is Method, 'GOLEM' whose type is Method, 'action models' whose type is Method, 'integrated learning system' whose type is Method, 'simulated construction' whose type is Task, 'ooce domains' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents an algorithm for computing optical flow, shape, motion, lighting, and albedo from an image sequence of a rigidly-moving Lambertian object under distant illumination. The problem is formulated in a manner that subsumes structure from motion, multi-view stereo, and photo-metric stereo as special cases. The algorithm utilizes both spatial and temporal intensity variation as cues: the former constrains flow and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces. The algorithm works by iteratively estimating affine camera parameters, illumination , shape, and albedo in an alternating fashion. Results are demonstrated on videos of hand-held objects moving in front of a fixed light and camera.", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'computing optical flow, shape, motion, lighting, and albedo' whose type is Task, 'image sequence' whose type is Material, 'rigidly-moving Lambertian object' whose type is Material, 'distant illumination' whose type is OtherScientificTerm, 'problem' whose type is Generic, 'motion' whose type is Material, 'multi-view stereo' whose type is Material, 'photo-metric stereo' whose type is Material, 'algorithm' whose type is Generic, 'spatial and temporal intensity variation' whose type is OtherScientificTerm, 'cues' whose type is Generic, 'former' whose type is Generic, 'flow' whose type is OtherScientificTerm, 'latter' whose type is Generic, 'surface orientation' whose type is OtherScientificTerm, 'cues' whose type is Generic, 'dense reconstruction of both textured and texture-less surfaces' whose type is Task, 'algorithm' whose type is Generic, 'estimating affine camera parameters, illumination , shape, and albedo' whose type is Method, 'videos of hand-held objects' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper examines what kind of  similarity  between  words  can be represented by what kind of  word vectors  in the  vector space model  . Through two experiments, three  methods for constructing word vectors  , i.e.,  LSA-based, cooccurrence-based and dictionary-based methods  , were compared in terms of the ability to represent two kinds of  similarity  , i.e.,  taxonomic similarity  and  associative similarity  . The result of the comparison was that the  dictionary-based word vectors  better reflect  taxonomic similarity  , while the  LSA-based and the cooccurrence-based word vectors  better reflect  associative similarity  . ", "output": "The abstract text contains these key words below: 'similarity  between  words' whose type is OtherScientificTerm, 'word vectors' whose type is OtherScientificTerm, 'vector space model' whose type is Method, 'methods' whose type is Generic, 'constructing word vectors' whose type is Task, 'LSA-based, cooccurrence-based and dictionary-based methods' whose type is Method, 'similarity' whose type is Generic, 'taxonomic similarity' whose type is OtherScientificTerm, 'associative similarity' whose type is OtherScientificTerm, 'dictionary-based word vectors' whose type is OtherScientificTerm, 'taxonomic similarity' whose type is OtherScientificTerm, 'LSA-based and the cooccurrence-based word vectors' whose type is OtherScientificTerm, 'associative similarity' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n We describe a  generative probabilistic model of natural language  , which we call  HBG  , that takes advantage of detailed  linguistic information  to resolve  ambiguity  .   HBG  incorporates  lexical, syntactic, semantic, and structural information  from the  parse tree  into the  disambiguation process  in a novel way. We use a  corpus of bracketed sentences  , called a  Treebank  , in combination with  decision tree building  to tease out the relevant aspects of a  parse tree  that will determine the correct  parse  of a  sentence  . This stands in contrast to the usual approach of further  grammar  tailoring via the usual  linguistic introspection  in the hope of generating the correct  parse  . In  head-to-head tests  against one of the best existing robust  probabilistic parsing models  , which we call  P-CFG , the  HBG model  significantly outperforms  P-CFG  , increasing the  parsing accuracy  rate from 60% to 75%, a 37% reduction in error. ", "output": "The abstract text contains these key words below: 'generative probabilistic model of natural language' whose type is Method, 'HBG' whose type is Method, 'linguistic information' whose type is OtherScientificTerm, 'ambiguity' whose type is Task, 'HBG' whose type is Method, 'lexical, syntactic, semantic, and structural information' whose type is OtherScientificTerm, 'parse tree' whose type is OtherScientificTerm, 'disambiguation process' whose type is Task, 'corpus of bracketed sentences' whose type is Material, 'Treebank' whose type is Material, 'decision tree building' whose type is Method, 'parse tree' whose type is OtherScientificTerm, 'parse' whose type is OtherScientificTerm, 'grammar  tailoring' whose type is Task, 'linguistic introspection' whose type is Method, 'parse' whose type is OtherScientificTerm, 'head-to-head tests' whose type is OtherScientificTerm, 'robust  probabilistic parsing models' whose type is Method, 'P-CFG' whose type is Method, 'HBG model' whose type is Method, 'P-CFG' whose type is Method, 'parsing accuracy  rate' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n the security domain a key problem is identifying rare behaviours of interest. Training examples for these behaviours may or may not exist, and if they do exist there will be few examples, quite probably one. We present a novel weakly supervised algorithm that can detect behaviours that either have never before been seen or for which there are few examples. Global context is modelled, allowing the detection of abnormal behaviours that in isolation appear normal. Pragmatic aspects are considered, such that no parameter tuning is required and real time performance is achieved.", "output": "The abstract text contains these key words below: 'security domain' whose type is Task, 'identifying rare behaviours of interest' whose type is Task, 'rare behaviours of interest' whose type is OtherScientificTerm, 'Training examples' whose type is Material, 'behaviours' whose type is Generic, 'they' whose type is Generic, 'weakly supervised algorithm' whose type is Method, 'behaviours' whose type is OtherScientificTerm, 'Global context' whose type is OtherScientificTerm, 'detection of abnormal behaviours' whose type is Task, 'Pragmatic aspects' whose type is OtherScientificTerm, 'parameter tuning' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present an application of  ambiguity packing and stochastic disambiguation techniques  for  Lexical-Functional Grammars (LFG)  to the domain of  sentence condensation  . Our system incorporates a  linguistic parser/generator  for  LFG  , a  transfer component  for  parse reduction  operating on  packed parse forests  , and a  maximum-entropy model  for  stochastic output selection  . Furthermore, we propose the use of standard  parser evaluation methods  for automatically evaluating the  summarization  quality of  sentence condensation systems  . An  experimental evaluation  of  summarization  quality shows a close correlation between the  automatic parse-based evaluation  and a  manual evaluation  of generated  strings  . Overall  summarization  quality of the proposed system is state-of-the-art, with guaranteed  grammaticality  of the  system output  due to the use of a  constraint-based parser/generator  . ", "output": "The abstract text contains these key words below: 'ambiguity packing and stochastic disambiguation techniques' whose type is Method, 'Lexical-Functional Grammars (LFG)' whose type is Method, 'sentence condensation' whose type is Task, 'system' whose type is Generic, 'linguistic parser/generator' whose type is Method, 'LFG' whose type is Method, 'transfer component' whose type is Method, 'parse reduction' whose type is Task, 'packed parse forests' whose type is OtherScientificTerm, 'maximum-entropy model' whose type is Method, 'stochastic output selection' whose type is Task, 'parser evaluation methods' whose type is Method, 'summarization  quality' whose type is Metric, 'sentence condensation systems' whose type is Method, 'summarization  quality' whose type is Metric, 'automatic parse-based evaluation' whose type is Method, 'manual evaluation' whose type is Method, 'summarization  quality' whose type is Metric, 'system' whose type is Generic, 'grammaticality' whose type is Metric, 'constraint-based parser/generator' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e investigate the problem of fine-grained sketch-based image retrieval (SBIR), where free-hand human sketches are used as queries to perform instance-level retrieval of images. This is an extremely challenging task because (i) visual comparisons not only need to be fine-grained but also executed cross-domain, (ii) free-hand (finger) sketches are highly abstract, making fine-grained matching harder, and most importantly (iii) annotated cross-domain sketch-photo datasets required for training are scarce, challenging many state-of-the-art machine learning techniques. In this paper, for the first time, we address all these challenges, providing a step towards the capabilities that would underpin a commercial sketch-based image retrieval application. We introduce a new database of 1,432 sketch-photo pairs from two categories with 32,000 fine-grained triplet ranking annotations. We then develop a deep triplet-ranking model for instance-level SBIR with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data. Extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training deep networks for fine-grained cross-domain ranking tasks.", "output": "The abstract text contains these key words below: 'fine-grained sketch-based image retrieval (SBIR)' whose type is Task, 'free-hand human sketches' whose type is OtherScientificTerm, 'instance-level retrieval of images' whose type is Task, 'visual comparisons' whose type is OtherScientificTerm, 'free-hand (finger) sketches' whose type is OtherScientificTerm, 'fine-grained matching' whose type is Task, 'annotated cross-domain sketch-photo datasets' whose type is Material, 'machine learning techniques' whose type is Method, 'sketch-based image retrieval application' whose type is Task, 'fine-grained triplet ranking annotations' whose type is OtherScientificTerm, 'deep triplet-ranking model' whose type is Method, 'instance-level SBIR' whose type is Task, 'data augmentation' whose type is Method, 'staged pre-training strategy' whose type is Method, 'insufficient fine-grained training data' whose type is Material, 'data sufficiency' whose type is OtherScientificTerm, 'over-fitting avoidance' whose type is OtherScientificTerm, 'deep networks' whose type is Method, 'fine-grained cross-domain ranking tasks' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents an analysis of  temporal anaphora  in  sentences  which contain  quantification over events , within the framework of  Discourse Representation Theory . The analysis in (Partee, 1984) of  quantified sentences , introduced by a  temporal connective , gives the wrong  truth-conditions  when the  temporal connective  in the  subordinate clause  is before or after. This  problem  has been previously analyzed in (de Swart, 1991) as an instance of the  proportion problem  and given a solution from a  Generalized Quantifier approach . By using a careful distinction between the different notions of  reference time  based on (Kamp and Reyle, 1993), we propose a solution to this  problem , within the framework of  DRT . We show some applications of this  solution  to additional  temporal anaphora phenomena  in  quantified sentences .\n", "output": "The abstract text contains these key words below: 'temporal anaphora' whose type is Task, 'quantification over events' whose type is OtherScientificTerm, 'Discourse Representation Theory' whose type is Method, 'quantified sentences' whose type is Material, 'temporal connective' whose type is OtherScientificTerm, 'temporal connective' whose type is OtherScientificTerm, 'subordinate clause' whose type is OtherScientificTerm, 'problem' whose type is Generic, 'proportion problem' whose type is OtherScientificTerm, 'solution' whose type is Generic, 'Generalized Quantifier approach' whose type is Method, 'reference time' whose type is OtherScientificTerm, 'solution' whose type is Generic, 'problem' whose type is Generic, 'DRT' whose type is Method, 'solution' whose type is Generic, 'temporal anaphora phenomena' whose type is Task, 'quantified sentences' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "With performance above 97%  accuracy  for  newspaper text ,  part of speech (pos) tagging  might be considered a solved problem. Previous studies have shown that allowing the  parser  to resolve  pos tag ambiguity  does not improve performance. However, for  grammar formalisms  which use more  fine-grained grammatical categories , for example  tag  and  ccg ,  tagging accuracy  is much lower. In fact, for these  formalisms , premature  ambiguity resolution  makes  parsing  infeasible. We describe a  multi-tagging approach  which maintains a suitable level of  lexical category ambiguity  for accurate and efficient  ccg parsing . We extend this  multi-tagging approach  to the  pos level  to overcome errors introduced by automatically assigned  pos tags . Although  pos tagging accuracy  seems high, maintaining some  pos tag ambiguity  in the  language processing pipeline  results in more accurate  ccg supertagging .\n", "output": "The abstract text contains these key words below: 'accuracy' whose type is Metric, 'newspaper text' whose type is Material, 'part of speech (pos) tagging' whose type is Task, 'parser' whose type is Method, 'pos tag ambiguity' whose type is OtherScientificTerm, 'grammar formalisms' whose type is Method, 'fine-grained grammatical categories' whose type is OtherScientificTerm, 'tag' whose type is OtherScientificTerm, 'ccg' whose type is OtherScientificTerm, 'tagging accuracy' whose type is Metric, 'formalisms' whose type is Generic, 'premature  ambiguity resolution' whose type is OtherScientificTerm, 'parsing' whose type is Task, 'multi-tagging approach' whose type is Method, 'lexical category ambiguity' whose type is OtherScientificTerm, 'ccg parsing' whose type is Task, 'multi-tagging approach' whose type is Method, 'pos level' whose type is OtherScientificTerm, 'pos tags' whose type is OtherScientificTerm, 'pos tagging accuracy' whose type is Metric, 'pos tag ambiguity' whose type is OtherScientificTerm, 'language processing pipeline' whose type is Method, 'ccg supertagging' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " English  is shown to be trans-context-free on the basis of  coordinations  of the respectively type that involve  strictly syntactic cross-serial agreement . The  agreement  in question involves  number  in  nouns  and  reflexive pronouns  and is syntactic rather than semantic in nature because  grammatical number  in  English , like  grammatical gender  in  languages  such as  French , is partly arbitrary. The formal proof, which makes crucial use of the  Interchange Lemma  of Ogden et al., is so constructed as to be valid even if  English  is presumed to contain  grammatical sentences  in which respectively operates across a pair of  coordinate phrases  one of whose members has fewer  conjuncts  than the other; it thus goes through whatever the facts may be regarding  constructions  with unequal numbers of  conjuncts  in the  scope  of respectively, whereas other  arguments  have foundered on this problem. ", "output": "The abstract text contains these key words below: 'English' whose type is Material, 'coordinations' whose type is OtherScientificTerm, 'strictly syntactic cross-serial agreement' whose type is OtherScientificTerm, 'agreement' whose type is Generic, 'nouns' whose type is OtherScientificTerm, 'reflexive pronouns' whose type is OtherScientificTerm, 'grammatical number' whose type is OtherScientificTerm, 'English' whose type is Material, 'grammatical gender' whose type is OtherScientificTerm, 'languages' whose type is Material, 'French' whose type is Material, 'Interchange Lemma' whose type is Method, 'English' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e attack an inexplicably under-explored language genre of spoken language\u2014lyrics in music\u2014via completely unsuper-vised induction of an SMT-style stochastic transduction grammar for hip hop lyrics, yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input. Unlike previous efforts, we choose the domain of hip hop lyrics, which is particularly unstructured and noisy. A novel feature of our approach is that it is completely unsupervised and requires no a priori linguistic or phonetic knowledge. In spite of the level of difficulty of the challenge, the model nevertheless produces fluent output as judged by human evaluators, and performs significantly better than widely used phrase-based SMT models upon the same task.", "output": "The abstract text contains these key words below: 'under-explored language genre of spoken language' whose type is Material, 'lyrics in music' whose type is Material, 'unsuper-vised induction' whose type is Method, 'SMT-style stochastic transduction grammar' whose type is Method, 'hip hop lyrics' whose type is Material, 'fully-automatically learned challenge-response system' whose type is Method, 'rhyming lyrics' whose type is Material, 'hip hop lyrics' whose type is Material, 'approach' whose type is Generic, 'priori linguistic or phonetic knowledge' whose type is OtherScientificTerm, 'model' whose type is Generic, 'human evaluators' whose type is OtherScientificTerm, 'phrase-based SMT models' whose type is Method, 'task' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We focus on the problem of building large  repositories  of  lexical conceptual structure (LCS) representations  for  verbs  in multiple  languages . One of the main results of this work is the definition of a relation between  broad semantic classes  and  LCS meaning components . Our  acquisition program - LEXICALL -  takes, as input, the result of previous work on  verb classification  and  thematic grid tagging , and outputs  LCS representations  for different  languages . These  representations  have been ported into  English, Arabic and Spanish lexicons , each containing approximately 9000  verbs . We are currently using these  lexicons  in an  operational foreign language tutoring  and  machine translation . \n", "output": "The abstract text contains these key words below: 'lexical conceptual structure (LCS) representations' whose type is Method, 'broad semantic classes' whose type is OtherScientificTerm, 'LCS meaning components' whose type is Method, 'acquisition program - LEXICALL -' whose type is Method, 'verb classification' whose type is Task, 'thematic grid tagging' whose type is Task, 'LCS representations' whose type is Method, 'representations' whose type is Generic, 'English, Arabic and Spanish lexicons' whose type is OtherScientificTerm, 'lexicons' whose type is Generic, 'operational foreign language tutoring' whose type is Task, 'machine translation' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents a  corpus study  that explores the extent to which captions contribute to recognizing the intended message of an  information graphic . It then presents an implemented  graphic interpretation system  that takes into account a variety of  communicative signals , and an evaluation study showing that evidence obtained from  shallow processing  of the graphic's caption has a significant impact on the system's success. This work is part of a larger project whose goal is to provide  sight-impaired users  with effective access to  information graphics .\n", "output": "The abstract text contains these key words below: 'corpus study' whose type is Task, 'information graphic' whose type is OtherScientificTerm, 'graphic interpretation system' whose type is Method, 'communicative signals' whose type is Material, 'shallow processing' whose type is Method, 'graphic's caption' whose type is Material, 'system' whose type is Generic, 'sight-impaired users' whose type is OtherScientificTerm, 'information graphics' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Currently several  grammatical formalisms  converge towards being declarative and towards utilizing  context-free phrase-structure grammar  as a backbone, e.g.  LFG  and  PATR-II . Typically the processing of these formalisms is organized within a  chart-parsing framework . The declarative character of the  formalisms  makes it important to decide upon an overall  optimal control strategy  on the part of the processor. In particular, this brings the  rule-invocation strategy  into critical focus: to gain maximal  processing efficiency , one has to determine the best way of putting the  rules  to use. The aim of this paper is to provide a survey and a practical comparison of fundamental  rule-invocation strategies  within  context-free chart parsing .\n", "output": "The abstract text contains these key words below: 'grammatical formalisms' whose type is Method, 'context-free phrase-structure grammar' whose type is Method, 'LFG' whose type is OtherScientificTerm, 'PATR-II' whose type is OtherScientificTerm, 'formalisms' whose type is Generic, 'chart-parsing framework' whose type is Method, 'formalisms' whose type is Generic, 'optimal control strategy' whose type is Method, 'rule-invocation strategy' whose type is Method, 'rules' whose type is OtherScientificTerm, 'rule-invocation strategies' whose type is Method, 'context-free chart parsing' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper gives an overall account of a prototype  natural language question answering system  , called  Chat-80  .  Chat-80  has been designed to be both efficient and easily adaptable to a variety of applications. The system is implemented entirely in  Prolog  , a  programming language  based on  logic  . With the aid of a  logic-based grammar formalism  called  extraposition grammars  ,  Chat-80  translates  English questions  into the  Prolog   subset of logic  . The resulting  logical expression  is then transformed by a  planning algorithm  into efficient  Prolog  , cf.  query optimisation  in a  relational database  . Finally, the  Prolog form  is executed to yield the answer. ", "output": "The abstract text contains these key words below: 'natural language question answering system' whose type is Method, 'Chat-80' whose type is Method, 'Chat-80' whose type is Method, 'system' whose type is Generic, 'Prolog' whose type is OtherScientificTerm, 'programming language' whose type is OtherScientificTerm, 'logic' whose type is OtherScientificTerm, 'logic-based grammar formalism' whose type is Method, 'extraposition grammars' whose type is Method, 'Chat-80' whose type is Method, 'Prolog   subset of logic' whose type is OtherScientificTerm, 'logical expression' whose type is OtherScientificTerm, 'planning algorithm' whose type is Method, 'Prolog' whose type is OtherScientificTerm, 'query optimisation' whose type is Method, 'relational database' whose type is Material, 'Prolog form' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we develop a geometric framework for linear or nonlinear discriminant subspace learning and classification. In our framework, the structures of classes are conceptualized as a semi-Riemannian manifold which is considered as a submanifold embedded in an ambient semi-Riemannian space. The class structures of original samples can be characterized and deformed by local metrics of the semi-Riemannian space. Semi-Riemannian metrics are uniquely determined by the smoothing of discrete functions and the nullity of the semi-Riemannian space. Based on the geometrization of class structures, optimizing class structures in the feature space is equivalent to maximizing the quadratic quantities of metric tensors in the semi-Riemannian space. Thus supervised discriminant subspace learning reduces to unsupervised semi-Riemannian mani-fold learning. Based on the proposed framework, a novel algorithm, dubbed as Semi-Riemannian Discriminant Analysis (SRDA), is presented for subspace-based classification. The performance of SRDA is tested on face recognition (singular case) and handwritten capital letter classification (nonsingular case) against existing algorithms. The experimental results show that SRDA works well on recognition and classification, implying that semi-Riemannian geometry is a promising new tool for pattern recognition and machine learning.", "output": "The abstract text contains these key words below: 'geometric framework' whose type is Method, 'linear or nonlinear discriminant subspace learning and classification' whose type is Task, 'framework' whose type is Generic, 'structures of classes' whose type is OtherScientificTerm, 'semi-Riemannian manifold' whose type is OtherScientificTerm, 'submanifold' whose type is OtherScientificTerm, 'ambient semi-Riemannian space' whose type is OtherScientificTerm, 'class structures' whose type is OtherScientificTerm, 'local metrics of the semi-Riemannian space' whose type is Metric, 'Semi-Riemannian metrics' whose type is Metric, 'smoothing of discrete functions' whose type is Method, 'nullity of the semi-Riemannian space' whose type is OtherScientificTerm, 'geometrization of class structures' whose type is Method, 'class structures' whose type is OtherScientificTerm, 'feature space' whose type is OtherScientificTerm, 'quadratic quantities of metric tensors' whose type is OtherScientificTerm, 'semi-Riemannian space' whose type is OtherScientificTerm, 'supervised discriminant subspace learning' whose type is Method, 'unsupervised semi-Riemannian mani-fold learning' whose type is Method, 'framework' whose type is Generic, 'algorithm' whose type is Generic, 'Semi-Riemannian Discriminant Analysis (SRDA)' whose type is Method, 'subspace-based classification' whose type is Task, 'SRDA' whose type is Method, 'face recognition (singular case' whose type is Task, 'handwritten capital letter classification (nonsingular case)' whose type is Task, 'algorithms' whose type is Generic, 'SRDA' whose type is Generic, 'recognition' whose type is Task, 'classification' whose type is Task, 'semi-Riemannian geometry' whose type is Method, 'pattern recognition' whose type is Task, 'machine learning' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including Stochastic Variance Reduced Gradient (SVRG) and Stochastic Dual Coordinate Ascent (SDCA). For a large family of penalized empirical risk minimization problems , our methods exploit data dependent local smoothness of the loss functions near the optimum, while maintaining convergence guarantees. Our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better. Empirically, we provide thorough numerical results to back up our theory. Additionally we present algorithms exploiting local smoothness in more aggressive ways, which perform even better in practice.", "output": "The abstract text contains these key words below: 'non-uniform sampling strategies' whose type is Method, 'stochastic optimization algorithms' whose type is Method, 'linear convergence' whose type is OtherScientificTerm, 'Stochastic Variance Reduced Gradient (SVRG)' whose type is Method, 'Stochastic Dual Coordinate Ascent (SDCA)' whose type is Method, 'penalized empirical risk minimization problems' whose type is Task, 'methods' whose type is Generic, 'data dependent local smoothness' whose type is OtherScientificTerm, 'loss functions' whose type is OtherScientificTerm, 'optimum' whose type is OtherScientificTerm, 'convergence guarantees' whose type is OtherScientificTerm, 'local smoothness' whose type is OtherScientificTerm, 'theory' whose type is Generic, 'algorithms' whose type is Generic, 'local smoothness' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Previous work has used  monolingual parallel corpora  to extract and generate  paraphrases  . We show that this task can be done using  bilingual parallel corpora  , a much more commonly available  resource  . Using  alignment techniques  from  phrase-based statistical machine translation  , we show how  paraphrases  in one  language  can be identified using a  phrase  in another language as a pivot. We define a  paraphrase probability  that allows  paraphrases  extracted from a  bilingual parallel corpus  to be ranked using  translation probabilities  , and show how it can be refined to take  contextual information  into account. We evaluate our  paraphrase extraction and ranking methods  using a set of  manual word alignments  , and contrast the  quality  with  paraphrases  extracted from  automatic alignments  . ", "output": "The abstract text contains these key words below: 'monolingual parallel corpora' whose type is Material, 'paraphrases' whose type is OtherScientificTerm, 'task' whose type is Generic, 'bilingual parallel corpora' whose type is Material, 'alignment techniques' whose type is Method, 'phrase-based statistical machine translation' whose type is Task, 'paraphrases' whose type is OtherScientificTerm, 'paraphrase probability' whose type is OtherScientificTerm, 'paraphrases' whose type is OtherScientificTerm, 'bilingual parallel corpus' whose type is Material, 'translation probabilities' whose type is OtherScientificTerm, 'it' whose type is Generic, 'contextual information' whose type is OtherScientificTerm, 'paraphrase extraction and ranking methods' whose type is Method, 'manual word alignments' whose type is Material, 'quality' whose type is Metric, 'paraphrases' whose type is OtherScientificTerm, 'automatic alignments' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In our current research into the design of  cognitively well-motivated interfaces  relying primarily on the  display of graphical information , we have observed that  graphical information  alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users' expectations. This can occur due to too much  information  being requested, too little,  information  of the wrong kind, etc. To solve this problem, we are working towards the integration of  natural language generation  to augment the  interaction ", "output": "The abstract text contains these key words below: 'cognitively well-motivated interfaces' whose type is Task, 'display of graphical information' whose type is OtherScientificTerm, 'graphical information' whose type is OtherScientificTerm, 'natural language generation' whose type is Task, 'interaction' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Listen-Communicate-Show (LCS)  is a new paradigm for  human interaction with data sources  . We integrate a  spoken language understanding system  with  intelligent mobile agents  that mediate between  users  and  information sources  . We have built and will demonstrate an application of this approach called  LCS-Marine  . Using  LCS-Marine  , tactical personnel can converse with their logistics system to place a supply or information request. The request is passed to a  mobile, intelligent agent  for execution at the appropriate  database  .  Requestors  can also instruct the system to notify them when the status of a  request  changes or when a  request  is complete. We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in  new domains  . ", "output": "The abstract text contains these key words below: 'Listen-Communicate-Show (LCS)' whose type is Task, 'human interaction with data sources' whose type is Task, 'spoken language understanding system' whose type is Method, 'intelligent mobile agents' whose type is Method, 'information sources' whose type is Material, 'approach' whose type is Generic, 'LCS-Marine' whose type is Task, 'LCS-Marine' whose type is Task, 'mobile, intelligent agent' whose type is Method, 'system' whose type is Generic, 'new domains' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The  interlingual approach to MT  has been repeatedly advocated by researchers originally interested in  natural language understanding  who take  machine translation  to be one possible application. However, not only the  ambiguity  but also the vagueness which every  natural language  inevitably has leads this approach into essential difficulties. In contrast, our project, the  Mu-project , adopts the  transfer approach  as the basic framework of  MT . This paper describes the detailed construction of the  transfer phase  of our system from  Japanese  to  English , and gives some examples of problems which seem difficult to treat in the  interlingual approach . The basic design principles of the  transfer phase  of our system have already been mentioned in (1) (2). Some of the principles which are relevant to the topic of this paper are: (a)  Multiple Layer of Grammars  (b)  Multiple Layer Presentation  (c)  Lexicon Driven Processing  (d)  Form-Oriented Dictionary Description . This paper also shows how these principles are realized in the current system.\n", "output": "The abstract text contains these key words below: 'interlingual approach' whose type is Method, 'MT' whose type is Generic, 'natural language understanding' whose type is Task, 'machine translation' whose type is Task, 'Mu-project' whose type is Method, 'transfer approach' whose type is Method, 'MT' whose type is Task, 'transfer phase' whose type is Generic, 'system' whose type is Generic, 'Japanese' whose type is Material, 'English' whose type is Material, 'interlingual approach' whose type is Method, 'transfer phase' whose type is Generic, 'system' whose type is Generic, 'principles' whose type is Generic, 'Multiple Layer of Grammars' whose type is Method, 'Multiple Layer Presentation' whose type is Method, 'Lexicon Driven Processing' whose type is Method, 'Form-Oriented Dictionary Description' whose type is Method, 'principles' whose type is Generic, 'system' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper examines the properties of  feature-based partial descriptions  built on top of  Halliday's systemic networks . We show that the crucial operation of  consistency checking  for such descriptions is NP-complete, and therefore probably intractable, but proceed to develop  algorithms  which can sometimes alleviate the unpleasant consequences of this  intractability .\n", "output": "The abstract text contains these key words below: 'feature-based partial descriptions' whose type is OtherScientificTerm, 'Halliday's systemic networks' whose type is Method, 'consistency checking' whose type is Method, 'descriptions' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper describes a system (  RAREAS  ) which synthesizes marine weather forecasts directly from  formatted weather data  . Such  synthesis  appears feasible in certain  natural sublanguages  with  stereotyped text structure  .  RAREAS  draws on several kinds of  linguistic and non-linguistic knowledge  and mirrors a forecaster's apparent tendency to ascribe less precise  temporal adverbs  to more remote meteorological events. The approach can easily be adapted to synthesize  bilingual or multi-lingual texts  . ", "output": "The abstract text contains these key words below: 'system' whose type is Generic, 'RAREAS' whose type is Method, 'marine weather forecasts' whose type is Task, 'formatted weather data' whose type is Material, 'synthesis' whose type is Generic, 'natural sublanguages  with  stereotyped text structure' whose type is Material, 'RAREAS' whose type is Method, 'linguistic and non-linguistic knowledge' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'bilingual or multi-lingual texts' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he compact description of a video sequence through a single image map and a dominant motion has applications in several domains, including video browsing and retrieval, compression, mosaicing, and visual summarization. Building such a representation requires the capability to register all the frames with respect to the dominant object in the scene, a task which has been, in the past, addressed through temporally localized motion estimates. In this paper, we show how the lack of temporal consistency associated with such estimates can undermine the validity of the dominant motion assumption, leading to oscillation between different scene interpretations and poor registration. To avoid this oscillation, we augment the motion model with a generic temporal constraint which increases the robustness against competing interpretations, leading to more meaningful content summarization.", "output": "The abstract text contains these key words below: 'compact description of a video sequence' whose type is Task, 'image map' whose type is OtherScientificTerm, 'dominant motion' whose type is OtherScientificTerm, 'domains' whose type is Generic, 'video browsing and retrieval' whose type is Task, 'compression' whose type is Task, 'mosaicing' whose type is Task, 'visual summarization' whose type is Task, 'representation' whose type is Generic, 'task' whose type is Generic, 'localized motion estimates' whose type is Method, 'lack of temporal consistency' whose type is OtherScientificTerm, 'temporal consistency' whose type is OtherScientificTerm, 'estimates' whose type is Generic, 'validity of the dominant motion assumption' whose type is OtherScientificTerm, 'dominant motion assumption' whose type is OtherScientificTerm, 'oscillation between different scene interpretations and poor registration' whose type is OtherScientificTerm, 'oscillation' whose type is Generic, 'motion model' whose type is Method, 'generic temporal constraint' whose type is OtherScientificTerm, 'robustness' whose type is Metric, 'content summarization' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In order to boost the  translation quality  of  EBMT  based on a small-sized  bilingual corpus  , we use an out-of-domain  bilingual corpus  and, in addition, the  language model  of an in-domain  monolingual corpus  . We conducted experiments with an  EBMT system  . The two  evaluation measures  of the  BLEU score  and the  NIST score  demonstrated the effect of using an out-of-domain  bilingual corpus  and the possibility of using the  language model  . ", "output": "The abstract text contains these key words below: 'EBMT' whose type is Method, 'small-sized  bilingual corpus' whose type is Material, 'out-of-domain  bilingual corpus' whose type is Material, 'language model' whose type is Method, 'in-domain  monolingual corpus' whose type is Material, 'EBMT system' whose type is Method, 'evaluation measures' whose type is Metric, 'BLEU score' whose type is Metric, 'NIST score' whose type is Metric, 'out-of-domain  bilingual corpus' whose type is Material, 'language model' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The work presented in this paper is the first step in a project which aims to cluster and summarise  electronic discussions  in the context of  help-desk applications . The eventual objective of this project is to use these  summaries  to assist help-desk users and operators. In this paper, we identify  features  of  electronic discussions  that influence the  clustering process , and offer a  filtering mechanism  that removes undesirable  influences . We tested the  clustering and filtering processes  on  electronic newsgroup discussions , and evaluated their  performance  by means of two experiments :  coarse-level clustering  simple  information retrieval .\n", "output": "The abstract text contains these key words below: 'electronic discussions' whose type is Task, 'help-desk applications' whose type is Task, 'features' whose type is OtherScientificTerm, 'electronic discussions' whose type is Task, 'clustering process' whose type is Method, 'filtering mechanism' whose type is Method, 'clustering and filtering processes' whose type is Method, 'electronic newsgroup discussions' whose type is Material, 'experiments' whose type is Generic, 'coarse-level clustering' whose type is Task, 'information retrieval' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "iagrams are common tools for representing complex concepts , relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation and reasoning, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.", "output": "The abstract text contains these key words below: 'Diagrams' whose type is Method, 'complex concepts' whose type is OtherScientificTerm, 'relationships' whose type is OtherScientificTerm, 'events' whose type is OtherScientificTerm, 'natural images' whose type is Material, 'Understanding natural images' whose type is Task, 'natural images' whose type is Material, 'computer vision' whose type is Task, 'diagram understanding' whose type is Task, 'diagram interpretation and reasoning' whose type is Task, 'task' whose type is Generic, 'structure of a diagram' whose type is OtherScientificTerm, 'Diagram Parse Graphs (DPG)' whose type is Method, 'structure of diagrams' whose type is OtherScientificTerm, 'syntactic parsing of diagrams' whose type is Task, 'DPGs' whose type is Method, 'semantic interpretation and reasoning of diagrams' whose type is Task, 'diagram question answering' whose type is Task, 'LSTM-based method' whose type is Method, 'syntactic parsing of diagrams' whose type is Task, 'DPG-based attention model' whose type is Method, 'diagram question answering' whose type is Task, 'dataset' whose type is Generic, 'diagrams' whose type is Material, 'models' whose type is Generic, 'syntactic parsing and question answering in diagrams' whose type is Task, 'DPGs' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we aim to automatically render aging faces in a personalized way. Basically, a set of age-group specific dictionaries are learned, where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups, and a linear combination of these patterns expresses a particular personalized aging process. Moreover, two factors are taken into consideration in the dictionary learning process. First, beyond the aging dictionaries, each subject may have extra personalized facial characteristics, e.g. mole, which are invariant in the aging process. Second, it is challenging or even impossible to collect faces of all age groups for a particular subject, yet much easier and more practical to get face pairs from neighboring age groups. Thus a personality-aware coupled reconstruction loss is utilized to learn the dictionaries based on face pairs from neighboring age groups. Extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts in term of personalized aging progression, as well as the performance gain for cross-age face verification by synthesizing aging faces.", "output": "The abstract text contains these key words below: 'aging faces' whose type is OtherScientificTerm, 'age-group specific dictionaries' whose type is Method, 'dictionary bases' whose type is OtherScientificTerm, 'aging process pattern' whose type is OtherScientificTerm, 'linear combination' whose type is Method, 'patterns' whose type is Generic, 'personalized aging process' whose type is Task, 'dictionary learning process' whose type is Method, 'aging dictionaries' whose type is Method, 'personalized facial characteristics' whose type is OtherScientificTerm, 'mole' whose type is OtherScientificTerm, 'aging process' whose type is Task, 'personality-aware coupled reconstruction loss' whose type is OtherScientificTerm, 'dictionaries' whose type is Generic, 'solution' whose type is Generic, 'state-of-the-arts' whose type is Generic, 'personalized aging progression' whose type is OtherScientificTerm, 'cross-age face verification' whose type is Task, 'synthesizing aging faces' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present a novel algorithm for estimating the broad 3D geometric structure of outdoor video scenes. Leveraging spatio-temporal video segmentation, we decompose a dynamic scene captured by a video into geometric classes, based on predictions made by region-classifiers that are trained on appearance and motion features. By examining the homogeneity of the prediction, we combine predictions across multiple segmentation hierarchy levels alleviating the need to determine the granularity a priori. We built a novel, extensive dataset on geometric context of video to evaluate our method, consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames. To further scale beyond this dataset, we propose a semi-supervised learning framework to expand the pool of labeled data with high confidence predictions obtained from unlabeled data. Our system produces an accurate prediction of geometric context of video achieving 96% accuracy across main geometric classes.", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, '3D geometric structure of outdoor video scenes' whose type is Task, 'spatio-temporal video segmentation' whose type is Method, 'dynamic scene' whose type is OtherScientificTerm, 'geometric classes' whose type is OtherScientificTerm, 'region-classifiers' whose type is Method, 'appearance and motion features' whose type is OtherScientificTerm, 'segmentation hierarchy levels' whose type is OtherScientificTerm, 'granularity a priori' whose type is OtherScientificTerm, 'dataset' whose type is Generic, 'geometric context of video' whose type is OtherScientificTerm, 'method' whose type is Generic, 'annotated outdoor videos' whose type is OtherScientificTerm, 'dataset' whose type is Generic, 'semi-supervised learning framework' whose type is Method, 'labeled data' whose type is Material, 'high confidence predictions' whose type is OtherScientificTerm, 'unlabeled data' whose type is OtherScientificTerm, 'system' whose type is Generic, 'geometric context of video' whose type is OtherScientificTerm, 'accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n In this paper, a discrimination and robustness oriented  adaptive learning procedure  is proposed to deal with the task of  syntactic ambiguity resolution  . Owing to the problem of  insufficient training data  and  approximation error  introduced by the  language model  , traditional  statistical approaches  , which resolve  ambiguities  by indirectly and implicitly using  maximum likelihood method  , fail to achieve high  performance  in real applications. The proposed method remedies these problems by adjusting the parameters to maximize the  accuracy rate  directly. To make the proposed algorithm robust, the possible variations between the  training corpus  and the real tasks are also taken into consideration by enlarging the  separation margin  between the correct candidate and its competing members. Significant improvement has been observed in the test. The  accuracy rate  of  syntactic disambiguation  is raised from 46.0% to 60.62% by using this novel approach. ", "output": "The abstract text contains these key words below: 'adaptive learning procedure' whose type is Method, 'syntactic ambiguity resolution' whose type is Task, 'insufficient training data' whose type is OtherScientificTerm, 'approximation error' whose type is OtherScientificTerm, 'language model' whose type is Method, 'statistical approaches' whose type is Method, 'ambiguities' whose type is OtherScientificTerm, 'maximum likelihood method' whose type is Method, 'method' whose type is Generic, 'problems' whose type is Generic, 'algorithm' whose type is Generic, 'separation margin' whose type is OtherScientificTerm, 'accuracy rate' whose type is Metric, 'syntactic disambiguation' whose type is Task, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Combination methods  are an effective way of improving  system performance  . This paper examines the benefits of  system combination  for  unsupervised WSD  . We investigate several  voting- and arbiter-based combination strategies  over a diverse pool of  unsupervised WSD systems  . Our  combination methods  rely on  predominant senses  which are derived automatically from  raw text  . Experiments using the  SemCor  and  Senseval-3 data sets  demonstrate that our ensembles yield significantly better results when compared with state-of-the-art. ", "output": "The abstract text contains these key words below: 'Combination methods' whose type is Method, 'system combination' whose type is Method, 'unsupervised WSD' whose type is Task, 'voting- and arbiter-based combination strategies' whose type is Method, 'unsupervised WSD systems' whose type is Method, 'combination methods' whose type is Method, 'predominant senses' whose type is OtherScientificTerm, 'raw text' whose type is Material, 'SemCor  and  Senseval-3 data sets' whose type is Material, 'ensembles' whose type is Generic, 'state-of-the-art' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The  psycholinguistic literature  provides evidence for  syntactic priming , i.e., the tendency to repeat structures. This paper describes a method for incorporating  priming  into an  incremental probabilistic parser . Three models are compared, which involve  priming  of  rules  between  sentences , within  sentences , and within  coordinate structures . These models simulate the reading time advantage for  parallel structures  found in  human data , and also yield a small increase in overall  parsing accuracy .\n", "output": "The abstract text contains these key words below: 'psycholinguistic literature' whose type is OtherScientificTerm, 'syntactic priming' whose type is OtherScientificTerm, 'method' whose type is Generic, 'priming' whose type is OtherScientificTerm, 'incremental probabilistic parser' whose type is Method, 'priming' whose type is OtherScientificTerm, 'rules' whose type is OtherScientificTerm, 'coordinate structures' whose type is OtherScientificTerm, 'parallel structures' whose type is OtherScientificTerm, 'human data' whose type is Material, 'parsing accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper, we cast the problem of point cloud matching as a shape matching problem by transforming each of the given point clouds into a shape representation called the Schr\u00f6dinger distance transform (SDT) representation. This is achieved by solving a static Schr\u00f6dinger equation instead of the corresponding static Hamilton-Jacobi equation in this setting. The SDT representation is an analytic expression and following the theoretical physics literature, can be normalized to have unit L2 norm-making it a square-root density, which is identified with a point on a unit Hilbert sphere, whose intrinsic geometry is fully known. The Fisher-Rao metric, a natural metric for the space of densities leads to analytic expressions for the geodesic distance between points on this sphere. In this paper, we use the well known Riemannian framework never before used for point cloud matching, and present a novel matching algorithm. We pose point set matching under rigid and non-rigid transformations in this framework and solve for the transformations using standard nonlinear optimization techniques. Finally, to evaluate the performance of our algorithm-dubbed SDTM-we present several synthetic and real data examples along with extensive comparisons to state-of-the-art techniques. The experiments show that our algorithm outperforms state-of-the-art point set registration algorithms on many quantitative metrics.", "output": "The abstract text contains these key words below: 'point cloud matching' whose type is Task, 'shape matching problem' whose type is Task, 'point clouds' whose type is OtherScientificTerm, 'shape representation' whose type is Method, 'Schr\u00f6dinger distance transform (SDT) representation' whose type is Method, 'static Schr\u00f6dinger equation' whose type is OtherScientificTerm, 'static Hamilton-Jacobi equation' whose type is OtherScientificTerm, 'SDT representation' whose type is Method, 'analytic expression' whose type is OtherScientificTerm, 'theoretical physics literature' whose type is OtherScientificTerm, 'unit L2 norm' whose type is OtherScientificTerm, 'it' whose type is Generic, 'square-root density' whose type is OtherScientificTerm, 'unit Hilbert sphere' whose type is OtherScientificTerm, 'intrinsic geometry' whose type is OtherScientificTerm, 'Fisher-Rao metric' whose type is Metric, 'natural metric' whose type is Generic, 'space of densities' whose type is OtherScientificTerm, 'analytic expressions' whose type is OtherScientificTerm, 'geodesic distance' whose type is OtherScientificTerm, 'sphere' whose type is Generic, 'Riemannian framework' whose type is Method, 'point cloud matching' whose type is Task, 'matching algorithm' whose type is Method, 'point set matching' whose type is Task, 'rigid and non-rigid transformations' whose type is OtherScientificTerm, 'framework' whose type is Generic, 'transformations' whose type is Generic, 'nonlinear optimization techniques' whose type is Method, 'algorithm' whose type is Method, 'dubbed SDTM' whose type is Method, 'synthetic and real data examples' whose type is Material, 'state-of-the-art techniques' whose type is Generic, 'algorithm' whose type is Generic, 'point set registration algorithms' whose type is Method, 'quantitative metrics' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "espite their success, convolutional neural networks are computationally expensive because they must examine all image locations. Stochastic attention-based models have been shown to improve computational efficiency at test time, but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates. Borrowing techniques from the literature on training deep generative models, we present the Wake-Sleep Recurrent Attention Model, a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients. We show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation.", "output": "The abstract text contains these key words below: 'convolutional neural networks' whose type is Method, 'they' whose type is Generic, 'Stochastic attention-based models' whose type is Method, 'computational efficiency' whose type is Metric, 'they' whose type is Generic, 'intractable posterior inference' whose type is Task, 'stochastic gradient estimates' whose type is Task, 'Borrowing techniques' whose type is Method, 'deep generative models' whose type is Method, 'Wake-Sleep Recurrent Attention Model' whose type is Method, 'method' whose type is Generic, 'stochastic attention networks' whose type is Method, 'posterior inference' whose type is Task, 'stochastic gradients' whose type is OtherScientificTerm, 'method' whose type is Generic, 'training time' whose type is Metric, 'stochastic attention networks' whose type is Method, 'image classification' whose type is Task, 'caption generation' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe a set of experiments to explore  statistical techniques  for ranking and selecting the best  translations  in a  graph  of  translation hypotheses . In a previous paper (Carl, 2007) we have described how the  hypotheses graph  is generated through  shallow mapping  and  permutation rules . We have given examples of its  nodes  consisting of  vectors representing morpho-syntactic properties  of  words  and  phrases . This paper describes a number of methods for elaborating  statistical feature functions  from some of the  vector components . The  feature functions  are trained off-line on different types of  text  and their  log-linear combination  is then used to retrieve the best M  translation paths  in the  graph . We compare two  language modelling toolkits , the  CMU  and the  SRI toolkit  and arrive at three results: 1)  word-lemma based feature function models  produce better results than  token-based models , 2) adding a  PoS-tag feature function  to the  word-lemma model  improves the output and 3)  weights  for  lexical translations  are suitable if the  training material  is similar to the  texts  to be translated.", "output": "The abstract text contains these key words below: 'statistical techniques' whose type is Method, 'ranking' whose type is Task, 'graph  of  translation hypotheses' whose type is OtherScientificTerm, 'hypotheses graph' whose type is OtherScientificTerm, 'shallow mapping' whose type is Method, 'permutation rules' whose type is Method, 'nodes' whose type is OtherScientificTerm, 'vectors representing morpho-syntactic properties' whose type is OtherScientificTerm, 'methods' whose type is Generic, 'statistical feature functions' whose type is OtherScientificTerm, 'vector components' whose type is Method, 'feature functions' whose type is OtherScientificTerm, 'log-linear combination' whose type is OtherScientificTerm, 'translation paths' whose type is OtherScientificTerm, 'graph' whose type is OtherScientificTerm, 'language modelling toolkits' whose type is Method, 'CMU  and the  SRI toolkit' whose type is Method, 'word-lemma based feature function models' whose type is Method, 'token-based models' whose type is Method, 'PoS-tag feature function' whose type is OtherScientificTerm, 'word-lemma model' whose type is Method, 'lexical translations' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper shows that it is very often possible to identify the  source language  of medium-length speeches in the  EUROPARL corpus  on the basis of  frequency counts  of  word n-grams  (87.2%-96.7%  accuracy  depending on  classification method ). The paper also examines in detail which  positive markers  are most powerful and identifies a number of linguistic aspects as well as culture- and domain-related ones.\n", "output": "The abstract text contains these key words below: 'medium-length speeches' whose type is Material, 'EUROPARL corpus' whose type is Material, 'frequency counts  of  word n-grams' whose type is Method, 'word n-grams' whose type is OtherScientificTerm, 'accuracy' whose type is Metric, 'classification method' whose type is Method, 'positive markers' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Using  natural language processing , we carried out a trend survey on  Japanese natural language processing studies  that have been done over the last ten years. We determined the changes in the number of papers published for each research organization and on each research area as well as the relationship between research organizations and research areas. This paper is useful for both recognizing trends in  Japanese NLP  and constructing a method of supporting trend surveys using  NLP .\n", "output": "The abstract text contains these key words below: 'natural language processing' whose type is Method, 'trend survey' whose type is Task, 'trend survey on  Japanese natural language processing studies' whose type is Task, 'Japanese natural language processing studies' whose type is Material, 'Japanese NLP' whose type is Material, 'trend surveys' whose type is Task, 'NLP' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper proposes that  sentence analysis  should be treated as  defeasible reasoning  , and presents such a treatment for  Japanese sentence analyses  using an  argumentation system  by Konolige, which is a  formalization  of  defeasible reasoning  , that includes  arguments  and  defeat rules  that capture  defeasibility  . ", "output": "The abstract text contains these key words below: 'sentence analysis' whose type is Task, 'defeasible reasoning' whose type is Method, 'treatment' whose type is Generic, 'Japanese sentence analyses' whose type is Task, 'argumentation system' whose type is Method, 'formalization  of  defeasible reasoning' whose type is Method, 'defeasible reasoning' whose type is Method, 'arguments' whose type is OtherScientificTerm, 'defeat rules' whose type is OtherScientificTerm, 'defeasibility' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper describes a novel  system  for  acquiring adjectival subcategorization frames  ( scfs ) and associated frequency information from  English   corpus data . The  system  incorporates a  decision-tree classifier  for 30  scf types  which tests for the presence of  grammatical relations  ( grs ) in the  output  of a robust  statistical parser . It uses a powerful  pattern-matching language  to classify  grs  into  frames  hierarchically in a way that mirrors  inheritance-based lexica . The  experiments  show that the  system  is able to detect  scf types  with  70% precision  and  66% recall rate . A new  tool  for  linguistic annotation  of  scfs  in  corpus data  is also introduced which can considerably alleviate the process of obtaining  training and test data  for  subcategorization acquisition .\n", "output": "The abstract text contains these key words below: 'system' whose type is Generic, 'acquiring adjectival subcategorization frames  ( scfs )' whose type is Task, 'subcategorization frames  ( scfs )' whose type is OtherScientificTerm, 'English   corpus data' whose type is Material, 'system' whose type is Generic, 'decision-tree classifier' whose type is Method, 'grammatical relations  ( grs )' whose type is OtherScientificTerm, 'robust  statistical parser' whose type is Method, 'It' whose type is Generic, 'pattern-matching language' whose type is OtherScientificTerm, 'grs' whose type is OtherScientificTerm, 'inheritance-based lexica' whose type is OtherScientificTerm, 'system' whose type is Generic, 'precision' whose type is Metric, 'recall' whose type is Metric, 'tool' whose type is Generic, 'linguistic annotation  of  scfs' whose type is Task, 'scfs' whose type is OtherScientificTerm, 'training and test data' whose type is Material, 'subcategorization acquisition' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed (e.g. face detection or database retrieval). In such domains a cascade of simple classifiers each trained to achieve high detection rates and modest false positive rates can yield a final detector with many desirable features: including high detection rates, very low false positive rates, and fast performance. Achieving extremely high detection rates, rather than low error, is not a task typically addressed by machine learning algorithms. We propose a new variant of AdaBoost as a mechanism for training the simple classifiers used in the cascade. Experimental results in the domain of face detection show the training algorithm yields significant improvements in performance over conventional AdaBoost. The final face detection system can process 15 frames per second, achieves over 90% detection, and a false positive rate of 1 in a 1,000,000.", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'fast detection' whose type is Task, 'distribution' whose type is OtherScientificTerm, 'positive and negative examples' whose type is Material, 'face detection' whose type is Task, 'database retrieval' whose type is Task, 'cascade of simple classifiers' whose type is Method, 'classifiers' whose type is Method, 'detection rates' whose type is Metric, 'modest false positive rates' whose type is Metric, 'detector' whose type is Generic, 'features' whose type is OtherScientificTerm, 'detection rates' whose type is Metric, 'false positive rates' whose type is Metric, 'fast performance' whose type is Metric, 'detection rates' whose type is Metric, 'low error' whose type is Metric, 'machine learning algorithms' whose type is Method, 'AdaBoost' whose type is Method, 'classifiers' whose type is OtherScientificTerm, 'cascade' whose type is Generic, 'face detection' whose type is Task, 'training algorithm' whose type is Method, 'AdaBoost' whose type is Method, 'face detection system' whose type is Method, 'detection' whose type is Metric, 'false positive rate' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Multimodal interfaces require effective  parsing  and understanding of  utterances  whose content is distributed across multiple input modes. Johnston 1998 presents an approach in which strategies for  multimodal integration  are stated declaratively using a  unification-based grammar  that is used by a  multidimensional chart parser  to compose inputs. This approach is highly expressive and supports a broad class of  interfaces , but offers only limited potential for mutual compensation among the input modes, is subject to significant concerns in terms of computational complexity, and complicates selection among alternative multimodal interpretations of the input. In this paper, we present an alternative approach in which  multimodal parsing and understanding  are achieved using a  weighted finite-state device  which takes  speech and gesture streams  as inputs and outputs their joint interpretation. This approach is significantly more efficient, enables tight-coupling of multimodal understanding with  speech recognition , and provides a general probabilistic framework for  multimodal ambiguity resolution .\n", "output": "The abstract text contains these key words below: 'Multimodal interfaces' whose type is Task, 'parsing' whose type is Method, 'approach' whose type is Generic, 'multimodal integration' whose type is Task, 'unification-based grammar' whose type is Method, 'multidimensional chart parser' whose type is Method, 'approach' whose type is Generic, 'interfaces' whose type is OtherScientificTerm, 'computational complexity' whose type is Metric, 'approach' whose type is Generic, 'multimodal parsing and understanding' whose type is Task, 'weighted finite-state device' whose type is Method, 'speech and gesture streams' whose type is Material, 'approach' whose type is Generic, 'multimodal understanding' whose type is Task, 'speech recognition' whose type is Task, 'multimodal ambiguity resolution' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "wo main classes of approaches have been studied to perform monocular nonrigid 3D reconstruction: Template-based methods and Non-rigid Structure from Motion techniques. While the first ones have been applied to reconstruct poorly-textured surfaces, they assume the availability of a 3D shape model prior to reconstruction. By contrast, the second ones do not require such a shape template, but, instead, rely on points being tracked throughout a video sequence, and are thus ill-suited to handle poorly-textured surfaces. In this paper, we introduce a template-free approach to reconstructing a poorly-textured, deformable surface. To this end, we leverage surface isometry and formulate 3D reconstruction as the joint problem of non-rigid image registration and depth estimation. Our experiments demonstrate that our approach yields much more accurate 3D reconstructions than state-of-the-art techniques.", "output": "The abstract text contains these key words below: 'approaches' whose type is Generic, 'monocular nonrigid 3D reconstruction' whose type is Task, 'Template-based methods' whose type is Method, 'Non-rigid Structure from Motion techniques' whose type is Method, 'ones' whose type is Generic, 'poorly-textured surfaces' whose type is OtherScientificTerm, 'they' whose type is Generic, '3D shape model' whose type is Method, 'reconstruction' whose type is Task, 'ones' whose type is Generic, 'shape template' whose type is OtherScientificTerm, 'video sequence' whose type is Material, 'poorly-textured surfaces' whose type is OtherScientificTerm, 'template-free approach' whose type is Method, 'poorly-textured, deformable surface' whose type is OtherScientificTerm, 'surface isometry' whose type is Method, '3D reconstruction' whose type is Task, 'joint problem of non-rigid image registration and depth estimation' whose type is Task, 'approach' whose type is Generic, '3D reconstructions' whose type is Task, 'state-of-the-art techniques' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper introduces a robust  interactive method for speech understanding  . The  generalized LR parsing  is enhanced in this approach.  Parsing  proceeds from left to right correcting minor errors. When a very noisy  portion  is detected, the  parser  skips that  portion  using a fake  non-terminal symbol  . The unidentified  portion  is resolved by  re-utterance  of that  portion  which is parsed very efficiently by using the  parse record  of the first  utterance  . The  user  does not have to speak the whole  sentence  again. This method is also capable of handling  unknown words  , which is important in practical systems. Detected  unknown words  can be incrementally incorporated into the  dictionary  after the interaction with the  user  . A  pilot system  has shown great effectiveness of this approach. ", "output": "The abstract text contains these key words below: 'robust  interactive method' whose type is Generic, 'speech understanding' whose type is Task, 'generalized LR parsing' whose type is Method, 'approach' whose type is Generic, 'Parsing' whose type is Method, 'parser' whose type is Method, 'non-terminal symbol' whose type is OtherScientificTerm, 'method' whose type is Generic, 'unknown words' whose type is OtherScientificTerm, 'approach' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e address the problem of populating object category detection datasets with dense, per-object 3D reconstructions , bootstrapped from class labels, ground truth figure-ground segmentations and a small set of keypoint annotations. Our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion, then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions. The visual hull sampling process attempts to intersect an object's projection cone with the cones of minimal subsets of other similar objects among those pictured from certain vantage points. We show that our method is able to produce convincing per-object 3D reconstructions on one of the most challenging existing object-category detection datasets, PASCAL VOC. Our results may re-stimulate once popular geometry-oriented model-based recognition approaches.", "output": "The abstract text contains these key words below: 'object category detection datasets' whose type is Material, 'per-object 3D reconstructions' whose type is Task, 'ground truth figure-ground segmentations' whose type is Method, 'keypoint annotations' whose type is Method, 'algorithm' whose type is Generic, 'camera viewpoint' whose type is OtherScientificTerm, 'rigid structure-from-motion' whose type is OtherScientificTerm, 'object shapes' whose type is OtherScientificTerm, 'visual hull proposals' whose type is OtherScientificTerm, 'loose within-class shape similarity assumptions' whose type is OtherScientificTerm, 'visual hull sampling process' whose type is Method, 'projection cone' whose type is OtherScientificTerm, 'method' whose type is Generic, 'per-object 3D reconstructions' whose type is Task, 'object-category detection datasets' whose type is Material, 'PASCAL VOC' whose type is Material, 'geometry-oriented model-based recognition approaches' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we describe a  phrase-based unigram model  for  statistical machine translation  that uses a much simpler set of  model parameters  than similar  phrase-based models  . The  units of translation  are  blocks  - pairs of  phrases  . During  decoding  , we use a  block unigram model  and a  word-based trigram language model  . During  training  , the  blocks  are learned from  source interval projections  using an underlying  word alignment  . We show experimental results on  block selection criteria  based on  unigram  counts and  phrase  length. ", "output": "The abstract text contains these key words below: 'phrase-based unigram model' whose type is Method, 'statistical machine translation' whose type is Task, 'model parameters' whose type is OtherScientificTerm, 'phrase-based models' whose type is Method, 'blocks' whose type is OtherScientificTerm, 'decoding' whose type is Task, 'block unigram model' whose type is Method, 'word-based trigram language model' whose type is Method, 'training' whose type is Task, 'blocks' whose type is OtherScientificTerm, 'source interval projections' whose type is Method, 'word alignment' whose type is Method, 'block selection criteria' whose type is Metric, 'unigram  counts' whose type is OtherScientificTerm, 'phrase  length' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Machine transliteration/back-transliteration  plays an important role in many  multilingual speech and language applications . In this paper, a novel framework for  machine transliteration/backtransliteration  that allows us to carry out  direct orthographical mapping (DOM)  between two different  languages  is presented. Under this framework, a  joint source-channel transliteration model , also called  n-gram transliteration model (n-gram TM) , is further proposed to model the  transliteration process . We evaluate the proposed methods through several  transliteration/backtransliteration experiments  for  English/Chinese and English/Japanese language pairs . Our study reveals that the proposed method not only reduces an extensive  system development effort  but also improves the  transliteration accuracy  significantly.\n", "output": "The abstract text contains these key words below: 'Machine transliteration/back-transliteration' whose type is Task, 'multilingual speech and language applications' whose type is Task, 'framework' whose type is Generic, 'machine transliteration/backtransliteration' whose type is Task, 'direct orthographical mapping (DOM)' whose type is Method, 'framework' whose type is Generic, 'joint source-channel transliteration model' whose type is Method, 'n-gram transliteration model (n-gram TM)' whose type is Method, 'transliteration process' whose type is Method, 'methods' whose type is Generic, 'transliteration/backtransliteration' whose type is Task, 'English/Chinese and English/Japanese language pairs' whose type is Material, 'method' whose type is Generic, 'transliteration accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e introduce a method to accelerate the evaluation of object detection cascades with the help of a divide-and-conquer procedure in the space of candidate regions. Compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation, the proposed method requires fewer evaluations of the classifier functions, thereby speeding up the search. Furthermore, we show how the recently developed efficient subwindow search (ESS) procedure [11] can be integrated into the last stage of our method. This allows us to use our method to act not only as a faster procedure for cascade evaluation, but also as a tool to perform efficient branch-and-bound object detection with nonlinear quality functions, in particular kernel-ized support vector machines. Experiments on the PASCAL VOC 2006 dataset show an acceleration of more than 50% by our method compared to standard cascade evaluation.", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'evaluation of object detection cascades' whose type is Task, 'divide-and-conquer procedure' whose type is Method, 'space of candidate regions' whose type is Method, 'exhaustive procedure' whose type is Method, 'cascade evaluation' whose type is Task, 'method' whose type is Generic, 'classifier functions' whose type is OtherScientificTerm, 'search' whose type is Task, 'subwindow search (ESS) procedure' whose type is Method, 'method' whose type is Generic, 'method' whose type is Generic, 'cascade evaluation' whose type is Task, 'branch-and-bound object detection' whose type is Task, 'nonlinear quality functions' whose type is OtherScientificTerm, 'kernel-ized support vector machines' whose type is Method, 'PASCAL VOC 2006 dataset' whose type is Material, 'method' whose type is Generic, 'cascade evaluation' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This study presents a  method to automatically acquire paraphrases  using  bilingual corpora , which utilizes the  bilingual dependency relations  obtained by projecting a  monolingual dependency parse  onto the other language sentence based on  statistical alignment techniques . Since the  paraphrasing method  is capable of clearly disambiguating the  sense  of an original  phrase  using the  bilingual context  of  dependency relation , it would be possible to obtain interchangeable  paraphrases  under a given  context . Also, we provide an advanced method to acquire  generalized translation knowledge  using the extracted  paraphrases . We applied the method to acquire the  generalized translation knowledge  for  Korean-English translation . Through experiments with  parallel corpora  of a  Korean and English language pairs , we show that our  paraphrasing method  effectively extracts  paraphrases  with high  precision , 94.3% and 84.6% respectively for  Korean  and  English , and the  translation knowledge  extracted from the  bilingual corpora  could be generalized successfully using the  paraphrases  with the 12.5%  compression ratio .\n", "output": "The abstract text contains these key words below: 'method to automatically acquire paraphrases' whose type is Method, 'bilingual corpora' whose type is Material, 'bilingual dependency relations' whose type is OtherScientificTerm, 'monolingual dependency parse' whose type is OtherScientificTerm, 'statistical alignment techniques' whose type is Method, 'paraphrasing method' whose type is Method, 'bilingual context  of  dependency relation' whose type is OtherScientificTerm, 'paraphrases' whose type is OtherScientificTerm, 'method' whose type is Generic, 'generalized translation knowledge' whose type is OtherScientificTerm, 'paraphrases' whose type is OtherScientificTerm, 'method' whose type is Generic, 'generalized translation knowledge' whose type is OtherScientificTerm, 'Korean-English translation' whose type is Task, 'parallel corpora  of a  Korean and English language pairs' whose type is Material, 'paraphrasing method' whose type is Method, 'paraphrases' whose type is OtherScientificTerm, 'precision' whose type is Metric, 'Korean' whose type is Material, 'English' whose type is Material, 'translation knowledge' whose type is OtherScientificTerm, 'bilingual corpora' whose type is Material, 'paraphrases' whose type is OtherScientificTerm, 'compression ratio' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper explores the issue of using different  co-occurrence similarities  between  terms  for separating  query terms  that are useful for  retrieval  from those that are harmful. The hypothesis under examination is that  useful terms  tend to be more similar to each other than to other  query terms . Preliminary experiments with similarities computed using  first-order and second-order co-occurrence  seem to confirm the hypothesis.  Term similarities  could then be used for determining which  query terms  are useful and best reflect the user's information need. A possible application would be to use this source of evidence for tuning the  weights  of the  query terms .\n", "output": "The abstract text contains these key words below: 'co-occurrence similarities' whose type is OtherScientificTerm, 'query terms' whose type is Generic, 'retrieval' whose type is Task, 'those' whose type is Generic, 'useful terms' whose type is Generic, 'query terms' whose type is Generic, 'similarities' whose type is Generic, 'first-order and second-order co-occurrence' whose type is OtherScientificTerm, 'Term similarities' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present an image set classification algorithm based on unsupervised clustering of labeled training and unla-beled test data where labels are only used in the stopping criterion. The probability distribution of each class over the set of clusters is used to define a true set based similarity measure. To this end, we propose an iterative sparse spectral clustering algorithm. In each iteration, a proximity matrix is efficiently recomputed to better represent the local subspace structure. Initial clusters capture the global data structure and finer clusters at the later stages capture the subtle class differences not visible at the global scale. Image sets are compactly represented with multiple Grass-mannian manifolds which are subsequently embedded in Euclidean space with the proposed spectral clustering algorithm. We also propose an efficient eigenvector solver which not only reduces the computational cost of spectral clustering by many folds but also improves the clustering quality and final classification results. Experiments on five standard datasets and comparison with seven existing techniques show the efficacy of our algorithm.", "output": "The abstract text contains these key words below: 'image set classification algorithm' whose type is Method, 'unsupervised clustering' whose type is Method, 'labeled training and unla-beled test data' whose type is Material, 'stopping criterion' whose type is OtherScientificTerm, 'probability distribution' whose type is OtherScientificTerm, 'clusters' whose type is OtherScientificTerm, 'set based similarity measure' whose type is Metric, 'iterative sparse spectral clustering algorithm' whose type is Method, 'proximity matrix' whose type is OtherScientificTerm, 'local subspace structure' whose type is OtherScientificTerm, 'Initial clusters' whose type is OtherScientificTerm, 'global data structure' whose type is OtherScientificTerm, 'finer clusters' whose type is OtherScientificTerm, 'subtle class differences' whose type is OtherScientificTerm, 'global scale' whose type is OtherScientificTerm, 'Image sets' whose type is Material, 'Grass-mannian manifolds' whose type is Method, 'Euclidean space' whose type is OtherScientificTerm, 'spectral clustering algorithm' whose type is Method, 'eigenvector solver' whose type is Method, 'computational cost' whose type is Metric, 'spectral clustering' whose type is Method, 'clustering quality' whose type is Metric, 'classification results' whose type is Metric, 'datasets' whose type is Material, 'algorithm' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ast algorithms for nearest neighbor (NN) search have in large part focused on 2 distance. Here we develop an approach for 1 distance that begins with an explicit and exactly distance-preserving embedding of the points into 2 2. We show how this can efficiently be combined with random-projection based methods for 2 NN search, such as locality-sensitive hashing (LSH) or random projection trees. We rigorously establish the correctness of the methodology and show by experimentation using LSH that it is competitive in practice with available alternatives.", "output": "The abstract text contains these key words below: 'Fast algorithms' whose type is Generic, 'nearest neighbor (NN) search' whose type is Task, 'distance' whose type is OtherScientificTerm, 'approach' whose type is Generic, '1 distance' whose type is OtherScientificTerm, 'distance-preserving embedding' whose type is OtherScientificTerm, 'this' whose type is Generic, 'random-projection based methods' whose type is Method, 'NN search' whose type is Task, 'locality-sensitive hashing (LSH)' whose type is Method, 'random projection trees' whose type is Method, 'LSH' whose type is Method, 'it' whose type is Generic, 'alternatives' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Empirical experience and observations have shown us when powerful and highly tunable  classifiers  such as  maximum entropy classifiers ,  boosting  and  SVMs  are applied to  language processing tasks , it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point. To further improve performance, various  error correction mechanisms  have been developed, but in practice, most of them cannot be relied on to predictably improve performance on  unseen data ; indeed, depending upon the  test set , they are as likely to degrade accuracy as to improve it. This problem is especially severe if the  base classifier  has already been finely tuned. In recent work, we introduced  N-fold Templated Piped Correction, or NTPC (\"nitpick\") , an intriguing  error corrector  that is designed to work in these extreme operating conditions. Despite its simplicity, it consistently and robustly improves the accuracy of existing highly accurate  base models . This paper investigates some of the more surprising claims made by  NTPC , and presents experiments supporting an  Occam's Razor argument  that more complex models are damaging or unnecessary in practice.\n", "output": "The abstract text contains these key words below: 'classifiers' whose type is Method, 'maximum entropy classifiers' whose type is Method, 'boosting' whose type is Method, 'SVMs' whose type is Method, 'language processing tasks' whose type is Task, 'error correction mechanisms' whose type is Method, 'base classifier' whose type is Generic, 'N-fold Templated Piped Correction, or NTPC (\"nitpick\")' whose type is Method, 'error corrector' whose type is Method, 'it' whose type is Generic, 'base models' whose type is Generic, 'NTPC' whose type is Method, 'Occam's Razor argument' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n  Chart parsing  is  directional  in the sense that it works from the starting point (usually the beginning of the sentence) extending its activity usually in a rightward manner. We shall introduce the concept of a  chart  that works outward from  islands  and makes sense of as much of the  sentence  as it is actually possible, and after that will lead to predictions of missing  fragments  . So, for any place where the easily identifiable  fragments  occur in the  sentence  , the process will extend to both the left and the right of the  islands  , until possibly completely missing  fragments  are reached. At that point, by virtue of the fact that both a left and a right context were found,  heuristics  can be introduced that predict the nature of the missing  fragments  . ", "output": "The abstract text contains no keywords"}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n We describe the methods and hardware that we are using to produce a real-time demonstration of an  integrated Spoken Language System  . We describe algorithms that greatly reduce the computation needed to compute the  N-Best sentence hypotheses  . To avoid  grammar coverage problems  we use a  fully-connected first-order statistical class grammar  . The  speech-search algorithm  is implemented on a  board  with a single  Intel i860 chip  , which provides a factor of 5 speed-up over a  SUN 4  for  straight C code  . The  board  plugs directly into the  VME bus  of the  SUN4  , which controls the system and contains the  natural language system  and  application back end  . ", "output": "The abstract text contains these key words below: 'methods' whose type is Generic, 'hardware' whose type is Generic, 'integrated Spoken Language System' whose type is Method, 'algorithms' whose type is Generic, 'N-Best sentence hypotheses' whose type is OtherScientificTerm, 'grammar coverage problems' whose type is OtherScientificTerm, 'fully-connected first-order statistical class grammar' whose type is Method, 'speech-search algorithm' whose type is Method, 'board' whose type is OtherScientificTerm, 'Intel i860 chip' whose type is OtherScientificTerm, 'SUN 4' whose type is OtherScientificTerm, 'straight C code' whose type is OtherScientificTerm, 'board' whose type is OtherScientificTerm, 'VME bus' whose type is OtherScientificTerm, 'SUN4' whose type is OtherScientificTerm, 'system' whose type is Generic, 'natural language system' whose type is Method, 'application back end' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we discuss a proposed  user knowledge modeling architecture  for the  ICICLE system , a  language tutoring application  for deaf learners of  written English . The model will represent the  language proficiency  of the user and is designed to be referenced during both  writing analysis  and  feedback production . We motivate our  model design  by citing relevant research on  second language and cognitive skill acquisition , and briefly discuss preliminary empirical evidence supporting the  design . We conclude by showing how our  design  can provide a rich and  robust information base  to a language assessment / correction application by modeling  user proficiency  at a high level of granularity and specificity.\n", "output": "The abstract text contains these key words below: 'user knowledge modeling architecture' whose type is Method, 'ICICLE system' whose type is Task, 'language tutoring application' whose type is Task, 'deaf learners' whose type is OtherScientificTerm, 'written English' whose type is Material, 'model' whose type is Generic, 'writing analysis' whose type is Task, 'feedback production' whose type is Task, 'model design' whose type is Generic, 'second language and cognitive skill acquisition' whose type is Task, 'design' whose type is Generic, 'design' whose type is Generic, 'language assessment / correction application' whose type is Task, 'user proficiency' whose type is OtherScientificTerm, 'granularity' whose type is Metric, 'specificity' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This abstract describes a  natural language system  which deals usefully with  ungrammatical input  and describes some actual and potential applications of it in  computer aided second language learning  . However, this is not the only area in which the principles of the system might be used, and the aim in building it was simply to demonstrate the workability of the general mechanism, and provide a framework for assessing developments of it. ", "output": "The abstract text contains these key words below: 'natural language system' whose type is Method, 'ungrammatical input' whose type is OtherScientificTerm, 'it' whose type is Generic, 'computer aided second language learning' whose type is Task, 'this' whose type is Generic, 'system' whose type is Generic, 'it' whose type is Generic, 'it' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Both  rhetorical structure  and  punctuation  have been helpful in  discourse processing . Based on a  corpus annotation project , this paper reports the  discursive usage  of 6  Chinese punctuation marks  in  news commentary texts :  Colon ,  Dash ,  Ellipsis ,  Exclamation Mark ,  Question Mark , and  Semicolon . The  rhetorical patterns  of these marks are compared against  patterns  around  cue phrases  in general. Results show that these  Chinese punctuation marks , though fewer in number than  cue phrases , are easy to identify, have strong correlation with certain relations, and can be used as distinctive indicators of nuclearity in  Chinese texts .\n", "output": "The abstract text contains these key words below: 'rhetorical structure' whose type is OtherScientificTerm, 'punctuation' whose type is OtherScientificTerm, 'discourse processing' whose type is Task, 'corpus annotation project' whose type is Task, 'discursive usage  of 6  Chinese punctuation marks' whose type is Task, 'Chinese punctuation marks' whose type is OtherScientificTerm, 'news commentary texts' whose type is Material, 'Colon' whose type is OtherScientificTerm, 'Dash' whose type is OtherScientificTerm, 'Ellipsis' whose type is OtherScientificTerm, 'Exclamation Mark' whose type is OtherScientificTerm, 'Question Mark' whose type is OtherScientificTerm, 'Semicolon' whose type is OtherScientificTerm, 'rhetorical patterns' whose type is OtherScientificTerm, 'marks' whose type is Generic, 'patterns  around  cue phrases' whose type is OtherScientificTerm, 'Chinese punctuation marks' whose type is OtherScientificTerm, 'cue phrases' whose type is OtherScientificTerm, 'indicators of nuclearity' whose type is OtherScientificTerm, 'Chinese texts' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper addresses the issue of  word-sense ambiguity  in extraction from  machine-readable resources  for the construction of  large-scale knowledge sources . We describe two experiments: one which ignored  word-sense distinctions , resulting in 6.3% accuracy for  semantic classification  of  verbs  based on (Levin, 1993); and one which exploited  word-sense distinctions , resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that  verb semantics  and  syntactic behavior  are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving  semantic information  from  syntactic cues  if we first divide the  syntactic cues  into distinct groupings that correlate with different  word senses . Finally, we show that we can provide effective acquisition techniques for novel  word senses  using a combination of online sources.\n", "output": "The abstract text contains these key words below: 'word-sense ambiguity' whose type is Task, 'machine-readable resources' whose type is Material, 'construction of  large-scale knowledge sources' whose type is Task, 'word-sense distinctions' whose type is OtherScientificTerm, 'accuracy' whose type is Metric, 'semantic classification' whose type is Method, 'word-sense distinctions' whose type is OtherScientificTerm, 'accuracy' whose type is Metric, 'verb semantics' whose type is OtherScientificTerm, 'syntactic behavior' whose type is OtherScientificTerm, 'semantic information' whose type is OtherScientificTerm, 'syntactic cues' whose type is OtherScientificTerm, 'syntactic cues' whose type is OtherScientificTerm, 'word senses' whose type is OtherScientificTerm, 'techniques' whose type is Generic, 'word senses' whose type is OtherScientificTerm, 'online sources' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ecent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games.\n", "output": "The abstract text contains these key words below: 'computer vision' whose type is Task, 'high-capacity models' whose type is Method, 'large datasets' whose type is Material, 'large datasets' whose type is Material, 'pixel-level labels' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'pixel-accurate semantic label maps' whose type is OtherScientificTerm, 'images' whose type is Material, 'modern computer games' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "peech-based depression detection has gained importance in recent years, but most research has used relatively quiet conditions or examined a single corpus per study. Little is thus known about the robustness of speech cues in the wild. This study compares the effect of noise and reverberation on depression prediction using 1) standard mel-frequency cepstral coefficients (MFCCs), and 2) features designed for noise robustness, damped oscillator cepstral coefficients (DOCCs). Data come from the 2014 AudioVisual Emotion Recognition Challenge (AVEC). Results using additive noise and reverberation reveal a consistent pattern of findings for multiple evaluation metrics under both matched and mismatched conditions. First and most notably: standard MFCC features suffer dramatically under test/train mismatch for both noise and reverberation; DOCC features are far more robust. Second, including higher-order cepstral coefficients is generally beneficial. Third, artificial neural networks tend to outperform support vector regression. Fourth, spontaneous speech appears to offer better robustness than read speech. Finally, a cross-corpus (and cross-language) experiment reveals better noise and reverberation robustness for DOCCs than for MFCCs. Implications and future directions for real-world robust depression detection are discussed.", "output": "The abstract text contains these key words below: 'Speech-based depression detection' whose type is Task, 'robustness' whose type is Metric, 'speech cues' whose type is OtherScientificTerm, 'noise' whose type is OtherScientificTerm, 'reverberation' whose type is OtherScientificTerm, 'depression prediction' whose type is Task, 'mel-frequency cepstral coefficients (MFCCs)' whose type is Method, 'features' whose type is Generic, 'noise robustness' whose type is Metric, 'damped oscillator cepstral coefficients (DOCCs)' whose type is Method, 'AudioVisual Emotion Recognition Challenge (AVEC)' whose type is Material, 'additive noise' whose type is OtherScientificTerm, 'reverberation' whose type is OtherScientificTerm, 'evaluation metrics' whose type is Metric, 'MFCC features' whose type is Method, 'noise' whose type is OtherScientificTerm, 'reverberation' whose type is OtherScientificTerm, 'DOCC features' whose type is Method, 'higher-order cepstral coefficients' whose type is OtherScientificTerm, 'artificial neural networks' whose type is Method, 'support vector regression' whose type is Method, 'spontaneous speech' whose type is Material, 'read speech' whose type is Material, 'cross-corpus (and cross-language) experiment' whose type is OtherScientificTerm, 'noise and reverberation robustness' whose type is Metric, 'DOCCs' whose type is Method, 'MFCCs' whose type is Method, 'real-world robust depression detection' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "earned confidence measures gain increasing importance for outlier removal and quality improvement in stereo vision. However, acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction, active sensing devices and/or synthetic scenes. To overcome this problem, we propose a new, flexible , and scalable way for generating training data that only requires a set of stereo images as input. The key idea of our approach is to use different view points for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm. This enables us to generate a huge amount of training data in a fully automated manner. Among other experiments, we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the KITTI2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data.", "output": "The abstract text contains these key words below: 'Learned confidence measures' whose type is Method, 'outlier removal' whose type is Task, 'quality improvement' whose type is Task, 'stereo vision' whose type is Task, 'task' whose type is Generic, 'manual interaction' whose type is OtherScientificTerm, 'active sensing devices' whose type is OtherScientificTerm, 'synthetic scenes' whose type is OtherScientificTerm, 'problem' whose type is Generic, 'stereo images' whose type is Material, 'approach' whose type is Generic, 'view points' whose type is OtherScientificTerm, 'multiple depth maps' whose type is OtherScientificTerm, 'stereo algorithm' whose type is Method, 'approach' whose type is Generic, 'learned confidence measures' whose type is Method, 'KITTI2012 dataset' whose type is Material, 'them' whose type is Generic, 'automatically generated training data' whose type is Material, 'laser ground truth data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we present a novel, customizable :  IE paradigm  that takes advantage of  predicate-argument structures  . We also introduce a new way of automatically identifying  predicate argument structures  , which is central to our  IE paradigm  . It is based on: (1) an extended set of  features  ; and (2)  inductive decision tree learning  . The experimental results prove our claim that accurate  predicate-argument structures  enable high quality  IE  results. ", "output": "The abstract text contains these key words below: 'IE paradigm' whose type is Method, 'predicate-argument structures' whose type is OtherScientificTerm, 'automatically identifying  predicate argument structures' whose type is Task, 'IE paradigm' whose type is Method, 'It' whose type is Generic, 'features' whose type is OtherScientificTerm, 'inductive decision tree learning' whose type is Method, 'predicate-argument structures' whose type is OtherScientificTerm, 'IE' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper we summarize the experiences gained from a field trial of a speaker verification system. In the test implementation access to two rooms at the University of Frankfurt had been controlled by a speaker verification system. The paper is organized as follows: Firstly, we will describe the system concepts and implementation issues. Secondly, results of the user evaluation are reported. During the field trial all speech data was recorded. The data base created in this way has been used extensively for simulation experiments. In chapter 4 we will describe recent experiments focusing on the use of Hidden Markov Models.", "output": "The abstract text contains these key words below: 'speaker verification system' whose type is Method, 'speaker verification system' whose type is Method, 'user evaluation' whose type is Method, 'speech data' whose type is Material, 'Hidden Markov Models' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Terminology structuring  has been the subject of much work in the context of  terms  extracted from  corpora : given a set of  terms , obtained from an existing resource or extracted from a  corpus , identifying  hierarchical (or other types of) relations  between these  terms . The present paper focusses on  terminology structuring  by  lexical methods , which match  terms  on the basis on their  content words , taking  morphological variants  into account. Experiments are done on a 'flat' list of  terms  obtained from an originally  hierarchically-structured terminology : the French version of the  US National Library of Medicine MeSH thesaurus . We compare the  lexically-induced relations  with the original  MeSH relations : after a quantitative evaluation of their congruence through  recall and precision metrics , we perform a qualitative, human analysis ofthe 'new'  relations  not present in the  MeSH . This analysis shows, on the one hand, the limits of the  lexical structuring method . On the other hand, it also reveals some specific structuring choices and  naming conventions  made by the  MeSH  designers, and emphasizes ontological commitments that cannot be left to  automatic structuring .\n", "output": "The abstract text contains these key words below: 'Terminology structuring' whose type is Task, 'corpus' whose type is Material, 'hierarchical (or other types of) relations' whose type is OtherScientificTerm, 'terminology structuring' whose type is Task, 'lexical methods' whose type is Method, 'morphological variants' whose type is OtherScientificTerm, 'hierarchically-structured terminology' whose type is Material, 'US National Library of Medicine MeSH thesaurus' whose type is Material, 'lexically-induced relations' whose type is OtherScientificTerm, 'MeSH relations' whose type is OtherScientificTerm, 'recall and precision metrics' whose type is Metric, 'MeSH' whose type is OtherScientificTerm, 'lexical structuring method' whose type is Method, 'MeSH' whose type is OtherScientificTerm, 'automatic structuring' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present an algorithm for calibrated camera relative pose estimation from lines. Given three lines with two of the lines parallel and orthogonal to the third we can compute the relative rotation between two images. We can also compute the relative translation from two intersection points. We also present a framework in which such lines can be detected. We evaluate the performance of the algorithm using synthetic and real data. The intended use of the algorithm is with robust hypothesize-and-test frameworks such as RANSAC. Our approach is suitable for urban and indoor environments where most lines are either parallel or orthogonal to each other.", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'calibrated camera relative pose estimation' whose type is Task, 'relative rotation' whose type is OtherScientificTerm, 'relative translation' whose type is OtherScientificTerm, 'framework' whose type is Generic, 'algorithm' whose type is Generic, 'synthetic and real data' whose type is Material, 'algorithm' whose type is Generic, 'hypothesize-and-test frameworks' whose type is Method, 'RANSAC' whose type is Method, 'approach' whose type is Generic, 'urban and indoor environments' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this paper we present a novel autonomous pipeline to build a personalized parametric model (pose-driven avatar) using a single depth sensor. Our method first captures a few high-quality scans of the user rotating herself at multiple poses from different views. We fit each incomplete scan using template fitting techniques with a generic human template, and register all scans to every pose using global consistency constraints. After registration, these watertight models with different poses are used to train a parametric model in a fashion similar to the SCAPE method. Once the parametric model is built, it can be used as an anim-itable avatar or more interestingly synthesizing dynamic 3D models from single-view depth videos. Experimental results demonstrate the effectiveness of our system to produce dynamic models.", "output": "The abstract text contains these key words below: 'autonomous pipeline' whose type is Generic, 'personalized parametric model (pose-driven avatar)' whose type is Method, 'single depth sensor' whose type is OtherScientificTerm, 'method' whose type is Generic, 'template fitting techniques' whose type is Method, 'human template' whose type is OtherScientificTerm, 'global consistency constraints' whose type is OtherScientificTerm, 'watertight models' whose type is Method, 'parametric model' whose type is Method, 'SCAPE method' whose type is Method, 'parametric model' whose type is Method, 'it' whose type is Generic, 'anim-itable avatar' whose type is OtherScientificTerm, 'dynamic 3D models' whose type is Method, 'single-view depth videos' whose type is Material, 'system' whose type is Generic, 'dynamic models' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we present our recent work on harvesting  English-Chinese bitexts  of the laws of Hong Kong from the  Web  and aligning them to the  subparagraph  level via utilizing the  numbering system  in the  legal text hierarchy  . Basic methodology and practical techniques are reported in detail. The resultant  bilingual corpus  , 10.4M  English words  and 18.3M  Chinese characters  , is an authoritative and comprehensive  text collection  covering the specific and special domain of HK laws. It is particularly valuable to  empirical MT research  . This piece of work has also laid a foundation for exploring and harvesting  English-Chinese bitexts  in a larger volume from the  Web  . ", "output": "The abstract text contains these key words below: 'English-Chinese bitexts' whose type is Material, 'them' whose type is Generic, 'numbering system' whose type is Method, 'legal text hierarchy' whose type is OtherScientificTerm, 'bilingual corpus' whose type is Material, 'It' whose type is Generic, 'empirical MT research' whose type is Task, 'English-Chinese bitexts' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we deal with a recently developed  large Czech MWE database  containing at the moment 160 000  MWEs  (treated as  lexical units ). It was compiled from various resources such as  encyclopedias  and  dictionaries , public  databases  of  proper names  and  toponyms ,  collocations  obtained from  Czech WordNet , lists of  botanical and zoological terms  and others. We describe the structure of the  database  and give basic types of  MWEs  according to domains they belong to. We compare the built  MWEs database  with the  corpus data  from  Czech National Corpus  (approx. 100 mil. tokens) and present results of this comparison in the paper. These  MWEs  have not been obtained from the  corpus  since their frequencies in it are rather low. To obtain a more complete list of  MWEs  we propose and use a technique exploiting the  Word Sketch Engine , which allows us to work with  statistical parameters  such as frequency of  MWEs  and their components as well as with the  salience  for the whole  MWEs . We also discuss exploitation of the  database  for working out a more adequate  tagging  and  lemmatization . The final goal is to be able to recognize  MWEs  in  corpus text  and lemmatize them as complete  lexical units , i. e. to make  tagging  and  lemmatization  more adequate.\n", "output": "The abstract text contains these key words below: 'large Czech MWE database' whose type is Material, 'MWEs' whose type is OtherScientificTerm, 'lexical units' whose type is OtherScientificTerm, 'It' whose type is Generic, 'encyclopedias' whose type is Material, 'dictionaries' whose type is Material, 'public  databases  of  proper names  and  toponyms' whose type is Material, 'collocations' whose type is Generic, 'Czech WordNet' whose type is Material, 'lists of  botanical and zoological terms' whose type is Material, 'database' whose type is Generic, 'MWEs' whose type is OtherScientificTerm, 'MWEs database' whose type is Material, 'Czech National Corpus' whose type is Material, 'MWEs' whose type is OtherScientificTerm, 'corpus' whose type is Generic, 'MWEs' whose type is OtherScientificTerm, 'technique' whose type is Generic, 'Word Sketch Engine' whose type is Task, 'statistical parameters' whose type is OtherScientificTerm, 'MWEs' whose type is OtherScientificTerm, 'MWEs' whose type is OtherScientificTerm, 'database' whose type is Generic, 'tagging' whose type is Task, 'lemmatization' whose type is Task, 'MWEs' whose type is OtherScientificTerm, 'them' whose type is Generic, 'lexical units' whose type is OtherScientificTerm, 'tagging' whose type is Task, 'lemmatization' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n real-world action recognition problems, low-level features cannot adequately characterize the rich spatial-temporal structures in action videos. In this work, we encode actions based on attributes that describes actions as high-level concepts e.g., jump forward or motion in the air. We base our analysis on two types of action attributes. One type of action attributes is generated by humans. The second type is data-driven attributes, which are learned from data using dictionary learning methods. Attribute-based representation may exhibit high variance due to noisy and redundant attributes. We propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set. Three attribute selection criteria are proposed and formulated as a submodular optimization problem. A greedy optimization algorithm is presented and guaranteed to be at least (1-1/e)-approximation to the optimum. Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches.", "output": "The abstract text contains these key words below: 'real-world action recognition problems' whose type is Task, 'low-level features' whose type is OtherScientificTerm, 'rich spatial-temporal structures' whose type is OtherScientificTerm, 'action videos' whose type is Material, 'high-level concepts' whose type is OtherScientificTerm, 'action attributes' whose type is OtherScientificTerm, 'action attributes' whose type is OtherScientificTerm, 'data-driven attributes' whose type is OtherScientificTerm, 'dictionary learning methods' whose type is Method, 'Attribute-based representation' whose type is Method, 'noisy and redundant attributes' whose type is OtherScientificTerm, 'discriminative and compact attribute-based representation' whose type is Method, 'discriminative attributes' whose type is OtherScientificTerm, 'attribute selection criteria' whose type is Metric, 'submodular optimization problem' whose type is Task, 'greedy optimization algorithm' whose type is Method, 'Olympic Sports and UCF101 datasets' whose type is Material, 'attribute-based representation' whose type is Method, 'action recognition algorithms' whose type is Method, 'recognition approaches' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "At MIT Lincoln Laboratory, we have been developing a  Korean-to-English machine translation system   CCLINC (Common Coalition Language System at Lincoln Laboratory)  . The  CCLINC Korean-to-English translation system  consists of two  core modules  ,  language understanding and generation modules  mediated by a  language neutral meaning representation  called a  semantic frame  . The key features of the system include: (i) Robust efficient  parsing  of  Korean  (a  verb final language  with  overt case markers  , relatively  free word order  , and frequent omissions of  arguments  ). (ii) High quality  translation  via  word sense disambiguation  and accurate  word order generation  of the  target language  . (iii)  Rapid system development  and porting to new  domains  via  knowledge-based automated acquisition of grammars  . Having been trained on  Korean newspaper articles  on missiles and chemical biological warfare, the system produces the  translation output  sufficient for content understanding of the  original document  . ", "output": "The abstract text contains these key words below: 'Korean-to-English machine translation system' whose type is Method, 'CCLINC (Common Coalition Language System at Lincoln Laboratory)' whose type is Method, 'CCLINC Korean-to-English translation system' whose type is Method, 'core modules' whose type is Generic, 'language understanding and generation modules' whose type is Method, 'language neutral meaning representation' whose type is Method, 'semantic frame' whose type is OtherScientificTerm, 'system' whose type is Generic, 'parsing  of  Korean' whose type is Task, 'Korean' whose type is Material, 'verb final language' whose type is Material, 'overt case markers' whose type is OtherScientificTerm, 'translation' whose type is Task, 'word sense disambiguation' whose type is Method, 'word order generation' whose type is Task, 'knowledge-based automated acquisition of grammars' whose type is Method, 'Korean newspaper articles' whose type is Material, 'missiles and chemical biological warfare' whose type is Material, 'system' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper describes the framework of a  Korean phonological knowledge base system  using the  unification-based grammar formalism  :  Korean Phonology Structure Grammar (KPSG)  . The approach of  KPSG  provides an explicit development model for constructing a computational  phonological system  :  speech recognition  and  synthesis system  . We show that the proposed approach is more describable than other approaches such as those employing a traditional  generative phonological approach  . ", "output": "The abstract text contains these key words below: 'Korean phonological knowledge base system' whose type is Task, 'unification-based grammar formalism' whose type is Method, 'Korean Phonology Structure Grammar (KPSG)' whose type is Method, 'approach' whose type is Generic, 'KPSG' whose type is Method, 'phonological system' whose type is Task, 'speech recognition  and  synthesis system' whose type is Task, 'approach' whose type is Generic, 'approaches' whose type is Generic, 'those' whose type is Generic, 'generative phonological approach' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n The  LOGON MT demonstrator  assembles independently valuable  general-purpose NLP components  into a  machine translation pipeline  that capitalizes on  output quality  . The demonstrator embodies an interesting combination of  hand-built, symbolic resources  and  stochastic processes  . ", "output": "The abstract text contains these key words below: 'LOGON MT demonstrator' whose type is Method, 'general-purpose NLP components' whose type is Method, 'machine translation pipeline' whose type is Method, 'demonstrator' whose type is Task, 'hand-built, symbolic resources' whose type is Material, 'stochastic processes' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper describes a domain independent strategy for the  multimedia articulation of answers  elicited by a  natural language interface  to  database query applications  .  Multimedia answers  include  videodisc images  and heuristically-produced complete  sentences  in  text  or  text-to-speech form  .  Deictic reference  and  feedback  about the  discourse  are enabled. The  interface  thus presents the application as cooperative and conversational. ", "output": "The abstract text contains these key words below: 'domain independent strategy' whose type is Method, 'multimedia articulation of answers' whose type is Task, 'natural language interface' whose type is OtherScientificTerm, 'database query applications' whose type is Task, 'Multimedia answers' whose type is Material, 'videodisc images' whose type is Material, 'text-to-speech form' whose type is OtherScientificTerm, 'Deictic reference' whose type is OtherScientificTerm, 'feedback' whose type is OtherScientificTerm, 'discourse' whose type is OtherScientificTerm, 'interface' whose type is Generic, 'application' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n spite of over two decades of intense research, illumination and pose invariance remain prohibitively challenging aspects of face recognition for most practical applications. The objective of this work is to recognize faces using video sequences both for training and recognition input, in a realistic, unconstrained setup in which lighting, pose and user motion pattern have a wide variability and face images are of low resolution. In particular there are three areas of novelty: (i) we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation, learnt offline, to generalize in the presence of extreme illumination changes; (ii) we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses; and (iii) we introduce an accurate video sequence \" reillumination \" algorithm to achieve robustness to face motion patterns in video. We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme illumination, pose and head motion variation. On this challenging data set our system consistently demonstrated a nearly perfect recognition rate (over 99.7% on all three databases), significantly out-performing state-of-the-art commercial software and methods from the literature.", "output": "The abstract text contains these key words below: 'illumination' whose type is Task, 'pose invariance' whose type is Task, 'face recognition' whose type is Task, 'video sequences' whose type is Material, 'lighting' whose type is OtherScientificTerm, 'pose' whose type is OtherScientificTerm, 'user motion pattern' whose type is OtherScientificTerm, 'face images' whose type is Material, 'resolution' whose type is Metric, 'photometric model' whose type is Method, 'image formation' whose type is Task, 'statistical model' whose type is Method, 'generic face appearance variation' whose type is Task, 'extreme illumination changes' whose type is OtherScientificTerm, 'smoothness' whose type is OtherScientificTerm, 'geodesically local appearance manifold structure' whose type is OtherScientificTerm, 'robust same-identity likelihood' whose type is Method, 'unseen head poses' whose type is OtherScientificTerm, 'video sequence \" reillumination \" algorithm' whose type is Method, 'robustness' whose type is Metric, 'face motion patterns' whose type is OtherScientificTerm, 'video' whose type is Material, 'fully automatic recognition system' whose type is Method, 'method' whose type is Generic, 'video sequences' whose type is Material, 'illumination' whose type is OtherScientificTerm, 'pose' whose type is OtherScientificTerm, 'head motion variation' whose type is OtherScientificTerm, 'data set' whose type is Generic, 'system' whose type is Generic, 'recognition rate' whose type is Metric, 'databases' whose type is Material, 'commercial software' whose type is Method, 'methods' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "A major axis of research at LIMSI is directed at  multilingual, speaker-independent, large vocabulary speech dictation . In this paper the  LIMSI recognizer  which was evaluated in the  ARPA NOV93 CSR test  is described, and experimental results on the  WSJ and BREF corpora  under closely matched conditions are reported. For both  corpora   word recognition experiments  were carried out with  vocabularies  containing up to 20k  words . The recognizer makes use of  continuous density HMM  with  Gaussian mixture  for  acoustic modeling  and  n-gram statistics  estimated on the  newspaper texts  for  language modeling . The recognizer uses a  time-synchronous graph-search strategy  which is shown to still be viable with a 20k-word vocabulary when used with  bigram back-off language models . A second  forward pass , which makes use of a  word graph  generated with the  bigram , incorporates a  trigram language model .  Acoustic modeling  uses  cepstrum-based features ,  context-dependent phone models (intra and interword) ,  phone duration models , and  sex-dependent models .\n", "output": "The abstract text contains these key words below: 'multilingual, speaker-independent, large vocabulary speech dictation' whose type is Task, 'LIMSI recognizer' whose type is Method, 'ARPA NOV93 CSR test' whose type is Material, 'WSJ and BREF corpora' whose type is Material, 'corpora' whose type is Generic, 'word recognition' whose type is Task, 'recognizer' whose type is Generic, 'continuous density HMM' whose type is Method, 'Gaussian mixture' whose type is Method, 'acoustic modeling' whose type is Task, 'n-gram statistics' whose type is Method, 'newspaper texts' whose type is Material, 'language modeling' whose type is Task, 'recognizer' whose type is Generic, 'time-synchronous graph-search strategy' whose type is Method, 'bigram back-off language models' whose type is Method, 'word graph' whose type is OtherScientificTerm, 'bigram' whose type is Method, 'trigram language model' whose type is Method, 'Acoustic modeling' whose type is Task, 'cepstrum-based features' whose type is OtherScientificTerm, 'context-dependent phone models (intra and interword)' whose type is Method, 'phone duration models' whose type is Method, 'sex-dependent models' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we explore correlation of  dependency relation paths  to rank candidate answers in  answer extraction . Using the  correlation measure , we compare  dependency relations  of a candidate answer and mapped  question phrases  in  sentence  with the corresponding  relations  in question. Different from previous studies, we propose an  approximate phrase mapping algorithm  and incorporate the  mapping score  into the  correlation measure . The correlations are further incorporated into a  Maximum Entropy-based ranking model  which estimates  path weights  from training. Experimental results show that our method significantly outperforms state-of-the-art  syntactic relation-based methods  by up to 20% in  MRR .\n", "output": "The abstract text contains these key words below: 'correlation of  dependency relation paths' whose type is OtherScientificTerm, 'answer extraction' whose type is Task, 'correlation measure' whose type is Metric, 'dependency relations' whose type is OtherScientificTerm, 'approximate phrase mapping algorithm' whose type is Method, 'mapping score' whose type is OtherScientificTerm, 'correlation measure' whose type is Metric, 'correlations' whose type is Generic, 'Maximum Entropy-based ranking model' whose type is Method, 'method' whose type is Generic, 'syntactic relation-based methods' whose type is Method, 'MRR' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "onventional HMMs have weak duration constraints. In noisy conditions, the mismatch between corrupted speech signals and models trained on clean speech may cause the decoder to produce word matches with unrealistic durations. This paper presents a simple way to incorporate word duration constraints by unrolling HMMs to form a lattice where word duration probabilities can be applied directly to state transitions. The expanded HMMs are compatible with conventional Viterbi decoding. Experiments on connected-digit recognition show that when using explicit duration constraints the decoder generates word matches with more reasonable durations, and word error rates are significantly reduced across a broad range of noise conditions .", "output": "The abstract text contains these key words below: 'HMMs' whose type is Method, 'weak duration constraints' whose type is OtherScientificTerm, 'corrupted speech signals' whose type is Material, 'models' whose type is Generic, 'clean speech' whose type is Material, 'decoder' whose type is Method, 'word matches' whose type is OtherScientificTerm, 'unrealistic durations' whose type is OtherScientificTerm, 'word duration constraints' whose type is OtherScientificTerm, 'unrolling HMMs' whose type is Method, 'lattice' whose type is OtherScientificTerm, 'word duration probabilities' whose type is OtherScientificTerm, 'state transitions' whose type is OtherScientificTerm, 'HMMs' whose type is Method, 'Viterbi decoding' whose type is Method, 'connected-digit recognition' whose type is Task, 'duration constraints' whose type is OtherScientificTerm, 'decoder' whose type is Method, 'word matches' whose type is OtherScientificTerm, 'word error rates' whose type is Metric, 'noise conditions' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n the study of expressive speech communication, it is commonly accepted that the emotion perceived by the listener is a good approximation of the intended emotion conveyed by the speaker. This paper analyzes the validity of this assumption by comparing the mismatches between the assessments made by na\u00a8\u0131ve listeners and by the speakers that generated the data. The analysis is based on the hypothesis that people are better decoders of their own emotions. Therefore, self-assessments will be closer to the intended emotions. Using the IEMOCAP database, discrete (categorical) and continuous (attribute) emotional assessments evaluated by the actors and na\u00a8\u0131ve listeners are compared. The results indicate that there is a mismatch between the expression and perception of emotion. The speakers in the database assigned their own emotions to more specific emotional categories, which led to more extreme values in the activation-valence space.", "output": "The abstract text contains these key words below: 'expressive speech communication' whose type is Task, 'IEMOCAP database' whose type is Material, 'discrete (categorical) and continuous (attribute) emotional assessments' whose type is Task, 'expression and perception of emotion' whose type is OtherScientificTerm, 'database' whose type is Generic, 'activation-valence space' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ideo provides not only rich visual cues such as motion and appearance, but also much less explored long-range temporal interactions among objects. We aim to capture such interactions and to construct a powerful intermediate-level video representation for subsequent recognition. Motivated by this goal, we seek to obtain spatio-temporal over-segmentation of a video into regions that respect object boundaries and, at the same time, associate object pix-els over many video frames. The contributions of this paper are twofold. First, we develop an efficient spatio-temporal video segmentation algorithm, which naturally incorporates long-range motion cues from the past and future frames in the form of clusters of point tracks with coherent motion. Second, we devise a new track clustering cost function that includes occlusion reasoning, in the form of depth ordering constraints, as well as motion similarity along the tracks. We evaluate the proposed approach on a challenging set of video sequences of office scenes from feature length movies.", "output": "The abstract text contains these key words below: 'Video' whose type is Material, 'visual cues' whose type is OtherScientificTerm, 'motion' whose type is OtherScientificTerm, 'appearance' whose type is OtherScientificTerm, 'long-range temporal interactions' whose type is OtherScientificTerm, 'interactions' whose type is Generic, 'intermediate-level video representation' whose type is Method, 'recognition' whose type is Task, 'spatio-temporal over-segmentation' whose type is OtherScientificTerm, 'object boundaries' whose type is OtherScientificTerm, 'spatio-temporal video segmentation algorithm' whose type is Method, 'long-range motion cues' whose type is OtherScientificTerm, 'clusters of point tracks' whose type is OtherScientificTerm, 'track clustering cost function' whose type is Method, 'occlusion reasoning' whose type is OtherScientificTerm, 'depth ordering constraints' whose type is OtherScientificTerm, 'motion similarity' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'video sequences of office scenes' whose type is Material, 'movies' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "Two themes have evolved in  speech and text image processing  work at  Xerox PARC  that expand and redefine the role of  recognition technology  in  document-oriented applications . One is the development of systems that provide functionality similar to that of  text processors  but operate directly on  audio and scanned image data . A second, related theme is the use of  speech and text-image recognition  to retrieve arbitrary, user-specified information from  documents with signal content . This paper discusses three research initiatives at  PARC  that exemplify these themes: a  text-image editor [1], a  wordspotter  for  voice editing and indexing [12], and a  decoding framework  for  scanned-document content retrieval [4]. The discussion focuses on key concepts embodied in the research that enable novel  signal-based document processing functionality .\n", "output": "The abstract text contains these key words below: 'themes' whose type is Generic, 'speech and text image processing' whose type is Task, 'recognition technology' whose type is Method, 'document-oriented applications' whose type is Task, 'One' whose type is Generic, 'systems' whose type is Generic, 'text processors' whose type is Method, 'audio and scanned image data' whose type is Material, 'theme' whose type is Generic, 'speech and text-image recognition' whose type is Method, 'documents with signal content' whose type is OtherScientificTerm, 'research' whose type is Generic, 'themes' whose type is Generic, 'text-image editor' whose type is Method, 'wordspotter' whose type is Method, 'voice editing and indexing' whose type is Task, 'decoding framework' whose type is Method, 'scanned-document content retrieval' whose type is Task, 'signal-based document processing functionality' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n An  entity-oriented approach to restricted-domain parsing  is proposed. In this approach, the definitions of the  structure  and  surface representation  of  domain entities  are grouped together. Like  semantic grammar  , this allows easy exploitation of  limited domain semantics  . In addition, it facilitates  fragmentary recognition  and the use of  multiple parsing strategies  , and so is particularly useful for robust  recognition of extra-grammatical input  . Several advantages from the point of view of  language definition  are also noted. Representative samples from an  entity-oriented language definition  are presented, along with a  control structure  for an  entity-oriented parser  , some  parsing strategies  that use the  control structure  , and worked examples of  parses  . A  parser  incorporating the  control structure  and the  parsing strategies  is currently under  implementation  . ", "output": "The abstract text contains these key words below: 'entity-oriented approach' whose type is Method, 'restricted-domain parsing' whose type is Task, 'approach' whose type is Generic, 'structure  and  surface representation  of  domain entities' whose type is OtherScientificTerm, 'semantic grammar' whose type is Method, 'this' whose type is Generic, 'limited domain semantics' whose type is OtherScientificTerm, 'it' whose type is Generic, 'fragmentary recognition' whose type is Task, 'multiple parsing strategies' whose type is Method, 'recognition of extra-grammatical input' whose type is OtherScientificTerm, 'entity-oriented language definition' whose type is OtherScientificTerm, 'control structure' whose type is OtherScientificTerm, 'entity-oriented parser' whose type is Method, 'parsing strategies' whose type is Method, 'control structure' whose type is OtherScientificTerm, 'parser' whose type is Method, 'control structure' whose type is OtherScientificTerm, 'parsing strategies' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " Sentiment Classification  seeks to identify a piece of  text  according to its author's general feeling toward their  subject , be it positive or negative. Traditional  machine learning techniques  have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the  training and test data  with respect to  topic . This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with  training data  labeled with  emoticons , which has the potential of being independent of  domain ,  topic  and time.\n", "output": "The abstract text contains these key words below: 'Sentiment Classification' whose type is Task, 'machine learning techniques' whose type is Method, 'problem' whose type is Generic, 'training and test data' whose type is Material, 'training data' whose type is Material, 'emoticons' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e describe Yoopick, a combinatorial sports prediction market that implements a flexible betting language, and in turn facilitates fine-grained probabilistic estimation of outcomes.", "output": "The abstract text contains these key words below: 'Yoopick' whose type is Method, 'combinatorial sports prediction market' whose type is Method, 'flexible betting language' whose type is OtherScientificTerm, 'fine-grained probabilistic estimation of outcomes' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We describe a new method for the representation of  NLP structures  within  reranking approaches . We make use of a  conditional log-linear model , with  hidden variables  representing the  assignment  of  lexical items  to  word clusters  or  word senses . The model learns to automatically make these  assignments  based on a  discriminative training criterion .  Training  and  decoding  with the model requires summing over an exponential number of  hidden-variable assignments : the required summations can be computed efficiently and exactly using  dynamic programming . As a case study, we apply the model to  parse reranking . The model gives an  F-measure improvement  of ~1.25% beyond the  base parser , and an ~0.25% improvement beyond  Collins (2000) reranker . Although our experiments are focused on  parsing , the techniques described generalize naturally to  NLP structures  other than  parse trees .\n", "output": "The abstract text contains these key words below: 'method' whose type is Generic, 'NLP structures' whose type is OtherScientificTerm, 'reranking approaches' whose type is Method, 'conditional log-linear model' whose type is Method, 'hidden variables' whose type is OtherScientificTerm, 'word clusters' whose type is OtherScientificTerm, 'word senses' whose type is OtherScientificTerm, 'model' whose type is Generic, 'discriminative training criterion' whose type is Metric, 'model' whose type is Generic, 'hidden-variable assignments' whose type is OtherScientificTerm, 'summations' whose type is Generic, 'dynamic programming' whose type is Method, 'model' whose type is Generic, 'parse reranking' whose type is Task, 'model' whose type is Generic, 'F-measure' whose type is Metric, 'base parser' whose type is Method, 'Collins (2000) reranker' whose type is Method, 'parsing' whose type is Task, 'techniques' whose type is Generic, 'NLP structures' whose type is OtherScientificTerm, 'parse trees' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents a  phrase-based statistical machine translation method  , based on  non-contiguous phrases  , i.e.  phrases  with gaps. A method for producing such  phrases  from a  word-aligned corpora  is proposed. A  statistical translation model  is also presented that deals such  phrases  , as well as a  training method  based on the maximization of  translation accuracy  , as measured with the  NIST evaluation metric  .  Translations  are produced by means of a  beam-search decoder  . Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the  training data  . ", "output": "The abstract text contains these key words below: 'phrase-based statistical machine translation method' whose type is Method, 'non-contiguous phrases' whose type is Material, 'method' whose type is Generic, 'phrases' whose type is Generic, 'word-aligned corpora' whose type is Material, 'statistical translation model' whose type is Method, 'phrases' whose type is Generic, 'training method' whose type is Method, 'maximization of  translation accuracy' whose type is Metric, 'NIST evaluation metric' whose type is Metric, 'Translations' whose type is OtherScientificTerm, 'beam-search decoder' whose type is Method, 'method' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper proposes a framework in which Lagrangian Particle Dynamics is used for the segmentation of high density crowd flows and detection of flow instabilities. For this purpose, a flow field generated by a moving crowd is treated as an aperiodic dynamical system. A grid of particles is overlaid on the flow field, and is advected using a numerical integration scheme. The evolution of particles through the flow is tracked using a Flow Map, whose spatial gradients are subsequently used to setup a Cauchy Green Deformation tensor for quantifying the amount by which the neighboring particles have diverged over the length of the integration. The maximum eigenvalue of the tensor is used to construct a Finite Time Lyapunov Exponent (FTLE) field, which reveals the Lagrangian Coherent Structures (LCS) present in the underlying flow. The LCS divide flow into regions of qualitatively different dynamics and are used to locate boundaries of the flow segments in a normalized cuts framework. Any change in the number of flow segments over time is regarded as an instability, which is detected by establishing correspondences between flow segments over time. The experiments are conducted on a challenging set of videos taken from Google Video and a National Geographic documentary.", "output": "The abstract text contains these key words below: 'Lagrangian Particle Dynamics' whose type is Method, 'segmentation of high density crowd flows' whose type is Task, 'detection of flow instabilities' whose type is Task, 'flow field' whose type is OtherScientificTerm, 'moving crowd' whose type is OtherScientificTerm, 'aperiodic dynamical system' whose type is Method, 'grid of particles' whose type is OtherScientificTerm, 'flow field' whose type is OtherScientificTerm, 'numerical integration scheme' whose type is Method, 'evolution of particles' whose type is OtherScientificTerm, 'Flow Map' whose type is Method, 'spatial gradients' whose type is OtherScientificTerm, 'Cauchy Green Deformation tensor' whose type is Method, 'maximum eigenvalue' whose type is OtherScientificTerm, 'tensor' whose type is Generic, 'Finite Time Lyapunov Exponent (FTLE) field' whose type is OtherScientificTerm, 'Lagrangian Coherent Structures (LCS)' whose type is OtherScientificTerm, 'LCS' whose type is OtherScientificTerm, 'boundaries of the flow segments' whose type is OtherScientificTerm, 'normalized cuts framework' whose type is Method, 'Google Video' whose type is Material, 'National Geographic documentary' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "any description logics (DLs) combine knowledge representation on an abstract, logical level with an interface to \"concrete\" domains such as numbers and strings. We propose to extend such DLs with key constraints that allow the expression of statements like \"US citizens are uniquely identified by their social security number\". Based on this idea, we introduce a number of natural description logics and present (un)decidability results and tight NEx-PTlME complexity bounds.", "output": "The abstract text contains these key words below: 'description logics (DLs)' whose type is Method, 'knowledge representation' whose type is Method, 'DLs' whose type is Method, 'natural description logics' whose type is Method, 'tight NEx-PTlME complexity bounds' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n The  transfer phase  in  machine translation (MT) systems  has been considered to be more complicated than  analysis  and  generation  , since it is inherently a conglomeration of individual  lexical rules  . Currently some attempts are being made to use  case-based reasoning  in  machine translation  , that is, to make decisions on the basis of  translation examples  at appropriate pints in  MT  . This paper proposes a new type of  transfer system  , called a  Similarity-driven Transfer System (SimTran)  , for use in such  case-based MT (CBMT)  . ", "output": "The abstract text contains these key words below: 'transfer phase' whose type is Task, 'machine translation (MT) systems' whose type is Task, 'analysis' whose type is Task, 'generation' whose type is Task, 'it' whose type is Generic, 'lexical rules' whose type is OtherScientificTerm, 'case-based reasoning' whose type is Method, 'machine translation' whose type is Task, 'MT' whose type is Task, 'transfer system' whose type is Method, 'Similarity-driven Transfer System (SimTran)' whose type is Method, 'case-based MT (CBMT)' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he automated segmentation of images into semantically meaningful parts requires shape information since low-level feature analysis alone often fails to reach this goal. We introduce a novel method of shape constrained image segmentation which is based on mixtures of feature distributions for color and texture as well as probabilistic shape knowledge. The combined approach is formulated in the framework of Bayesian statistics to account for the robust-ness requirement in image understanding. Experimental evidence shows that semantically meaningful segments are inferred , even when image data alone gives rise to ambiguous segmentations.", "output": "The abstract text contains these key words below: 'automated segmentation' whose type is Task, 'images' whose type is Material, 'shape information' whose type is OtherScientificTerm, 'low-level feature analysis' whose type is Method, 'method' whose type is Method, 'shape constrained image segmentation' whose type is Task, 'mixtures of feature distributions' whose type is OtherScientificTerm, 'color' whose type is OtherScientificTerm, 'texture' whose type is OtherScientificTerm, 'probabilistic shape knowledge' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'Bayesian statistics' whose type is Method, 'robust-ness requirement in image understanding' whose type is Task, 'image data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "n this work, we present a technique for robust estimation , which by explicitly incorporating the inherent uncertainty of the estimation procedure, results in a more efficient robust estimation algorithm. In addition, we build on recent work in randomized model verification, and use this to characterize the 'non-randomness' of a solution. The combination of these two strategies results in a robust estimation procedure that provides a significant speed-up over existing RANSAC techniques, while requiring no prior information to guide the sampling process. In particular, our algorithm requires, on average, 3-10 times fewer samples than standard RANSAC, which is in close agreement with theoretical predictions. The efficiency of the algorithm is demonstrated on a selection of geometric estimation problems .", "output": "The abstract text contains these key words below: 'technique' whose type is Generic, 'robust estimation' whose type is Task, 'inherent uncertainty of the estimation procedure' whose type is OtherScientificTerm, 'efficient robust estimation algorithm' whose type is Method, 'randomized model verification' whose type is Task, 'this' whose type is Generic, 'strategies' whose type is Generic, 'robust estimation procedure' whose type is Method, 'RANSAC techniques' whose type is Method, 'prior information' whose type is OtherScientificTerm, 'sampling process' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'RANSAC' whose type is Method, 'theoretical predictions' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'geometric estimation problems' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents a  maximum entropy word alignment algorithm  for  Arabic-English  based on  supervised training data  . We demonstrate that it is feasible to create  training material  for problems in  machine translation  and that a mixture of  supervised and unsupervised methods  yields superior  performance  . The  probabilistic model  used in the  alignment  directly models the  link decisions  . Significant improvement over traditional  word alignment techniques  is shown as well as improvement on several  machine translation tests  . Performance of the algorithm is contrasted with  human annotation performance  . ", "output": "The abstract text contains these key words below: 'maximum entropy word alignment algorithm' whose type is Method, 'Arabic-English' whose type is Material, 'supervised training data' whose type is Material, 'training material' whose type is Material, 'machine translation' whose type is Task, 'supervised and unsupervised methods' whose type is Method, 'probabilistic model' whose type is Method, 'alignment' whose type is Task, 'link decisions' whose type is Task, 'word alignment techniques' whose type is Method, 'machine translation tests' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'human annotation' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present an operable definition of  focus  which is argued to be of a cognito-pragmatic nature and explore how it is determined in  discourse  in a formalized manner. For this purpose, a file card model of  discourse model  and  knowledge store  is introduced enabling the  decomposition  and  formal representation  of its  determination process  as a programmable algorithm ( FDA ). Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of  focus  via  FDA  as a  discourse-level construct  into  speech synthesis systems , in particular,  concept-to-speech systems , is also briefly discussed.\n", "output": "The abstract text contains these key words below: 'operable definition' whose type is Generic, 'cognito-pragmatic nature' whose type is OtherScientificTerm, 'it' whose type is Generic, 'discourse' whose type is OtherScientificTerm, 'a file card model of  discourse model  and  knowledge store' whose type is Method, 'discourse model' whose type is Method, 'knowledge store' whose type is OtherScientificTerm, 'determination process' whose type is OtherScientificTerm, 'programmable algorithm' whose type is Method, 'FDA' whose type is Method, 'social and cognitive psychology' whose type is OtherScientificTerm, 'FDA' whose type is OtherScientificTerm, 'discourse-level construct' whose type is OtherScientificTerm, 'speech synthesis systems' whose type is Method, 'concept-to-speech systems' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n  Determiners  play an important role in conveying the  meaning  of an  utterance  , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the  global meaning  of a  sentence  , even if not in a precise way. Another problem with  determiners  is their inherent  ambiguity  . In this paper we propose a  logical formalism  , which, among other things, is suitable for representing  determiners  without forcing a particular  interpretation  when their  meaning  is still not clear. ", "output": "The abstract text contains these key words below: 'Determiners' whose type is Method, 'determiners' whose type is Method, 'ambiguity' whose type is OtherScientificTerm, 'logical formalism' whose type is Method, 'determiners' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "t is well-known that diversity among base classifiers is crucial for constructing a strong ensemble. Most existing ensemble methods obtain diverse individual learners through resampling the instances or features. In this paper, we propose an alternative way for ensemble construction by resampling pairwise constraints that specify whether a pair of instances belongs to the same class or not. Using pairwise constraints for ensemble construction is challenging because it remains unknown how to influence the base classifiers with the sampled pairwise constraints. We solve this problem with a two-step process. First, we transform the original instances into a new data representation using projections learnt from pairwise constraints. Then, we build the base clas-sifiers with the new data representation. We propose two methods for resampling pairwise constraints following the standard Bagging and Boosting algorithms, respectively. Extensive experiments validate the effectiveness of our method.", "output": "The abstract text contains these key words below: 'base classifiers' whose type is Method, 'ensemble' whose type is Task, 'ensemble methods' whose type is Method, 'ensemble construction' whose type is Task, 'resampling pairwise constraints' whose type is Task, 'pairwise constraints' whose type is OtherScientificTerm, 'ensemble construction' whose type is Task, 'base classifiers' whose type is Method, 'sampled pairwise constraints' whose type is OtherScientificTerm, 'data representation' whose type is Method, 'projections' whose type is OtherScientificTerm, 'pairwise constraints' whose type is OtherScientificTerm, 'base clas-sifiers' whose type is Method, 'data representation' whose type is Method, 'resampling pairwise constraints' whose type is Task, 'Bagging and Boosting algorithms' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a new HMM tagger that exploits  context  on both sides of a  word  to be tagged, and evaluate it in both the  unsupervised and supervised case . Along the way, we present the first comprehensive comparison of  unsupervised methods for part-of-speech tagging , noting that published results to date have not been comparable across  corpora  or  lexicons . Observing that the  quality  of the  lexicon  greatly impacts the  accuracy  that can be achieved by the  algorithms , we present a method of  HMM training  that improves  accuracy  when  training  of  lexical probabilities  is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a  supervised, non-training intensive framework .\n", "output": "The abstract text contains these key words below: 'HMM tagger' whose type is Method, 'it' whose type is Generic, 'unsupervised and supervised case' whose type is Task, 'unsupervised methods' whose type is Method, 'part-of-speech tagging' whose type is Task, 'accuracy' whose type is Metric, 'algorithms' whose type is Generic, 'HMM training' whose type is Method, 'lexical probabilities' whose type is OtherScientificTerm, 'tagger' whose type is Generic, 'supervised, non-training intensive framework' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper describes a recently collected  spoken language corpus  for the  ATIS (Air Travel Information System) domain  . This data collection effort has been co-ordinated by  MADCOW (Multi-site ATIS Data COllection Working group)  . We summarize the motivation for this effort, the goals, the implementation of a  multi-site data collection paradigm  , and the accomplishments of  MADCOW  in monitoring the  collection  and distribution of 12,000  utterances  of  spontaneous speech  from five sites for use in a  multi-site common evaluation of speech, natural language and spoken language  .", "output": "The abstract text contains these key words below: 'spoken language corpus' whose type is Material, 'ATIS (Air Travel Information System) domain' whose type is Task, 'data collection' whose type is Generic, 'multi-site data collection paradigm' whose type is Method, 'collection' whose type is Generic, 'spontaneous speech' whose type is Material, 'multi-site common evaluation of speech, natural language and spoken language' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents an approach to the  unsupervised learning  of  parts of speech  which uses both  morphological and syntactic information . While the  model  is more complex than those which have been employed for  unsupervised learning  of  POS tags in English , which use only  syntactic information , the variety of  languages  in the world requires that we consider  morphology  as well. In many  languages ,  morphology  provides better clues to a word's category than  word order . We present the  computational model  for  POS learning , and present results for applying it to  Bulgarian , a  Slavic language  with relatively  free word order  and  rich morphology .\n", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'unsupervised learning  of  parts of speech' whose type is Task, 'morphological and syntactic information' whose type is OtherScientificTerm, 'model' whose type is Generic, 'those' whose type is Generic, 'unsupervised learning  of  POS tags in English' whose type is Task, 'syntactic information' whose type is OtherScientificTerm, 'morphology' whose type is OtherScientificTerm, 'morphology' whose type is OtherScientificTerm, 'word order' whose type is OtherScientificTerm, 'computational model' whose type is Generic, 'POS learning' whose type is Task, 'it' whose type is Generic, 'Bulgarian' whose type is Material, 'Slavic language' whose type is Material, 'free word order' whose type is OtherScientificTerm, 'rich morphology' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a  syntax-based constraint  for  word alignment  , known as the  cohesion constraint  . It requires disjoint  English phrases  to be mapped to non-overlapping intervals in the  French sentence  . We evaluate the utility of this  constraint  in two different algorithms. The results show that it can provide a significant improvement in  alignment quality  . ", "output": "The abstract text contains these key words below: 'syntax-based constraint' whose type is OtherScientificTerm, 'word alignment' whose type is Task, 'cohesion constraint' whose type is OtherScientificTerm, 'It' whose type is Generic, 'English phrases' whose type is Material, 'French sentence' whose type is Material, 'constraint' whose type is Generic, 'algorithms' whose type is Generic, 'it' whose type is Generic, 'alignment quality' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "igh frequency oscillations (HFOs) are a promising biomarker of epileptic brain tissue and activity. HFOs additionally serve as a prototypical example of challenges in the analysis of discrete events in high-temporal resolution, intracranial EEG data. Two primary challenges are 1) dimensionality reduction, and 2) assessing feasibility of classification. Dimensionality reduction assumes that the data lie on a manifold with dimension less than that of the features space. However, previous HFO analysis have assumed a linear manifold, global across time, space (i.e. recording electrode/channel), and individual patients. Instead, we assess both a) whether linear methods are appropriate and b) the consistency of the manifold across time, space, and patients. We also estimate bounds on the Bayes classification error to quantify the distinction between two classes of HFOs (those occurring during seizures and those occurring due to other processes). This analysis provides the foundation for future clinical use of HFO features and guides the analysis for other discrete events, such as individual action potentials or multi-unit activity.", "output": "The abstract text contains these key words below: 'High frequency oscillations (HFOs)' whose type is OtherScientificTerm, 'epileptic brain tissue and activity' whose type is OtherScientificTerm, 'HFOs' whose type is OtherScientificTerm, 'analysis of discrete events' whose type is Task, 'high-temporal resolution, intracranial EEG data' whose type is Material, 'dimensionality reduction' whose type is Task, 'assessing feasibility of classification' whose type is Task, 'classification' whose type is Task, 'Dimensionality reduction' whose type is Task, 'manifold' whose type is Material, 'features space' whose type is OtherScientificTerm, 'HFO analysis' whose type is Task, 'linear manifold' whose type is OtherScientificTerm, 'linear methods' whose type is Method, 'manifold' whose type is OtherScientificTerm, 'bounds' whose type is OtherScientificTerm, 'Bayes classification error' whose type is OtherScientificTerm, 'HFOs' whose type is OtherScientificTerm, 'those' whose type is Generic, 'those' whose type is Generic, 'clinical use of HFO features' whose type is Task, 'HFO features' whose type is OtherScientificTerm, 'discrete events' whose type is OtherScientificTerm, 'action potentials' whose type is OtherScientificTerm, 'multi-unit activity' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper proposes a practical approach employing  n-gram models  and  error-correction rules  for  Thai key prediction  and  Thai-English language identification  . The paper also proposes  rule-reduction algorithm  applying  mutual information  to reduce the  error-correction rules  . Our algorithm reported more than 99%  accuracy  in both  language identification  and  key prediction  . ", "output": "The abstract text contains these key words below: 'approach' whose type is Generic, 'n-gram models' whose type is Method, 'error-correction rules' whose type is Method, 'Thai key prediction' whose type is Task, 'Thai-English language identification' whose type is Task, 'rule-reduction algorithm' whose type is Method, 'mutual information' whose type is OtherScientificTerm, 'error-correction rules' whose type is OtherScientificTerm, 'algorithm' whose type is Generic, 'accuracy' whose type is Metric, 'language identification' whose type is Task, 'key prediction' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "he introduction of prior knowledge has greatly enhanced numerous purely low-level driven image processing algorithms. In this work, we focus on the problem of non-rigid image registration. A number of powerful registration criteria have been developed in the last decade, most prominently the criterion of maximum mutual information. Although this criterion provides for good registration results in many applications, it remains a purely low-level criterion. As a consequence, registration results will deteriorate once this low-level information is corrupted, due to noise, partial occlusions or missing image structure. In this paper , we will develop a Bayesian framework that allows to impose statistically learned prior knowledge about the joint intensity distribution into image registration methods. The prior is given by a kernel density estimate on the space of joint intensity distributions computed from a representative set of pre-registered image pairs. This nonparametric prior accurately models previously learned intensity relations between various image modalities and slice locations. Experimental results demonstrate that the resulting registration process is more robust to missing low-level information as it favors intensity correspondences statistically consistent with the learned intensity distributions.", "output": "The abstract text contains these key words below: 'prior knowledge' whose type is OtherScientificTerm, 'low-level driven image processing algorithms' whose type is Method, 'non-rigid image registration' whose type is Task, 'registration criteria' whose type is Metric, 'maximum mutual information' whose type is Metric, 'criterion' whose type is Generic, 'it' whose type is Generic, 'low-level criterion' whose type is Metric, 'registration' whose type is Task, 'low-level information' whose type is OtherScientificTerm, 'noise' whose type is OtherScientificTerm, 'partial occlusions' whose type is OtherScientificTerm, 'missing image structure' whose type is OtherScientificTerm, 'Bayesian framework' whose type is Method, 'statistically learned prior knowledge' whose type is OtherScientificTerm, 'joint intensity distribution' whose type is OtherScientificTerm, 'image registration methods' whose type is Method, 'prior' whose type is Generic, 'kernel density estimate' whose type is Method, 'joint intensity distributions' whose type is OtherScientificTerm, 'pre-registered image pairs' whose type is Material, 'intensity relations' whose type is OtherScientificTerm, 'image modalities' whose type is OtherScientificTerm, 'slice locations' whose type is OtherScientificTerm, 'registration process' whose type is Method, 'missing low-level information' whose type is OtherScientificTerm, 'it' whose type is OtherScientificTerm, 'intensity correspondences' whose type is OtherScientificTerm, 'intensity distributions' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": " GLOSSER  is designed to support reading and learning to read in a foreign  language . There are four  language pairs  currently supported by  GLOSSER :  English-Bulgarian ,  English-Estonian ,  English-Hungarian  and  French-Dutch . The program is operational on UNIX and Windows '95 platforms, and has undergone a pilot user-study. A demonstration (in UNIX) for  Applied Natural Language Processing  emphasizes components put to novel technical uses in  intelligent computer-assisted morphological analysis (ICALL) , including  disambiguated morphological analysis  and  lemmatized indexing  for an  aligned bilingual corpus  of  word examples . ", "output": "The abstract text contains these key words below: 'GLOSSER' whose type is Method, 'reading and learning' whose type is Task, 'language pairs' whose type is Generic, 'GLOSSER' whose type is Method, 'English-Bulgarian' whose type is Material, 'English-Estonian' whose type is Material, 'English-Hungarian' whose type is Material, 'French-Dutch' whose type is Material, 'program' whose type is Generic, 'UNIX and Windows '95 platforms' whose type is OtherScientificTerm, 'user-study' whose type is OtherScientificTerm, 'Applied Natural Language Processing' whose type is Task, 'components' whose type is Generic, 'intelligent computer-assisted morphological analysis (ICALL)' whose type is Task, 'disambiguated morphological analysis' whose type is Method, 'lemmatized indexing' whose type is Method, 'aligned bilingual corpus' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "A  domain independent model  is proposed for the  automated interpretation  of  nominal compounds  in  English . This  model  is meant to account for  productive rules of interpretation  which are inferred from the  morpho-syntactic and semantic characteristics  of the  nominal constituents . In particular, we make extensive use of Pustejovsky's principles concerning the  predicative information  associated with  nominals . We argue that it is necessary to draw a line between  generalizable semantic principles  and  domain-specific semantic information . We explain this distinction and we show how this model may be applied to the  interpretation  of  compounds  in  real texts , provided that complementary  semantic information  are retrieved.\n", "output": "The abstract text contains these key words below: 'domain independent model' whose type is Method, 'automated interpretation  of  nominal compounds' whose type is Task, 'nominal compounds' whose type is OtherScientificTerm, 'English' whose type is Material, 'model' whose type is Generic, 'productive rules of interpretation' whose type is OtherScientificTerm, 'morpho-syntactic and semantic characteristics' whose type is OtherScientificTerm, 'nominal constituents' whose type is OtherScientificTerm, 'Pustejovsky's principles' whose type is Method, 'predicative information' whose type is OtherScientificTerm, 'nominals' whose type is OtherScientificTerm, 'generalizable semantic principles' whose type is OtherScientificTerm, 'domain-specific semantic information' whose type is OtherScientificTerm, 'model' whose type is Generic, 'interpretation  of  compounds' whose type is Task, 'semantic information' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The study addresses the problem of  automatic acquisition  of  entailment relations  between  verbs . While this task has much in common with  paraphrases acquisition  which aims to discover  semantic equivalence  between  verbs , the main challenge of  entailment acquisition  is to capture  asymmetric, or directional, relations . Motivated by the intuition that it often underlies the  local structure  of  coherent text , we develop a method that discovers  verb entailment  using evidence about  discourse relations  between  clauses  available in a  parsed corpus . In comparison with earlier work, the proposed method covers a much wider range of  verb entailment types  and learns the  mapping  between  verbs  with highly varied  argument structures .\n", "output": "The abstract text contains these key words below: 'automatic acquisition  of  entailment relations' whose type is Task, 'task' whose type is Generic, 'paraphrases acquisition' whose type is Task, 'semantic equivalence' whose type is OtherScientificTerm, 'entailment acquisition' whose type is Task, 'asymmetric, or directional, relations' whose type is OtherScientificTerm, 'local structure  of  coherent text' whose type is OtherScientificTerm, 'method' whose type is Generic, 'verb entailment' whose type is Task, 'discourse relations' whose type is OtherScientificTerm, 'parsed corpus' whose type is Material, 'method' whose type is Generic, 'verb entailment types' whose type is OtherScientificTerm, 'mapping  between  verbs' whose type is Task, 'highly varied  argument structures' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The  verb forms  are often claimed to convey two kinds of  information  : 1. whether the  event  described in a  sentence  is  present ,  past  or  future  (=  deictic information ) 2. whether the  event  described in a  sentence  is presented as completed, going on, just starting or being finished (=  aspectual information ). It will be demonstrated in this paper that one has to add a third component to the analysis of  verb form meanings , namely whether or not they express  habituality . The framework of the analysis is  model-theoretic semantics . ", "output": "The abstract text contains these key words below: 'verb forms' whose type is OtherScientificTerm, 'information' whose type is Generic, 'deictic information' whose type is OtherScientificTerm, 'aspectual information' whose type is OtherScientificTerm, 'analysis of  verb form meanings' whose type is Task, 'analysis' whose type is Generic, 'model-theoretic semantics' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "The present paper reports on a preparatory research for building a  language corpus annotation scenario  capturing the  discourse relations  in  Czech . We primarily focus on the description of the  syntactically motivated relations  in  discourse , basing our findings on the theoretical background of the  Prague Dependency Treebank 2.0  and the  Penn Discourse Treebank 2 . Our aim is to revisit the present-day  syntactico-semantic (tectogrammatical) annotation  in the  Prague Dependency Treebank , extend it for the purposes of a  sentence-boundary-crossing representation  and eventually to design a new,  discourse level  of  annotation . In this paper, we propose a feasible process of such a transfer, comparing the possibilities the  Praguian dependency-based approach  offers with the  Penn discourse annotation  based primarily on the analysis and classification of  discourse connectives .\n", "output": "The abstract text contains these key words below: 'language corpus annotation scenario' whose type is Material, 'discourse relations' whose type is OtherScientificTerm, 'Czech' whose type is Material, 'syntactically motivated relations  in  discourse' whose type is OtherScientificTerm, 'Prague Dependency Treebank 2.0' whose type is Material, 'Penn Discourse Treebank 2' whose type is Material, 'syntactico-semantic (tectogrammatical) annotation' whose type is OtherScientificTerm, 'Prague Dependency Treebank' whose type is Material, 'it' whose type is Generic, 'sentence-boundary-crossing representation' whose type is Task, 'discourse level  of  annotation' whose type is Task, 'Praguian dependency-based approach' whose type is Method, 'Penn discourse annotation' whose type is Method, 'analysis and classification of  discourse connectives' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "We present a framework for  word alignment  based on  log-linear models . All  knowledge sources  are treated as  feature functions , which depend on the  source langauge sentence , the  target language sentence  and possible additional variables.  Log-linear models  allow  statistical alignment models  to be easily extended by incorporating  syntactic information . In this paper, we use  IBM Model 3 alignment probabilities ,  POS correspondence , and  bilingual dictionary coverage  as  features . Our experiments show that  log-linear models  significantly outperform  IBM translation models .\n", "output": "The abstract text contains these key words below: 'framework' whose type is Generic, 'word alignment' whose type is Task, 'log-linear models' whose type is Method, 'knowledge sources' whose type is Material, 'feature functions' whose type is OtherScientificTerm, 'Log-linear models' whose type is Method, 'statistical alignment models' whose type is Method, 'syntactic information' whose type is OtherScientificTerm, 'IBM Model 3 alignment probabilities' whose type is OtherScientificTerm, 'POS correspondence' whose type is OtherScientificTerm, 'bilingual dictionary coverage' whose type is OtherScientificTerm, 'features' whose type is OtherScientificTerm, 'log-linear models' whose type is Method, 'IBM translation models' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents a new two-pass algorithm for Extra Large (more than 1M words) Vocabulary COntinuous Speech recognition based on the Information Retrieval (ELVIRCOS). The principle of this approach is to decompose a recognition process into two passes where the first pass builds the words subset for the second pass recognition by using information retrieval procedure. Word graph composition for continuous speech is presented. With this approach a high performances for large vocabulary speech recognition can be obtained.", "output": "The abstract text contains these key words below: 'two-pass algorithm' whose type is Generic, 'Extra Large (more than 1M words) Vocabulary COntinuous Speech recognition' whose type is Task, 'Information Retrieval (ELVIRCOS)' whose type is Task, 'approach' whose type is Generic, 'recognition process' whose type is Method, 'passes' whose type is Generic, 'first pass' whose type is Generic, 'words subset' whose type is OtherScientificTerm, 'second pass recognition' whose type is Generic, 'information retrieval procedure' whose type is Method, 'Word graph composition' whose type is Method, 'continuous speech' whose type is Material, 'approach' whose type is Generic, 'large vocabulary speech recognition' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents an algorithm for labeling curvilinear structure at multiple scales in line drawings and edge images Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data. Tokens are computed via a small-to-large scale grouping procedure employing a \" greedy \" , best-first, strategy for choosing the support of new tokens. The resulting image description is rich and redundant in that a given segment of image contour may be described by multiple tokens at different scales, and by more than one token at any given scale. This property facilitates selection and characterization of portions of the image based on local CURVE-ELEMENT attributes.\n", "output": "The abstract text contains these key words below: 'algorithm' whose type is Generic, 'labeling curvilinear structure' whose type is Task, 'line drawings' whose type is Material, 'edge images' whose type is Material, 'CURVE-ELEMENT tokens' whose type is OtherScientificTerm, 'spatially-indexed and scale-indexed data structure' whose type is OtherScientificTerm, 'image data' whose type is Material, 'small-to-large scale grouping procedure' whose type is Method, 'image description' whose type is Task, 'image contour' whose type is OtherScientificTerm, 'local CURVE-ELEMENT attributes' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e propose a novel technique called bispectral photo-metric stereo that makes effective use of fluorescence for shape reconstruction. Fluorescence is a common phenomenon occurring in many objects from natural gems and corals, to fluorescent dyes used in clothing. One of the important characteristics of fluorescence is its wavelength-shifting behavior: fluorescent materials absorb light at a certain wavelength and then reemit it at longer wavelengths. Due to the complexity of its emission process, fluo-rescence tends to be excluded from most algorithms in computer vision and image processing. In this paper, we show that there is a strong similarity between fluorescence and ideal diffuse reflection and that fluorescence can provide distinct clues on how to estimate an object's shape. Moreover , fluorescence's wavelength-shifting property enables us to estimate the shape of an object by applying photomet-ric stereo to emission-only images without suffering from specular reflection. This is the significant advantage of the fluorescence-based method over previous methods based on reflection.", "output": "The abstract text contains these key words below: 'technique' whose type is Generic, 'bispectral photo-metric stereo' whose type is Method, 'fluorescence' whose type is OtherScientificTerm, 'shape reconstruction' whose type is Task, 'Fluorescence' whose type is OtherScientificTerm, 'natural gems' whose type is OtherScientificTerm, 'fluorescent dyes' whose type is OtherScientificTerm, 'fluorescence' whose type is OtherScientificTerm, 'fluorescent materials' whose type is OtherScientificTerm, 'complexity' whose type is Metric, 'emission process' whose type is Method, 'fluo-rescence' whose type is OtherScientificTerm, 'algorithms' whose type is Generic, 'computer vision' whose type is Task, 'image processing' whose type is Task, 'similarity' whose type is OtherScientificTerm, 'fluorescence' whose type is OtherScientificTerm, 'diffuse reflection' whose type is OtherScientificTerm, 'fluorescence' whose type is OtherScientificTerm, 'fluorescence's wavelength-shifting property' whose type is OtherScientificTerm, 'shape' whose type is OtherScientificTerm, 'photomet-ric stereo' whose type is Method, 'emission-only images' whose type is Material, 'specular reflection' whose type is OtherScientificTerm, 'fluorescence-based method' whose type is Method, 'methods' whose type is Generic, 'reflection' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n Interpreting  metaphors  is an integral and inescapable process in  human understanding of natural language  . This paper discusses a  method of analyzing metaphors  based on the existence of a small number of  generalized metaphor mappings  . Each  generalized metaphor  contains a  recognition network  , a  basic mapping  , additional  transfer mappings  , and an  implicit intention component  . It is argued that the method reduces  metaphor interpretation  from a  reconstruction  to a  recognition task  . Implications towards automating certain aspects of  language learning  are also discussed. ", "output": "The abstract text contains these key words below: 'Interpreting  metaphors' whose type is Task, 'human understanding of natural language' whose type is Task, 'method' whose type is Generic, 'analyzing metaphors' whose type is Task, 'generalized metaphor mappings' whose type is Method, 'generalized metaphor' whose type is OtherScientificTerm, 'recognition network' whose type is Method, 'basic mapping' whose type is Method, 'transfer mappings' whose type is Method, 'implicit intention component' whose type is Method, 'method' whose type is Generic, 'metaphor interpretation' whose type is Task, 'reconstruction' whose type is Task, 'recognition task' whose type is Task, 'language learning' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper presents the results of automatically inducing a  Combinatory Categorial Grammar (CCG) lexicon  from a  Turkish dependency treebank . The fact that  Turkish  is an  agglutinating free word order language  presents a challenge for  language theories . We explored possible ways to obtain a  compact lexicon , consistent with  CCG principles , from a  treebank  which is an order of magnitude smaller than  Penn WSJ .\n", "output": "The abstract text contains these key words below: 'automatically inducing a  Combinatory Categorial Grammar (CCG) lexicon' whose type is Task, 'Combinatory Categorial Grammar (CCG) lexicon' whose type is OtherScientificTerm, 'Turkish dependency treebank' whose type is Material, 'Turkish' whose type is Material, 'agglutinating free word order language' whose type is Material, 'language theories' whose type is Method, 'compact lexicon' whose type is OtherScientificTerm, 'CCG principles' whose type is Method, 'treebank' whose type is Generic, 'Penn WSJ' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper describes to what extent  deep processing  may benefit from  shallow techniques  and it presents a  NLP system  which integrates a  linguistic PoS tagger and chunker  as a preprocessing module of a  broad coverage unification based grammar of Spanish . Experiments show that the  efficiency  of the overall analysis improves significantly and that our system also provides  robustness  to the  linguistic processing  while maintaining both the  accuracy  and the  precision  of the  grammar .\n", "output": "The abstract text contains these key words below: 'deep processing' whose type is Task, 'shallow techniques' whose type is Method, 'NLP system' whose type is Method, 'linguistic PoS tagger and chunker' whose type is Method, 'broad coverage unification based grammar of Spanish' whose type is Method, 'system' whose type is Generic, 'robustness' whose type is Metric, 'linguistic processing' whose type is Task, 'accuracy' whose type is Metric, 'precision' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we describe a novel  data structure  for  phrase-based statistical machine translation  which allows for the  retrieval  of arbitrarily long  phrases  while simultaneously using less  memory  than is required by current  decoder  implementations. We detail the  computational complexity  and  average retrieval times  for looking up  phrase translations  in our  suffix array-based data structure  . We show how  sampling  can be used to reduce the  retrieval time  by orders of magnitude with no loss in  translation quality  . ", "output": "The abstract text contains these key words below: 'data structure' whose type is OtherScientificTerm, 'phrase-based statistical machine translation' whose type is Task, 'retrieval  of arbitrarily long  phrases' whose type is Task, 'memory' whose type is OtherScientificTerm, 'decoder' whose type is Method, 'computational complexity' whose type is Metric, 'average retrieval times' whose type is Metric, 'phrase translations' whose type is OtherScientificTerm, 'suffix array-based data structure' whose type is OtherScientificTerm, 'sampling' whose type is Method, 'retrieval time' whose type is Metric, 'translation quality' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n This paper summarizes the formalism of  Category Cooccurrence Restrictions (CCRs)  and describes two  parsing algorithms  that interpret it.  CCRs  are  Boolean conditions  on the cooccurrence of  categories  in  local trees  which allow the  statement of generalizations  which cannot be captured in other current  syntax formalisms  . The use of  CCRs  leads to  syntactic descriptions  formulated entirely with  restrictive statements  . The paper shows how conventional algorithms for the analysis of  context free languages  can be adapted to the  CCR formalism  . Special attention is given to the part of the  parser  that checks the fulfillment of  logical well-formedness conditions  on  trees  . ", "output": "The abstract text contains these key words below: 'formalism of  Category Cooccurrence Restrictions (CCRs)' whose type is Task, 'Category Cooccurrence Restrictions (CCRs)' whose type is OtherScientificTerm, 'parsing algorithms' whose type is Method, 'it' whose type is Generic, 'CCRs' whose type is OtherScientificTerm, 'Boolean conditions' whose type is OtherScientificTerm, 'local trees' whose type is OtherScientificTerm, 'statement of generalizations' whose type is OtherScientificTerm, 'syntax formalisms' whose type is Method, 'CCRs' whose type is OtherScientificTerm, 'syntactic descriptions' whose type is OtherScientificTerm, 'restrictive statements' whose type is OtherScientificTerm, 'algorithms' whose type is Generic, 'context free languages' whose type is Material, 'CCR formalism' whose type is Task, 'parser' whose type is Method, 'logical well-formedness conditions' whose type is OtherScientificTerm, 'trees' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ur goal is to learn a Mahalanobis distance by minimizing a loss defined on the weighted sum of the precision at different ranks. Our core motivation is that minimizing a weighted rank loss is a natural criterion for many problems in computer vision such as person re-identification. We propose a novel metric learning formulation called Weighted Approximate Rank Component Analysis (WARCA). We then derive a scalable stochastic gradient descent algorithm for the resulting learning problem. We also derive an efficient non-linear extension of WARCA by using the kernel trick. Kernel space embedding decouples the training and prediction costs from the data dimension and enables us to plug inarbitrary distance measures which are more natural for the features. We also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently. We validate this new method on nine standard person re-identification datasets including two large scale Market-1501 and CUHK03 datasets and show that we improve upon the current state-of-the-art methods on all of them.", "output": "The abstract text contains these key words below: 'Mahalanobis distance' whose type is Task, 'loss' whose type is OtherScientificTerm, 'weighted sum' whose type is OtherScientificTerm, 'precision' whose type is Metric, 'ranks' whose type is OtherScientificTerm, 'weighted rank loss' whose type is OtherScientificTerm, 'computer vision' whose type is Task, 'person re-identification' whose type is Task, 'metric learning formulation' whose type is Method, 'Weighted Approximate Rank Component Analysis (WARCA)' whose type is Method, 'stochastic gradient descent algorithm' whose type is Method, 'learning problem' whose type is Generic, 'non-linear extension of WARCA' whose type is Method, 'WARCA' whose type is Method, 'kernel trick' whose type is Method, 'Kernel space embedding' whose type is OtherScientificTerm, 'training and prediction costs' whose type is OtherScientificTerm, 'data dimension' whose type is OtherScientificTerm, 'inarbitrary distance measures' whose type is Method, 'features' whose type is OtherScientificTerm, 'matrix rank degeneration' whose type is OtherScientificTerm, 'non-isolated minima' whose type is OtherScientificTerm, 'low-rank matrix optimization' whose type is Task, 'regularizer' whose type is Method, 'or-thonormality' whose type is OtherScientificTerm, 'learned matrix' whose type is OtherScientificTerm, 'method' whose type is Generic, 'person re-identification datasets' whose type is Material, 'scale Market-1501' whose type is Material, 'CUHK03 datasets' whose type is Material, 'them' whose type is Generic."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper we present  ONTOSCORE  , a system for scoring sets of  concepts  on the basis of an  ontology  . We apply our system to the task of  scoring  alternative  speech recognition hypotheses (SRH)  in terms of their  semantic coherence  . We conducted an  annotation experiment  and showed that  human annotators  can reliably differentiate between semantically coherent and incoherent  speech recognition hypotheses  . An evaluation of our system against the  annotated data  shows that, it successfully classifies 73.2% in a  German corpus  of 2.284  SRHs  as either coherent or incoherent (given a  baseline  of 54.55%). ", "output": "The abstract text contains these key words below: 'ONTOSCORE' whose type is Method, 'system' whose type is Generic, 'ontology' whose type is OtherScientificTerm, 'system' whose type is Generic, 'speech recognition hypotheses (SRH)' whose type is OtherScientificTerm, 'semantic coherence' whose type is OtherScientificTerm, 'speech recognition hypotheses' whose type is OtherScientificTerm, 'system' whose type is Generic, 'it' whose type is Generic, 'German corpus' whose type is Material, 'SRHs' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e present a scanning method that recovers dense sub-pixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as mi-cro phase shifting and modulated phase shifting.", "output": "The abstract text contains these key words below: 'scanning method' whose type is Method, 'dense sub-pixel camera-projector correspondence' whose type is OtherScientificTerm, 'photometric calibration' whose type is OtherScientificTerm, 'relative geometry' whose type is OtherScientificTerm, 'Subpixel accuracy' whose type is Metric, 'zero-crossings' whose type is OtherScientificTerm, 'unstructured patterns' whose type is OtherScientificTerm, 'gray-level band-pass white noise patterns' whose type is OtherScientificTerm, 'robustness' whose type is Metric, 'indirect lighting' whose type is OtherScientificTerm, 'scene discontinuities' whose type is OtherScientificTerm, 'method' whose type is Generic, 'scene geometry' whose type is OtherScientificTerm, 'subpixel precision' whose type is Metric, 'it' whose type is Generic, 'active reconstruction systems' whose type is Task, 'state of the art methods' whose type is Generic, 'mi-cro phase shifting' whose type is Method, 'modulated phase shifting' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "ith the recent popularity of animated GIFs on social media, there is need for ways to index them with rich meta-data. To advance research on animated GIF understanding, we collected a new dataset, Tumblr GIF (TGIF), with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing. The motivation for this work is to develop a testbed for image sequence description systems, where the task is to generate natural language descriptions for animated GIFs or video clips. To ensure a high quality dataset, we developed a series of novel quality controls to validate free-form text input from crowd-workers. We show that there is unambiguous association between visual content and natural language descriptions in our dataset, making it an ideal benchmark for the visual content captioning task. We perform extensive statistical analyses to compare our dataset to existing image and video description datasets. Next, we provide baseline results on the animated GIF description task, using three representative techniques: nearest neighbor, statistical machine translation , and recurrent neural networks. Finally, we show that models fine-tuned from our animated GIF description dataset can be helpful for automatic movie description.", "output": "The abstract text contains these key words below: 'animated GIFs' whose type is Material, 'social media' whose type is Material, 'rich meta-data' whose type is Material, 'animated GIF understanding' whose type is Task, 'dataset' whose type is Generic, 'Tumblr GIF (TGIF)' whose type is Material, 'animated GIFs' whose type is Material, 'natural language descriptions' whose type is Material, 'crowdsourcing' whose type is Method, 'image sequence description systems' whose type is Task, 'natural language descriptions' whose type is Material, 'animated GIFs' whose type is Material, 'video clips' whose type is Material, 'quality controls' whose type is Method, 'free-form text input' whose type is Material, 'visual content' whose type is Material, 'natural language descriptions' whose type is Material, 'dataset' whose type is Generic, 'it' whose type is Generic, 'visual content captioning task' whose type is Task, 'statistical analyses' whose type is OtherScientificTerm, 'dataset' whose type is Generic, 'image and video description datasets' whose type is Material, 'animated GIF description task' whose type is Task, 'representative techniques' whose type is Generic, 'nearest neighbor' whose type is Method, 'statistical machine translation' whose type is Method, 'recurrent neural networks' whose type is Method, 'animated GIF description dataset' whose type is Material, 'automatic movie description' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper presents a novel statistical singing voice conversion (SVC) technique with direct waveform modification based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms. SVC makes it possible to convert singing voice characteristics of an arbitrary source singer into those of an arbitrary target singer. However, speech quality of the converted singing voice is significantly degraded compared to that of a natural singing voice due to various factors, such as analysis and modeling errors in the vocoder-based framework. To alleviate this degradation, we propose a statistical conversion process that directly modifies the signal in the waveform domain by estimating the difference in the spectra of the source and target singers' singing voices. The differential spectral feature is directly estimated using a differential Gaussian mixture model (GMM) that is analytically derived from the traditional GMM used as a conversion model in the conventional SVC. The experimental results demonstrate that the proposed method makes it possible to significantly improve speech quality in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional SVC.", "output": "The abstract text contains these key words below: 'statistical singing voice conversion (SVC) technique' whose type is Method, 'direct waveform modification' whose type is Method, 'spectrum differential' whose type is OtherScientificTerm, 'voice timbre' whose type is OtherScientificTerm, 'vocoder' whose type is Method, 'converted singing voice waveforms' whose type is OtherScientificTerm, 'SVC' whose type is Method, 'singing voice characteristics' whose type is OtherScientificTerm, 'speech quality' whose type is Metric, 'converted singing voice' whose type is OtherScientificTerm, 'natural singing voice' whose type is OtherScientificTerm, 'vocoder-based framework' whose type is Method, 'statistical conversion process' whose type is Method, 'spectra' whose type is OtherScientificTerm, 'differential spectral feature' whose type is OtherScientificTerm, 'differential Gaussian mixture model (GMM)' whose type is Method, 'GMM' whose type is Method, 'conversion model' whose type is Method, 'SVC' whose type is Method, 'method' whose type is Generic, 'speech quality' whose type is Metric, 'conversion accuracy of singer identity' whose type is Metric, 'SVC' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n We introduce a new  interactive corpus exploration tool  called  InfoMagnets  .  InfoMagnets  aims at making  exploratory corpus analysis  accessible to researchers who are not experts in  text mining  . As evidence of its usefulness and usability, it has been used successfully in a research context to uncover relationships between  language  and  behavioral patterns  in two distinct domains:  tutorial dialogue (Kumar et al., submitted) and  on-line communities  (Arguello et al., 2006). As an  educational tool  , it has been used as part of a unit on  protocol analysis  in an  Educational Research Methods course  . ", "output": "The abstract text contains these key words below: 'interactive corpus exploration tool' whose type is Method, 'InfoMagnets' whose type is Method, 'InfoMagnets' whose type is Method, 'exploratory corpus analysis' whose type is Task, 'text mining' whose type is Task, 'it' whose type is Generic, 'domains' whose type is Generic, 'tutorial dialogue' whose type is Material, 'on-line communities' whose type is Material, 'educational tool' whose type is Method, 'it' whose type is Generic, 'protocol analysis' whose type is Task."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "e revisit the classical decision-theoretic problem of weighted expert voting from a statistical learning perspective. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed. Experimental results are provided to illustrate the theory.", "output": "The abstract text contains these key words below: 'classical decision-theoretic problem of weighted expert voting' whose type is Task, 'statistical learning perspective' whose type is Method, 'Nitzan-Paroush weighted majority' whose type is OtherScientificTerm, 'expert competence levels' whose type is OtherScientificTerm, 'sharp error estimates' whose type is Method, 'optimal rule' whose type is OtherScientificTerm, 'competence levels' whose type is OtherScientificTerm, 'Bayesian analyses' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this study, we propose a  knowledge-independent method  for aligning  terms  and thus extracting  translations  from a  small, domain-specific corpus  consisting of  parallel English and Chinese court judgments  from Hong Kong. With a  sentence-aligned corpus ,  translation equivalences  are suggested by analysing the  frequency profiles  of  parallel concordances . The method overcomes the limitations of  conventional statistical methods  which require  large corpora  to be effective, and  lexical approaches  which depend on existing  bilingual dictionaries . Pilot testing on a  parallel corpus  of about 113K  Chinese words  and 120K  English words  gives an encouraging 85%  precision  and 45%  recall . Future work includes fine-tuning the  algorithm  upon the analysis of the errors, and acquiring a  translation lexicon  for  legal terminology  by filtering out  general terms .\n", "output": "The abstract text contains these key words below: 'knowledge-independent method' whose type is Method, 'small, domain-specific corpus' whose type is Material, 'parallel English and Chinese court judgments' whose type is Material, 'sentence-aligned corpus' whose type is Material, 'translation equivalences' whose type is OtherScientificTerm, 'frequency profiles' whose type is Generic, 'parallel concordances' whose type is OtherScientificTerm, 'method' whose type is Generic, 'statistical methods' whose type is Method, 'large corpora' whose type is Material, 'lexical approaches' whose type is Method, 'bilingual dictionaries' whose type is Material, 'parallel corpus' whose type is Material, 'precision' whose type is Metric, 'recall' whose type is Metric, 'algorithm' whose type is Generic, 'translation lexicon' whose type is OtherScientificTerm, 'legal terminology' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "his paper investigates critical configurations for projective reconstruction from multiple images taken by a camera moving in a straight line. Projective reconstruction refers to a determination of the 3D geometrical configuration of a set of 3D points and cameras, given only correspondences between points in the images. A configuration of points and cameras is critical if it can not be determined uniquely (up to a projective transform) from the image coordinates of the points. It is shown that a configuration consisting of any number of cameras lying on a straight line, and any number of points lying on a twisted cubic constitutes a critical configuration. An alternative configuration consisting of a set of points and cameras all lying on a rational quartic curve exists.", "output": "The abstract text contains these key words below: 'projective reconstruction' whose type is Task, 'images' whose type is Material, 'Projective reconstruction' whose type is Task, '3D geometrical configuration' whose type is OtherScientificTerm, '3D points and cameras' whose type is OtherScientificTerm, 'image coordinates' whose type is OtherScientificTerm, 'rational quartic curve' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "How to obtain  hierarchical relations (e.g.  superordinate -hyponym relation ,  synonym relation ) is one of the most important problems for  thesaurus construction . A pilot system for extracting these  relations  automatically from an ordinary  Japanese language dictionary  (Shinmeikai Kokugojiten, published by Sansei-do, in machine readable form) is given. The features of the  definition sentences  in the  dictionary , the mechanical extraction of the  hierarchical relations  and the estimation of the results are discussed.\n", "output": "The abstract text contains these key words below: 'hierarchical relations' whose type is OtherScientificTerm, 'superordinate -hyponym relation' whose type is OtherScientificTerm, 'synonym relation' whose type is OtherScientificTerm, 'thesaurus construction' whose type is Task, 'relations' whose type is Generic, 'Japanese language dictionary' whose type is Material, 'features' whose type is OtherScientificTerm, 'definition sentences' whose type is OtherScientificTerm, 'dictionary' whose type is Generic, 'hierarchical relations' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This poster paper describes a  full scale two-level morphological description  (Karttunen, 1983; Koskenniemi, 1983) of  Turkish word structures . The description has been implemented using the  PC-KIMMO environment  (Antworth, 1990) and is based on a  root word lexicon  of about 23,000  roots words . Almost all the special cases of and exceptions to  phonological and morphological rules  have been implemented.  Turkish  is an  agglutinative language  with  word structures  formed by  productive affixations of derivational and inflectional suffixes  to  root words .  Turkish  has  finite-state  but nevertheless rather complex morphotactics.  Morphemes  added to a  root word  or a  stem  can convert the  word  from a  nominal  to a  verbal structure  or vice-versa, or can create  adverbial constructs . The  surface realizations  of  morphological constructions  are constrained and modified by a number of  phonetic rules  such as  vowel harmony .\n", "output": "The abstract text contains these key words below: 'full scale two-level morphological description' whose type is Task, 'Turkish word structures' whose type is Material, 'description' whose type is Generic, 'PC-KIMMO environment' whose type is Method, 'root word lexicon' whose type is Material, 'phonological and morphological rules' whose type is OtherScientificTerm, 'Turkish' whose type is Material, 'agglutinative language' whose type is Material, 'word structures' whose type is OtherScientificTerm, 'productive affixations of derivational and inflectional suffixes' whose type is OtherScientificTerm, 'Turkish' whose type is Material, 'finite-state' whose type is OtherScientificTerm, 'Morphemes' whose type is OtherScientificTerm, 'verbal structure' whose type is OtherScientificTerm, 'adverbial constructs' whose type is OtherScientificTerm, 'surface realizations  of  morphological constructions' whose type is Task, 'phonetic rules' whose type is OtherScientificTerm, 'vowel harmony' whose type is OtherScientificTerm."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n We discuss  maximum a posteriori estimation  of  continuous density hidden Markov models (CDHMM)  . The classical  MLE reestimation algorithms  , namely the  forward-backward algorithm  and the  segmental k-means algorithm  , are expanded and  reestimation formulas  are given for  HMM with Gaussian mixture observation densities  . Because of its adaptive nature,  Bayesian learning  serves as a unified approach for the following four  speech recognition  applications, namely  parameter smoothing  ,  speaker adaptation  ,  speaker group modeling  and  corrective training  . New experimental results on all four applications are provided to show the effectiveness of the  MAP estimation approach  . ", "output": "The abstract text contains these key words below: 'maximum a posteriori estimation' whose type is Method, 'continuous density hidden Markov models (CDHMM)' whose type is Method, 'MLE reestimation algorithms' whose type is Method, 'forward-backward algorithm' whose type is Method, 'segmental k-means algorithm' whose type is Method, 'reestimation formulas' whose type is OtherScientificTerm, 'HMM with Gaussian mixture observation densities' whose type is Method, 'Bayesian learning' whose type is Method, 'speech recognition  applications' whose type is Task, 'parameter smoothing' whose type is Task, 'speaker adaptation' whose type is Task, 'speaker group modeling' whose type is Task, 'corrective training' whose type is Task, 'applications' whose type is Generic, 'MAP estimation approach' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "robabilistic models have been previously shown to be efficient and effective for modeling and recognition of human motion. In particular we focus on methods which represent the human motion model as a triangulated graph. Previous approaches learned models based just on positions and velocities of the body parts while ignoring their appearance. Moreover, a heuristic approach was commonly used to obtain translation invariance. In this paper we suggest an improved approach for learning such models and using them for human motion recognition. The suggested approach combines multiple cues, i.e., positions, velocities and appearance into both the learning and detection phases. Furthermore, we introduce global variables in the model, which can represent global properties such as translation, scale or viewpoint. The model is learned in an unsupervised manner from un-labelled data. We show that the suggested hybrid proba-bilistic model (which combines global variables, like translation , with local variables, like relative positions and appearances of body parts), leads to: (i) faster convergence of learning phase, (ii) robustness to occlusions, and, (iii) higher recognition rate.", "output": "The abstract text contains these key words below: 'Probabilistic models' whose type is Method, 'modeling and recognition of human motion' whose type is Task, 'human motion model' whose type is Method, 'triangulated graph' whose type is Method, 'models' whose type is Generic, 'positions' whose type is OtherScientificTerm, 'velocities' whose type is OtherScientificTerm, 'appearance' whose type is OtherScientificTerm, 'heuristic approach' whose type is Method, 'translation invariance' whose type is OtherScientificTerm, 'approach' whose type is Generic, 'models' whose type is Generic, 'them' whose type is Generic, 'human motion recognition' whose type is Task, 'approach' whose type is Generic, 'cues' whose type is Generic, 'positions' whose type is OtherScientificTerm, 'velocities' whose type is OtherScientificTerm, 'appearance' whose type is OtherScientificTerm, 'learning and detection phases' whose type is Task, 'global variables' whose type is OtherScientificTerm, 'model' whose type is Generic, 'global properties' whose type is OtherScientificTerm, 'translation' whose type is OtherScientificTerm, 'scale' whose type is OtherScientificTerm, 'viewpoint' whose type is OtherScientificTerm, 'model' whose type is Generic, 'unsupervised manner' whose type is Method, 'un-labelled data' whose type is Material, 'hybrid proba-bilistic model' whose type is Method, 'global variables' whose type is OtherScientificTerm, 'translation' whose type is OtherScientificTerm, 'local variables' whose type is OtherScientificTerm, 'relative positions' whose type is OtherScientificTerm, 'appearances of body parts' whose type is OtherScientificTerm, 'faster convergence' whose type is Metric, 'learning phase' whose type is Task, 'robustness' whose type is Metric, 'occlusions' whose type is OtherScientificTerm, 'recognition rate' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "oint matrix triangularization is often used for estimating the joint eigenstructure of a set M of matrices, with applications in signal processing and machine learning. We consider the problem of approximate joint matrix triangularization when the matrices in M are jointly diagonalizable and real, but we only observe a set M' of noise perturbed versions of the matrices in M. Our main result is a first-order upper bound on the distance between any approximate joint triangularizer of the matrices in M' and any exact joint triangularizer of the matrices in M. The bound depends only on the observable matrices in M' and the noise level. In particular, it does not depend on optimization specific properties of the triangularizer, such as its proximity to critical points, that are typical of existing bounds in the literature. To our knowledge, this is the first a posteriori bound for joint matrix decomposition. We demonstrate the bound on synthetic data for which the ground truth is known.", "output": "The abstract text contains these key words below: 'Joint matrix triangularization' whose type is Task, 'joint eigenstructure' whose type is OtherScientificTerm, 'signal processing' whose type is Task, 'machine learning' whose type is Task, 'approximate joint matrix triangularization' whose type is Task, 'first-order upper bound' whose type is OtherScientificTerm, 'approximate joint triangularizer' whose type is Method, 'exact joint triangularizer' whose type is Method, 'bound' whose type is Generic, 'triangularizer' whose type is Method, 'posteriori bound' whose type is OtherScientificTerm, 'joint matrix decomposition' whose type is Task, 'synthetic data' whose type is Material."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "In this paper, we compare the relative effects of  segment order  ,  segmentation  and  segment contiguity  on the  retrieval performance  of a  translation memory system  . We take a selection of both  bag-of-words and segment order-sensitive string comparison methods  , and run each over both  character- and word-segmented data  , in combination with a range of  local segment contiguity models  (in the form of  N-grams  ). Over two distinct  datasets  , we find that  indexing  according to simple  character bigrams  produces a  retrieval accuracy  superior to any of the tested  word N-gram models  . Further,in their optimum  configuration  ,  bag-of-words methods  are shown to be equivalent to  segment order-sensitive methods  in terms of  retrieval accuracy  , but much faster. We also provide evidence that our findings are scalable. ", "output": "The abstract text contains these key words below: 'segment order' whose type is OtherScientificTerm, 'segmentation' whose type is OtherScientificTerm, 'segment contiguity' whose type is OtherScientificTerm, 'retrieval' whose type is Task, 'translation memory system' whose type is Method, 'bag-of-words and segment order-sensitive string comparison methods' whose type is Method, 'character- and word-segmented data' whose type is Material, 'local segment contiguity models' whose type is Method, 'N-grams' whose type is Method, 'indexing' whose type is Task, 'character bigrams' whose type is Method, 'retrieval accuracy' whose type is Metric, 'word N-gram models' whose type is Method, 'bag-of-words methods' whose type is Method, 'segment order-sensitive methods' whose type is Method, 'retrieval accuracy' whose type is Metric."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "\n The unique properties of  tree-adjoining grammars (TAG)  present a challenge for the application of  TAGs  beyond the limited confines of  syntax  , for instance, to the task of  semantic interpretation  or  automatic translation of natural language  . We present a variant of  TAGs  , called  synchronous TAGs  , which characterize correspondences between  languages  . The formalism's intended usage is to relate  expressions of natural languages  to their associated  semantics  represented in a  logical form language  , or to their  translates  in another  natural language  ; in summary, we intend it to allow  TAGs  to be used beyond their role in  syntax proper  . We discuss the application of  synchronous TAGs  to concrete examples, mentioning primarily in passing some computational issues that arise in its interpretation. ", "output": "The abstract text contains these key words below: 'tree-adjoining grammars (TAG)' whose type is Method, 'TAGs' whose type is Method, 'syntax' whose type is OtherScientificTerm, 'semantic interpretation' whose type is Task, 'automatic translation of natural language' whose type is Task, 'variant of  TAGs' whose type is Method, 'TAGs' whose type is Method, 'synchronous TAGs' whose type is Method, 'expressions of natural languages' whose type is OtherScientificTerm, 'semantics' whose type is OtherScientificTerm, 'logical form language' whose type is OtherScientificTerm, 'natural language' whose type is OtherScientificTerm, 'TAGs' whose type is Method, 'syntax proper' whose type is OtherScientificTerm, 'synchronous TAGs' whose type is Method."}, {"instruction": "Please extract the key word of this abstract and list their type.", "input": "This paper describes a new,  large scale discourse-level annotation  project - the  Penn Discourse TreeBank (PDTB) . We present an approach to annotating a level of  discourse structure  that is based on identifying  discourse connectives  and their  arguments . The  PDTB  is being built directly on top of the  Penn TreeBank  and  Propbank , thus supporting the extraction of useful  syntactic and semantic features  and providing a richer substrate for the development and evaluation of  practical algorithms . We provide a detailed preliminary analysis of  inter-annotator agreement  - both the  level of agreement  and the types of  inter-annotator variation .\n", "output": "The abstract text contains these key words below: 'large scale discourse-level annotation' whose type is Task, 'Penn Discourse TreeBank (PDTB)' whose type is Material, 'approach' whose type is Generic, 'discourse structure' whose type is OtherScientificTerm, 'discourse connectives' whose type is OtherScientificTerm, 'PDTB' whose type is Material, 'Penn TreeBank' whose type is Material, 'Propbank' whose type is Material, 'extraction of useful  syntactic and semantic features' whose type is Task, 'practical algorithms' whose type is Method, 'inter-annotator agreement' whose type is Metric, 'level of agreement' whose type is Metric, 'inter-annotator variation' whose type is OtherScientificTerm."}]